
## 4. 搭建一个Transformer

### 4.1 模型训练与优化

构建完Transformer架构后，我们需要实现训练循环和优化策略。Transformer的训练有一些特殊的考虑，如学习率调度、标签平滑等。

```python
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import time
from tqdm import tqdm

class TransformerTrainer:
    """Transformer训练器"""
    
    def __init__(self, model, train_loader, val_loader, device, config):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.config = config
        
        # 优化器
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # 损失函数
        self.criterion = nn.CrossEntropyLoss(
            ignore_index=config.get('pad_idx', 0),
            label_smoothing=config.get('label_smoothing', 0.1)
        )
        
        # 训练状态
        self.global_step = 0
        self.best_val_loss = float('inf')
    
    def _create_optimizer(self):
        """创建优化器"""
        return optim.Adam(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            betas=(0.9, 0.98),
            eps=1e-9,
            weight_decay=self.config.get('weight_decay', 0.0)
        )
    
    def _create_scheduler(self):
        """创建学习率调度器"""
        def lr_lambda(step):
            # Transformer原论文中的学习率调度
            d_model = self.config['d_model']
            warmup_steps = self.config['warmup_steps']
            
            step = max(1, step)  # 避免除零
            return (d_model ** -0.5) * min(step ** -0.5, step * warmup_steps ** -1.5)
        
        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)
    
    def train_epoch(self):
        """训练一个epoch"""
        self.model.train()
        total_loss = 0
        total_tokens = 0
        
        progress_bar = tqdm(self.train_loader, desc="Training")
        
        for batch_idx, batch in enumerate(progress_bar):
            src = batch['src'].to(self.device)
            tgt = batch['tgt'].to(self.device)
            
            # 创建掩码
            src_mask = create_padding_mask(src, self.config.get('pad_idx', 0))
            tgt_mask = create_causal_mask(tgt.size(1)).to(self.device)
            
            # 前向传播
            self.optimizer.zero_grad()
            
            # 解码器输入（去掉最后一个token）
            tgt_input = tgt[:, :-1]
            # 解码器目标（去掉第一个token）
            tgt_output = tgt[:, 1:]
            
            outputs = self.model(src, tgt_input, src_mask, tgt_mask)
            
            # 计算损失
            loss = self.criterion(
                outputs['output'].reshape(-1, outputs['output'].size(-1)),
                tgt_output.reshape(-1)
            )
            
            # 反向传播
            loss.backward()
            
            # 梯度裁剪
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config.get('max_grad_norm', 1.0)
            )
            
            self.optimizer.step()
            self.scheduler.step()
            
            # 统计
            batch_tokens = (tgt_output != self.config.get('pad_idx', 0)).sum().item()
            total_loss += loss.item() * batch_tokens
            total_tokens += batch_tokens
            self.global_step += 1
            
            # 更新进度条
            progress_bar.set_postfix({
                'loss': loss.item(),
                'lr': self.scheduler.get_last_lr()[0],
                'step': self.global_step
            })
        
        return total_loss / total_tokens
    
    def validate(self):
        """验证模型"""
        self.model.eval()
        total_loss = 0
        total_tokens = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                src = batch['src'].to(self.device)
                tgt = batch['tgt'].to(self.device)
                
                src_mask = create_padding_mask(src, self.config.get('pad_idx', 0))
                tgt_mask = create_causal_mask(tgt.size(1)).to(self.device)
                
                tgt_input = tgt[:, :-1]
                tgt_output = tgt[:, 1:]
                
                outputs = self.model(src, tgt_input, src_mask, tgt_mask)
                
                loss = self.criterion(
                    outputs['output'].reshape(-1, outputs['output'].size(-1)),
                    tgt_output.reshape(-1)
                )
                
                batch_tokens = (tgt_output != self.config.get('pad_idx', 0)).sum().item()
                total_loss += loss.item() * batch_tokens
                total_tokens += batch_tokens
        
        return total_loss / total_tokens
    
    def train(self, num_epochs):
        """完整训练流程"""
        print(f"开始训练 {num_epochs} 个epochs...")
        
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch + 1}/{num_epochs}")
            
            # 训练
            train_loss = self.train_epoch()
            
            # 验证
            val_loss = self.validate()
            
            print(f"Train Loss: {train_loss:.4f}")
            print(f"Val Loss: {val_loss:.4f}")
            print(f"Perplexity: {math.exp(val_loss):.2f}")
            
            # 保存最佳模型
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_model(f'best_model_epoch_{epoch+1}.pt')
                print("保存最佳模型")
    
    def save_model(self, path):
        """保存模型"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'global_step': self.global_step,
            'best_val_loss': self.best_val_loss,
            ```python
            'config': self.config
        }, path)
        print(f"模型已保存到 {path}")

class SimpleTranslationDataset(Dataset):
    """简单的翻译数据集"""
    
    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=100):
        self.src_sentences = src_sentences
        self.tgt_sentences = tgt_sentences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        self.max_len = max_len
        
        # 特殊token
        self.src_pad_idx = src_vocab.get('<pad>', 0)
        self.tgt_pad_idx = tgt_vocab.get('<pad>', 0)
        self.tgt_bos_idx = tgt_vocab.get('<bos>', 1)
        self.tgt_eos_idx = tgt_vocab.get('<eos>', 2)
    
    def __len__(self):
        return len(self.src_sentences)
    
    def __getitem__(self, idx):
        src_sent = self.src_sentences[idx]
        tgt_sent = self.tgt_sentences[idx]
        
        # 转换为token ids
        src_ids = [self.src_vocab.get(token, self.src_vocab.get('<unk>', 3)) 
                   for token in src_sent.split()]
        tgt_ids = [self.tgt_vocab.get(token, self.tgt_vocab.get('<unk>', 3)) 
                   for token in tgt_sent.split()]
        
        # 添加特殊token并截断
        src_ids = src_ids[:self.max_len-2]
        tgt_ids = [self.tgt_bos_idx] + tgt_ids[:self.max_len-2] + [self.tgt_eos_idx]
        
        # 填充
        src_ids += [self.src_pad_idx] * (self.max_len - len(src_ids))
        tgt_ids += [self.tgt_pad_idx] * (self.max_len - len(tgt_ids))
        
        return {
            'src': torch.tensor(src_ids, dtype=torch.long),
            'tgt': torch.tensor(tgt_ids, dtype=torch.long)
        }

def create_sample_data():
    """创建示例数据"""
    # 简单的英法翻译数据
    src_sentences = [
        "hello world",
        "how are you",
        "good morning",
        "thank you very much",
        "see you later",
        "what is your name",
        "i love you",
        "have a nice day"
    ] * 100  # 重复以增加数据量
    
    tgt_sentences = [
        "bonjour monde",
        "comment allez vous",
        "bonjour",
        "merci beaucoup",
        "a bientot",
        "quel est votre nom",
        "je vous aime",
        "bonne journee"
    ] * 100
    
    # 创建词汇表
    src_vocab = {'<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3}
    tgt_vocab = {'<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3}
    
    # 构建词汇表
    for sent in src_sentences:
        for token in sent.split():
            if token not in src_vocab:
                src_vocab[token] = len(src_vocab)
    
    for sent in tgt_sentences:
        for token in sent.split():
            if token not in tgt_vocab:
                tgt_vocab[token] = len(tgt_vocab)
    
    return src_sentences, tgt_sentences, src_vocab, tgt_vocab

def demonstrate_training():
    """演示训练过程"""
    
    # 创建数据
    src_sentences, tgt_sentences, src_vocab, tgt_vocab = create_sample_data()
    
    # 分割训练和验证数据
    split_idx = int(0.8 * len(src_sentences))
    train_src, val_src = src_sentences[:split_idx], src_sentences[split_idx:]
    train_tgt, val_tgt = tgt_sentences[:split_idx], tgt_sentences[split_idx:]
    
    # 创建数据集
    train_dataset = SimpleTranslationDataset(train_src, train_tgt, src_vocab, tgt_vocab)
    val_dataset = SimpleTranslationDataset(val_src, val_tgt, src_vocab, tgt_vocab)
    
    # 创建数据加载器
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # 模型配置
    config = {
        'd_model': 256,
        'n_heads': 8,
        'n_layers': 4,
        'd_ff': 1024,
        'learning_rate': 0.0001,
        'warmup_steps': 4000,
        'max_grad_norm': 1.0,
        'label_smoothing': 0.1,
        'pad_idx': 0,
        'weight_decay': 0.0001
    }
    
    # 创建模型
    model = Transformer(
        src_vocab_size=len(src_vocab),
        tgt_vocab_size=len(tgt_vocab),
        d_model=config['d_model'],
        n_heads=config['n_heads'],
        n_layers=config['n_layers'],
        d_ff=config['d_ff']
    )
    
    # 设备
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 创建训练器
    trainer = TransformerTrainer(model, train_loader, val_loader, device, config)
    
    print("=== 训练配置 ===")
    print(f"源词汇表大小: {len(src_vocab)}")
    print(f"目标词汇表大小: {len(tgt_vocab)}")
    print(f"训练样本数: {len(train_dataset)}")
    print(f"验证样本数: {len(val_dataset)}")
    print(f"模型参数量: {sum(p.numel() for p in model.parameters()):,}")
    print(f"设备: {device}")
    
    # 开始训练（演示版本，实际训练需要更多epochs）
    trainer.train(num_epochs=2)
    
    return trainer, model, src_vocab, tgt_vocab

# 运行训练演示
# trainer, trained_model, src_vocab, tgt_vocab = demonstrate_training()
```

### 4.2 推理与文本生成

训练完成后，我们需要实现推理功能，包括贪婪解码、束搜索等生成策略。

```python
class TransformerInference:
    """Transformer推理器"""
    
    def __init__(self, model, src_vocab, tgt_vocab, device, max_len=100):
        self.model = model.to(device)
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        self.device = device
        self.max_len = max_len
        
        # 创建反向词汇表
        self.src_idx2token = {v: k for k, v in src_vocab.items()}
        self.tgt_idx2token = {v: k for k, v in tgt_vocab.items()}
        
        # 特殊token
        self.pad_idx = tgt_vocab.get('<pad>', 0)
        self.bos_idx = tgt_vocab.get('<bos>', 1)
        self.eos_idx = tgt_vocab.get('<eos>', 2)
        self.unk_idx = tgt_vocab.get('<unk>', 3)
    
    def encode_sentence(self, sentence):
        """编码输入句子"""
        tokens = sentence.split()
        token_ids = [self.src_vocab.get(token, self.unk_idx) for token in tokens]
        
        # 添加填充
        if len(token_ids) < self.max_len:
            token_ids += [self.pad_idx] * (self.max_len - len(token_ids))
        else:
            token_ids = token_ids[:self.max_len]
        
        return torch.tensor(token_ids, dtype=torch.long).unsqueeze(0).to(self.device)
    
    def decode_sequence(self, token_ids):
        """解码token序列为文本"""
        tokens = []
        for token_id in token_ids:
            if token_id == self.eos_idx:
                break
            if token_id not in [self.pad_idx, self.bos_idx]:
                tokens.append(self.tgt_idx2token.get(token_id, '<unk>'))
        return ' '.join(tokens)
    
    def greedy_decode(self, src_sentence):
        """贪婪解码"""
        self.model.eval()
        
        with torch.no_grad():
            # 编码输入
            src = self.encode_sentence(src_sentence)
            src_mask = create_padding_mask(src, self.pad_idx)
            
            # 编码
            encoder_output, _ = self.model.encode(src, src_mask)
            
            # 初始化解码器输入
            tgt = torch.tensor([[self.bos_idx]], dtype=torch.long).to(self.device)
            
            # 逐步生成
            for _ in range(self.max_len - 1):
                tgt_mask = create_causal_mask(tgt.size(1)).to(self.device)
                
                # 解码
                output, _, _ = self.model.decode(tgt, encoder_output, src_mask, tgt_mask)
                
                # 获取下一个token
                next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)
                
                # 添加到序列
                tgt = torch.cat([tgt, next_token], dim=1)
                
                # 如果生成了结束token，停止
                if next_token.item() == self.eos_idx:
                    break
            
            return self.decode_sequence(tgt.squeeze().cpu().tolist())
    
    def beam_search(self, src_sentence, beam_size=4, alpha=0.7):
        """束搜索解码"""
        self.model.eval()
        
        with torch.no_grad():
            # 编码输入
            src = self.encode_sentence(src_sentence)
            src_mask = create_padding_mask(src, self.pad_idx)
            encoder_output, _ = self.model.encode(src, src_mask)
            
            # 初始化束
            beams = [(torch.tensor([[self.bos_idx]], dtype=torch.long).to(self.device), 0.0)]
            completed_beams = []
            
            for step in range(self.max_len - 1):
                candidates = []
                
                for seq, score in beams:
                    if seq[0, -1].item() == self.eos_idx:
                        completed_beams.append((seq, score))
                        continue
                    
                    tgt_mask = create_causal_mask(seq.size(1)).to(self.device)
                    output, _, _ = self.model.decode(seq, encoder_output, src_mask, tgt_mask)
                    
                    # 获取词汇表上的概率分布
                    probs = F.log_softmax(output[:, -1, :], dim=-1)
                    
                    # 获取top-k候选
                    top_k_probs, top_k_indices = torch.topk(probs, beam_size)
                    
                    for i in range(beam_size):
                        new_seq = torch.cat([seq, top_k_indices[:, i:i+1]], dim=1)
                        new_score = score + top_k_probs[0, i].item()
                        candidates.append((new_seq, new_score))
                
                # 选择最佳候选
                candidates.sort(key=lambda x: x[1] / (len(x[0][0]) ** alpha), reverse=True)
                beams = candidates[:beam_size]
                
                # 如果所有束都完成了，停止
                if len(completed_beams) >= beam_size:
                    break
            
            # 选择最佳完成的序列
            if completed_beams:
                best_seq, _ = max(completed_beams, 
                                key=lambda x: x[1] / (len(x[0][0]) ** alpha))
            else:
                best_seq, _ = max(beams, 
                                key=lambda x: x[1] / (len(x[0][0]) ** alpha))
            
            return self.decode_sequence(best_seq.squeeze().cpu().tolist())
    
    def translate_batch(self, sentences, method='greedy'):
        """批量翻译"""
        translations = []
        
        for sentence in sentences:
            if method == 'greedy':
                translation = self.greedy_decode(sentence)
            elif method == 'beam_search':
                translation = self.beam_search(sentence)
            else:
                raise ValueError(f"Unsupported method: {method}")
            
            translations.append(translation)
        
        return translations

def demonstrate_inference():
    """演示推理过程"""
    
    # 创建示例数据和词汇表
    src_sentences, tgt_sentences, src_vocab, tgt_vocab = create_sample_data()
    
    # 创建一个简单的模型用于演示
    model = Transformer(
        src_vocab_size=len(src_vocab),
        tgt_vocab_size=len(tgt_vocab),
        d_model=256,
        n_heads=8,
        n_layers=4,
        d_ff=1024
    )
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 创建推理器
    inference = TransformerInference(model, src_vocab, tgt_vocab, device)
    
    # 测试句子
    test_sentences = [
        "hello world",
        "how are you",
        "good morning",
        "thank you very much"
    ]
    
    print("=== 推理演示 ===")
    print("注意：由于模型未经训练，输出可能是随机的")
    
    for sentence in test_sentences:
        print(f"\n输入: {sentence}")
        
        # 贪婪解码
        greedy_result = inference.greedy_decode(sentence)
        print(f"贪婪解码: {greedy_result}")
        
        # 束搜索解码
        beam_result = inference.beam_search(sentence, beam_size=4)
        print(f"束搜索解码: {beam_result}")
    
    return inference

# 运行推理演示
inference_demo = demonstrate_inference()
```

### 4.3 模型评估与分析

为了评估Transformer模型的性能，我们需要实现各种评估指标和分析工具。

```python
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

class TransformerEvaluator:
    """Transformer模型评估器"""
    
    def __init__(self, model, inference, test_data):
        self.model = model
        self.inference = inference
        self.test_data = test_data
    
    def calculate_bleu(self, reference, candidate, n=4):
        """计算BLEU分数"""
        def get_ngrams(tokens, n):
            return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
        
        ref_tokens = reference.split()
        cand_tokens = candidate.split()
        
        if len(cand_tokens) == 0:
            return 0.0
        
        # 计算各阶n-gram的精确度
        precisions = []
        for i in range(1, n+1):
            ref_ngrams = Counter(get_ngrams(ref_tokens, i))
            cand_ngrams = Counter(get_ngrams(cand_tokens, i))
            
            if len(cand_ngrams) == 0:
                precisions.append(0.0)
                continue
            
            matches = sum((ref_ngrams & cand_ngrams).values())
            total = sum(cand_ngrams.values())
            precisions.append(matches / total)
        
        # 计算几何平均
        if any(p == 0 for p in precisions):
            return 0.0
        
        geo_mean = np.exp(np.mean(np.log(precisions)))
        
        # 简化的长度惩罚
        bp = min(1.0, len(cand_tokens) / len(ref_tokens)) if len(ref_tokens) > 0 else 0.0
        
        return bp * geo_mean
    
    def evaluate_model(self, num_samples=100):
        """评估模型性能"""
        
        # 随机选择测试样本
        indices = np.random.choice(len(self.test_data), min(num_samples, len(self.test_data)), replace=False)
        
        bleu_scores = []
        exact_matches = 0
        
        print("评估模型性能...")
        
        for idx in tqdm(indices):
            src_sentence = self.test_data[idx]['src']
            ref_sentence = self.test_data[idx]['tgt']
            
            # 生成翻译
            pred_sentence = self.inference.greedy_decode(src_sentence)
            
            # 计算BLEU分数
            bleu = self.calculate_bleu(ref_sentence, pred_sentence)
            bleu_scores.append(bleu)
            
            # 检查完全匹配
            if pred_sentence.strip() == ref_sentence.strip():
                exact_matches += 1
        
        # 计算统计指标
        avg_bleu = np.mean(bleu_scores)
        std_bleu = np.std(bleu_scores)
        exact_match_rate = exact_matches / len(indices)
        
        results = {
            'avg_bleu': avg_bleu,
            'std_bleu': std_bleu,
            'exact_match_rate': exact_match_rate,
            'bleu_scores': bleu_scores,
            'num_samples': len(indices)
        }
        
        return results
    
    def analyze_attention_patterns(self, sentence, layer_idx=0, head_idx=0):
        """分析注意力模式"""
        self.model.eval()
        
        with torch.no_grad():
            # 编码输入
            src = self.inference.encode_sentence(sentence)
            src_mask = create_padding_mask(src, self.inference.pad_idx)
            
            # 获取编码器输出和注意力权重
            encoder_output, encoder_attention = self.model.encode(src, src_mask)
            
            # 提取指定层和头的注意力权重
            attention_weights = encoder_attention[layer_idx][0, head_idx].cpu().numpy()
            
            # 获取有效token
            tokens = sentence.split()
            valid_len = len(tokens)
            
            # 可视化注意力权重
            plt.figure(figsize=(10, 8))
            sns.heatmap(
                attention_weights[:valid_len, :valid_len],
                xticklabels=tokens,
                yticklabels=tokens,
                cmap='Blues',
                annot=True,
                fmt='.2f'
            )
            plt.title(f'Attention Pattern (Layer {layer_idx}, Head {head_idx})')
            plt.xlabel('Key Positions')
            plt.ylabel('Query Positions')
            plt.tight_layout()
            plt.show()
            
            return attention_weights[:valid_len, :valid_len]
    
    def generate_evaluation_report(self, results):
        """生成评估报告"""
        
        print("=== 模型评估报告 ===")
        print(f"评估样本数: {results['num_samples']}")
        print(f"平均BLEU分数: {results['avg_bleu']:.4f} ± {results['std_bleu']:.4f}")
        print(f"完全匹配率: {results['exact_match_rate']:.2%}")
        
        # BLEU分数分布
        plt.figure(figsize=(10, 6))
        plt.hist(results['bleu_scores'], bins=20, alpha=0.7, edgecolor='black')
        plt.axvline(results['avg_bleu'], color='red', linestyle='--', 
                   label=f'平均值: {results["avg_bleu"]:.4f}')
        plt.xlabel('BLEU Score')
        plt.ylabel('Frequency')
        plt.title('BLEU Score Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        
        # 性能分析
        print(f"\n=== 性能分析 ===")
        high_quality = sum(1 for score in results['bleu_scores'] if score > 0.5)
        medium_quality = sum(1 for score in results['bleu_scores'] if 0.2 <= score <= 0.5)
        low_quality = sum(1 for score in results['bleu_scores'] if score < 0.2)
        
        print(f"高质量翻译 (BLEU > 0.5): {high_quality} ({high_quality/results['num_samples']:.1%})")
        print(f"中等质量翻译 (0.2 ≤ BLEU ≤ 0.5): {medium_quality} ({medium_quality/results['num_samples']:.1%})")
        print(f"低质量翻译 (BLEU < 0.2): {low_quality} ({low_quality/results['num_samples']:.1%})")

def demonstrate_evaluation():
    """演示评估过程"""
    
    # 创建测试数据
    src_sentences, tgt_sentences, src_vocab, tgt_vocab = create_sample_data()
    
    test_data = [
        {'src': src, 'tgt': tgt} 
        for src, tgt in zip(src_sentences[:50], tgt_sentences[:50])
    ]
    
    # 创建模型和推理器
    model = Transformer(
        src_vocab_size=len(src_vocab),
        tgt_vocab_size=len(tgt_vocab),
        d_model=256,
        n_heads=8,
        n_layers=4,
        d_ff=1024
    )
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    inference = TransformerInference(model, src_vocab, tgt_vocab, device)
    
    # 创建评估器
    evaluator = TransformerEvaluator(model, inference, test_data)
    
    # 评估模型
    results = evaluator.evaluate_model(num_samples=20)
    
    # 生成报告
    evaluator.generate_evaluation_report(results)
    
    # 分析注意力模式
    print("\n=== 注意力模式分析 ===")
    sample_sentence = "hello world"
    attention_pattern = evaluator.analyze_attention_patterns(sample_sentence)
    
    return evaluator, results

# 运行评估演示
# evaluator, eval_results = demonstrate_evaluation()
```

### 4.4 实际应用与优化建议

在实际应用中，Transformer模型还需要考虑许多工程和优化问题。

```python
class TransformerOptimizer:
    """Transformer优化器"""
    
    @staticmethod
    def model_size_analysis(model):
        """分析模型大小和复杂度"""
        
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # 按组件分析参数分布
        component_params = {}
        
        if hasattr(model, 'encoder'):
            encoder_params = sum(p.numel() for p in model.encoder.parameters())
            component_params['encoder'] = encoder_params
        
        if hasattr(model, 'decoder'):
            decoder_params = sum(p.numel() for p in model.decoder.parameters())
            component_params['decoder'] = decoder_params
        
        # 内存估算
        model_size_mb = total_params * 4 / (1024 * 1024)  # FP32
        model_size_mb_fp16 = total_params * 2 / (1024 * 1024)  # FP16
        
        analysis = {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'component_parameters': component_params,
            'model_size_mb_fp32': model_size_mb,
            'model_size_mb_fp16': model_size_mb_fp16,
            'estimated_training_memory_gb': model_size_mb * 4 / 1024,  # 考虑梯度和优化器状态
        }
        
        return analysis
    
    @staticmethod
    def suggest_hyperparameters(dataset_size, vocab_size, task_complexity='medium'):
        """根据数据集特征建议超参数"""
        
        suggestions = {}
        
        # 根据数据集大小调整模型规模
        if dataset_size < 10000:
            suggestions.update({
                'd_model': 256,
                'n_heads': 4,
                'n_layers': 4,
                'd_ff': 1024,
                'batch_size': 32,
                'learning_rate': 0.001
            })
        elif dataset_size < 100000:
            suggestions.update({
                'd_model': 512,
                'n_heads': 8,
                'n_layers': 6,
                'd_ff': 2048,
                'batch_size': 64,
                'learning_rate': 0.0005
            })
        else:
            suggestions.update({
                'd_model': 768,
                'n_heads': 12,
                'n_layers': 8,
                'd_ff': 3072,
                'batch_size': 128,
                'learning_rate': 0.0001
            })
        
        # 根据词汇表大小调整
        if vocab_size > 50000:
            suggestions['d_model'] = min(1024, suggestions['d_model'] * 1.5)
        
        # 根据任务复杂度调整
        complexity_multipliers = {
            'simple': 0.8,
            'medium': 1.0,
            'complex': 1.2
        }
        
        multiplier = complexity_multipliers.get(task_complexity, 1.0)
        suggestions['n_layers'] = int(suggestions['n_layers'] * multiplier)
        suggestions['d_ff'] = int(suggestions['d_ff'] * multiplier)
        
        # 训练相关建议
        suggestions.update({
            'warmup_steps': min(4000, dataset_size // 10),
            'max_grad_norm': 1.0,
            'dropout': 0.1,
            'label_smoothing': 0.1,
            'weight_decay': 0.0001
        })
        
        return suggestions
    
    @staticmethod
    def optimization_checklist():
        """优化检查清单"""
        
        checklist = {
            'data_preprocessing': [
                "✓ 数据清洗和去重",
                "✓ 适当的序列长度设置",
                "✓ 词汇表大小优化",
                "✓ 数据增强（如果适用）"
            ],
            'model_architecture': [
                "✓ 合适的模型规模选择",
                "✓ 层归一化位置优化",
                "✓ 激活函数选择",
                "✓ 权重初始化策略"
            ],
            'training_optimization': [
                "✓ 学习率调度策略",
                "✓ 批次大小优化",
                "✓ 梯度裁剪设置",
                "✓ 混合精度训练",
                "✓ 梯度累积"
            ],
            'inference_optimization': [
                "✓ 模型量化",
                "✓ 动态批处理",
                "✓ 缓存优化",
                "✓ 并行推理"
            ],
            'monitoring': [
                "✓ 训练损失监控",
                "✓ 验证指标跟踪",
                "✓ 注意力权重可视化",
                "✓ 梯度范数监控"
            ]
        }
        
        return checklist

def comprehensive_transformer_demo():
    """综合Transformer演示"""
    
    print("=== 综合Transformer演示 ===\n")
    
    # 1. 数据准备
    print("1. 数据准备")
    src_sentences, tgt_sentences, src_vocab, tgt_vocab = create_sample_data()
    print(f"   源语言词汇表大小: {len(src_vocab)}")
    print(f"   目标语言词汇表大小: {len(tgt_vocab)}")
    print(f"   训练样本数: {len(src_sentences)}")
    
    # 2. 超参数建议
    print("\n2. 超参数建议")
    suggestions = TransformerOptimizer.suggest_hyperparameters(
        len(src_sentences), len(src_vocab), 'medium'
    )
    print("   推荐超参数:")
    for key, value in suggestions.items():
        print(f"     {key}: {value}")
    
    # 3. 模型创建
    print("\n3. 模型创建")
    model = Transformer(
        src_vocab_size=len(src_vocab),
        tgt_vocab_size=len(tgt_vocab),
        d_model=suggestions['d_model'],
        n_heads=suggestions['n_heads'],
        n_layers=suggestions['n_layers'],
        d_ff=suggestions['d_ff']
    )
    
    # 4. 模型分析
    print("\n4. 模型分析")
    analysis = TransformerOptimizer.model_size_analysis(model)
    print(f"   总参数量: {analysis['total_parameters']:,}")
    print(f"   模型大小 (FP32): {analysis['model_size_mb_fp32']:.2f} MB")
    print(f"   模型大小 (FP16): {analysis['model_size_mb_fp16']:.2f} MB")
    print(f"   估计训练内存: {analysis['estimated_training_memory_gb']:.2f} GB")
    
    # 5. 优化检查清单
    print("\n5. 优化检查清单")
    checklist = TransformerOptimizer.optimization_checklist()
    for category, items in checklist.items():
        print(f"   {category.replace('_', ' ').title()}:")
        for item in items:
            print(f"     {item}")
    
    # 6. 实际建议
    print("\n6. 实际应用建议")
    print("   • 从小模型开始，逐步增加复杂度")
    print("   • 使用预训练模型进行迁移学习")
    print("   • 实施早停和模型检查点")
    print("   • 监控过拟合和欠拟合")
    print("   • 考虑使用知识蒸馏减小模型")
    print("   • 实施A/B测试评估改进效果")
    
    return model, suggestions, analysis

# 运行综合演示
final_model, final_suggestions, final_analysis = comprehensive_transformer_demo()
```

### 4.5 总结与展望

**Transformer的核心价值**：
1. **并行化能力**：相比RNN的串行计算，Transformer实现了高度并行化
2. **长距离依赖建模**：通过自注意力机制有效捕捉长距离依赖关系
3. **架构统一性**：为各种NLP任务提供了统一的架构框架
4. **可扩展性**：随着规模增长展现出强大的性能提升潜力

**实践要点总结**：
- 注意力机制是核心，理解其数学原理至关重要
- 位置编码解决了序列位置信息的问题
- 残差连接和层归一化保证了深层网络的训练稳定性
- 合适的超参数选择对模型性能影响巨大
- 评估和优化是持续改进的关键

**未来发展方向**：
- **效率优化**：稀疏注意力、线性注意力等降低计算复杂度
- **架构创新**：混合架构、多模态扩展等
- **应用拓展**：从NLP扩展到计算机视觉、语音等领域
- **理论理解**：深入理解Transformer的工作机制和表示能力

Transformer架构的成功不仅推动了NLP领域的快速发展，更为整个人工智能领域提供了重要启示。掌握Transformer的原理和实现，是理解现代深度学习模型的重要基础，也是进入大模型时代的必备技能。