# 5. 按模型分类的最佳实践

## 5.1 Llama 3 / 3.1 微调坑点与技巧

Llama 3系列模型是目前最受欢迎的开源大模型之一，其优秀的性能和相对较小的参数量使其成为微调的热门选择。然而，在实际微调过程中，Llama 3也有一些需要特别注意的地方。

**Llama 3的架构特点**：Llama 3采用了RMSNorm归一化、SwiGLU激活函数、RoPE位置编码等先进技术。这些设计虽然提升了模型性能，但也对微调策略提出了特殊要求。特别是RoPE位置编码对序列长度比较敏感，在处理超长文本时需要特别注意。

### 关键微调技巧

首先是**学习率设置**。Llama 3对学习率比较敏感，推荐使用较小的学习率。对于LoRA微调，建议学习率设置在1e-4到5e-5之间；对于全参数微调，建议使用1e-5到5e-6。学习率过大容易导致训练不稳定，甚至出现loss爆炸的情况。

其次是**序列长度处理**。Llama 3的训练序列长度为4096，但在微调时可以根据任务需求调整。如果任务涉及长文本处理，建议逐步增加序列长度，而不是一开始就使用最大长度，这样可以提高训练稳定性。

第三是**tokenizer的特殊处理**。Llama 3使用了特殊的tokenizer，在处理中文时可能存在效率问题。建议在微调前先测试tokenizer的编码效果，必要时可以考虑扩展词汇表。

### 常见坑点及解决方案

**坑点一：梯度爆炸**。Llama 3在某些情况下容易出现梯度爆炸，特别是在使用较大学习率或处理长序列时。解决方案是设置合适的梯度裁剪阈值（建议1.0），并使用warmup策略。

**坑点二：显存不足**。Llama 3-8B模型在全精度下需要约32GB显存，即使使用fp16也需要16GB以上。对于显存受限的情况，建议使用梯度检查点、DeepSpeed ZeRO等技术，或者采用QLoRA等量化微调方法。

**坑点三：收敛缓慢**。有时会遇到loss下降缓慢的情况，这通常是由于学习率过小或数据质量问题导致的。可以尝试适当增加学习率，或者检查数据预处理是否正确。

```python
# Llama 3微调的推荐配置
llama3_config = {
    "model_name": "meta-llama/Meta-Llama-3-8B",
    "lora_config": {
        "r": 16,
        "lora_alpha": 32,
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "lora_dropout": 0.05,
        "bias": "none",
        "task_type": "CAUSAL_LM"
    },
    "training_args": {
        "learning_rate": 2e-5,
        "per_device_train_batch_size": 4,
        "gradient_accumulation_steps": 4,
        "num_train_epochs": 3,
        "warmup_ratio": 0.1,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "fp16": True,
        "gradient_checkpointing": True
    }
}
```

## 5.2 Qwen (通义千问) 系列实战

Qwen系列模型是阿里巴巴推出的大语言模型，在中文处理方面表现优异。Qwen模型的一个显著特点是对中文的优化，以及相对较好的指令跟随能力。

**Qwen的独特优势**：Qwen在中文tokenizer方面做了大量优化，中文编码效率比Llama等模型高出不少。同时，Qwen在预训练阶段就加入了大量的中文指令数据，因此在中文任务上的微调效果通常更好。

### 微调策略建议

对于**中文任务**，Qwen通常只需要较少的微调数据就能达到不错的效果。建议使用相对较小的学习率（1e-5到5e-5），并且可以适当减少训练轮数。

对于**多轮对话任务**，Qwen的表现特别出色。在微调时，建议使用特殊的对话格式，并且注意保持对话的连贯性。可以使用专门的对话数据集进行微调。

### 实战经验分享

在实际项目中，我们发现Qwen在以下几个方面表现突出：一是中文理解能力强，特别是对于古诗词、成语等传统文化内容；二是数学推理能力相对较好，在涉及计算的任务中表现不错；三是代码生成能力也比较强，特别是Python代码。

但是，Qwen也有一些需要注意的地方：一是在英文任务上的表现可能不如专门针对英文优化的模型；二是在某些创意写作任务中，可能会显得比较"刻板"；三是模型的幻觉问题在某些情况下比较明显。

```python
# Qwen微调的推荐配置
qwen_config = {
    "model_name": "Qwen/Qwen-7B-Chat",
    "lora_config": {
        "r": 8,  # Qwen通常用较小的rank就能取得好效果
        "lora_alpha": 16,
        "target_modules": ["c_attn", "c_proj", "w1", "w2"],
        "lora_dropout": 0.1,
        "bias": "none"
    },
    "training_args": {
        "learning_rate": 1e-5,  # 相对较小的学习率
        "per_device_train_batch_size": 8,
        "gradient_accumulation_steps": 2,
        "num_train_epochs": 2,  # 通常2-3轮就够了
        "warmup_ratio": 0.05,
        "weight_decay": 0.01,
        "fp16": True
    }
}
```

## 5.3 Mistral / Gemma 等其他模型

除了Llama和Qwen，还有许多其他优秀的开源模型值得关注。Mistral 7B以其出色的性能和相对较小的参数量受到广泛关注；Gemma系列是Google推出的开源模型，在某些任务上表现优异。

**Mistral的特点**：Mistral采用了滑动窗口注意力机制，能够高效处理长序列。在微调时，Mistral通常收敛较快，对超参数的敏感性相对较低。推荐使用中等大小的学习率（2e-5到5e-5），并且可以使用相对较大的批次大小。

**Gemma的特点**：Gemma模型在安全性方面做了特殊优化，内置了一些安全过滤机制。在微调时需要注意这些机制可能会影响某些任务的性能。建议在微调前先测试模型在目标任务上的基础表现。

**选择建议**：在选择模型时，建议考虑以下因素：任务类型（中文任务优选Qwen，英文任务可选Llama或Mistral）；资源限制（显存受限时优选较小的模型）；性能要求（追求极致性能时可选择较大的模型）；部署环境（边缘部署优选轻量化模型）。

