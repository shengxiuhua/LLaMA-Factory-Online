
## 4. 文本表示技术的演进

文本表示的目的是将人类语言的自然形式转化为计算机可以处理的形式，也就是将文本数据数字化，使计算机能够对文本进行有效的分析和处理。文本表示是NLP领域的基础性工作，它直接影响甚至决定着NLP系统的质量和性能。

在NLP中，文本表示涉及将文本中的语言单位（如字、词、短语、句子等）以及它们之间的关系和结构信息转换为计算机能够理解和操作的形式，例如向量、矩阵或其他数据结构。这样的表示不仅需要保留足够的语义信息以便于后续的NLP任务（如文本分类、情感分析、机器翻译等），还需要考虑计算效率和存储效率。

文本表示技术经历了多个发展阶段，从早期的基于规则的方法，到统计学习方法，再到当前的深度学习技术，文本表示技术不断演进，为NLP的发展提供了强大支持。

### 4.1 词向量

向量空间模型（Vector Space Model, VSM）是NLP领域中一个基础且强大的文本表示方法，最早由哈佛大学Salton提出。向量空间模型通过将文本（包括单词、句子、段落或整个文档）转换为高维空间中的向量来实现文本的数学化表示。在这个模型中，每个维度代表一个特征项（例如字、词、词组或短语），而向量中的每个元素值代表该特征项在文本中的权重。这种权重通过特定的计算公式（如词频TF、逆文档频率TF-IDF等）来确定，反映了特征项在文本中的重要程度。

向量空间模型应用极其广泛，包括文本相似度计算、文本分类、信息检索等自然语言处理任务。它将复杂的文本数据转换为易于计算和分析的数学形式，使得文本的相似度计算和模式识别成为可能。此外，通过矩阵运算如特征值计算、奇异值分解（SVD）等方法，可以优化文本向量表示，进一步提升处理效率和效果。

然而，向量空间模型也存在明显问题。最主要的是数据稀疏性和维度灾难问题——由于特征项数量庞大导致向量维度极高，同时多数元素值为零。此外，由于模型基于特征项之间的独立性假设，忽略了文本中的结构信息，如词序和上下文信息，限制了模型的表现力。

```
VSM方法词向量示例：
句子："深度学习改变了人工智能"
词汇表大小：20000
句子包含词汇：["深度", "学习", "改变", "了", "人工智能"] = 5个词

向量表示：
vector = [0, 0, ..., 1, 0, ..., 1, 0, ..., 1, 0, ..., 1, 0, ..., 1, 0, ...]
                    ↑          ↑          ↑          ↑          ↑
# 20000维中只有5个位置为1，其余19995个位置为0
# 实际有效维度：仅5维（非零维度）
# 稀疏率：(20000-5)/20000 ≈ 99.975%
```

词汇表是一个包含所有可能出现词语的集合。在向量空间模型中，每个词对应词汇表中的一个位置，通过这种方式可以将词语转换为向量表示。例如，如果词汇表大小为20000，那么每个词都会被表示为一个20000维的向量，其中只有该词对应的位置为1，其他位置都为0。

为了解决这些问题，研究者们的工作主要集中在两个方面：一是改进特征表示方法，如借助图方法、主题方法等进行关键词抽取；二是改进和优化特征项权重的计算方法，可以在现有方法基础上进行融合计算或提出新的计算方法。

### 4.2 语言模型

N-gram模型是NLP领域中一种基于统计的语言模型，广泛应用于语音识别、手写识别、拼写纠错、机器翻译和搜索引擎等众多任务。N-gram模型的核心思想基于马尔可夫假设，即一个词的出现概率仅依赖于它前面的N-1个词。这里的N代表连续出现单词的数量，可以是任意正整数。例如，当N=1时，模型称为unigram，仅考虑单个词的概率；当N=2时，称为bigram，考虑前一个词来估计当前词的概率；当N=3时，称为trigram，考虑前两个词来估计第三个词的概率，以此类推。

N-gram模型通过条件概率链式规则来估计整个句子的概率。具体而言，对于给定的一个句子，模型会计算每个N-gram出现的条件概率，并将这些概率相乘以得到整个句子的概率。例如，对于句子"Deep learning is powerful"，作为trigram模型，我们会计算P("learning" | "Deep")、P("is" | "Deep", "learning")、P("powerful" | "learning", "is")等概率，并将它们相乘。

N-gram的优点是实现简单、容易理解，在许多任务中效果不错。但当N较大时，会出现数据稀疏性问题。模型的参数空间会急剧增大，相同的N-gram序列出现的概率变得非常低，导致模型无法有效学习，模型泛化能力下降。此外，N-gram模型忽略了词之间的长距离依赖关系，无法捕捉到句子中的复杂结构和语义信息。

尽管存在局限性，N-gram模型由于其简单性和实用性，在许多NLP任务中仍然被广泛使用。在某些应用中，结合N-gram模型和其他技术（如深度学习模型）可以获得更好的性能。

### 4.3 Word2Vec

Word2Vec是一种流行的词嵌入（Word Embedding）技术，由Tomas Mikolov等人在2013年提出。它是一种基于神经网络的语言模型，旨在通过学习词与词之间的上下文关系来生成词的密集向量表示。Word2Vec的核心思想是利用词在文本中的上下文信息来捕捉词之间的语义关系，从而使得语义相似或相关的词在向量空间中距离较近。

Word2Vec模型主要有两种架构：

- **连续词袋模型（CBOW, Continuous Bag of Words）**：根据目标词上下文中的词对应的词向量，计算并输出目标词的向量表示
- **Skip-Gram模型**：与CBOW相反，利用目标词的向量表示计算上下文中的词向量

实践验证CBOW适用于小型数据集，而Skip-Gram在大型语料中表现更好。

相比于传统的高维稀疏表示（如One-Hot编码），Word2Vec生成的是低维（通常几百维）的密集向量，有助于减少计算复杂度和存储需求。Word2Vec模型能够捕捉到词与词之间的语义关系，比如"医生"和"护士"在向量空间中的位置会比较接近，因为在大量文本中，它们通常会出现在相似的上下文中。Word2Vec模型也可以很好地泛化到未见过的词，因为它是基于上下文信息学习的，而不是基于词典。但由于CBOW/Skip-Gram模型是基于局部上下文的，无法捕捉到长距离的依赖关系，缺乏整体的词与词之间的关系，因此在一些复杂的语义任务上表现不佳。

### 4.4 ELMo

ELMo（Embeddings from Language Models）实现了从静态词向量到动态词向量的跨越式转变，解决了一词多义的问题。它首先在大型语料库上训练语言模型得到词向量模型，然后在特定任务上对模型进行微调，得到更适合该任务的词向量。ELMo首次将预训练思想引入到词向量生成中，使用双向LSTM结构，能够捕捉到词汇的上下文信息，生成更加丰富和准确的词向量表示。

ELMo采用典型的两阶段过程：第一阶段是利用语言模型进行预训练；第二阶段是在做特定任务时，从预训练网络中提取对应单词的词向量作为新特征补充到下游任务中。基于RNN的LSTM模型训练时间长，特征提取是ELMo模型优化和提升的关键。

```
示例：ELMo处理一词多义

句子1："我去银行存钱。"
句子2："我在河岸边散步。"

在传统Word2Vec中，"银行"和"河岸"的"行"字会有相同的向量表示。
但在ELMo中：
- 句子1中的"银行"会根据"存钱"等上下文生成偏向金融机构的向量
- 句子2中的"河岸"会根据"散步"等上下文生成偏向地理位置的向量
```

ELMo模型的主要优势在于其能够捕捉到词汇的多义性和上下文信息，生成的词向量更加丰富和准确，适用于多种NLP任务。然而，ELMo模型也存在一些问题，如模型复杂度高、训练时间长、计算资源消耗大等。尽管如此，ELMo的出现标志着NLP进入了预训练模型时代，为后续BERT、GPT等更强大的预训练模型奠定了基础。

