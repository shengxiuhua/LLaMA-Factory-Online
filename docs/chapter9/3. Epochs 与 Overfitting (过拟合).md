# 3. Epochs ä¸ Overfitting (è¿‡æ‹Ÿåˆ)

## 3.1 ç†è§£è¿‡æ‹Ÿåˆä¸è®­ç»ƒè½®æ•°

### ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿ

è¿‡æ‹Ÿåˆæ˜¯æŒ‡æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šè¡¨ç°ä¸ä½³çš„ç°è±¡ã€‚è¿™å°±åƒä¸€ä¸ªå­¦ç”Ÿæ­»è®°ç¡¬èƒŒè€ƒè¯•é¢˜ç›®çš„ç­”æ¡ˆï¼Œå´æ²¡æœ‰çœŸæ­£ç†è§£çŸ¥è¯†ç‚¹ï¼Œé‡åˆ°æ–°é¢˜ç›®å°±ä¸ä¼šåšäº†ã€‚

åœ¨å¤§æ¨¡å‹å¾®è°ƒä¸­ï¼Œç”±äºæ¨¡å‹å®¹é‡å¤§è€Œè®­ç»ƒæ•°æ®ç›¸å¯¹è¾ƒå°‘ï¼Œè¿‡æ‹Ÿåˆæ˜¯ä¸€ä¸ªå¸¸è§ä¸”ä¸¥é‡çš„é—®é¢˜ã€‚è®­ç»ƒè½®æ•°ï¼ˆEpochsï¼‰çš„é€‰æ‹©éœ€è¦åœ¨å……åˆ†å­¦ä¹ å’Œé¿å…è¿‡æ‹Ÿåˆä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚

**è¿‡æ‹Ÿåˆçš„å…¸å‹è¡¨ç°**ï¼š

1. **æŸå¤±å‡½æ•°è¡¨ç°**ï¼šè®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ï¼Œä½†éªŒè¯æŸå¤±å¼€å§‹ä¸Šå‡æˆ–åœæ»
2. **å‡†ç¡®ç‡è¡¨ç°**ï¼šè®­ç»ƒå‡†ç¡®ç‡å¾ˆé«˜ï¼ˆç”šè‡³æ¥è¿‘100%ï¼‰ï¼Œä½†éªŒè¯å‡†ç¡®ç‡è¿œä½äºè®­ç»ƒå‡†ç¡®ç‡
3. **æ³›åŒ–èƒ½åŠ›å·®**ï¼šæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å®Œç¾ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°ä¸ä½³
4. **å¯¹å™ªå£°æ•æ„Ÿ**ï¼šæ¨¡å‹å­¦ä¹ äº†è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°å’Œå¼‚å¸¸å€¼ï¼Œè€Œä¸æ˜¯çœŸæ­£çš„æ¨¡å¼

### å¦‚ä½•åˆ¤æ–­å’Œæ£€æµ‹è¿‡æ‹Ÿåˆï¼Ÿ

**ç›‘æ§æŒ‡æ ‡**ï¼š
- **è®­ç»ƒæŸå¤± vs éªŒè¯æŸå¤±**ï¼šä¸¤è€…å·®è·è¶Šå¤§ï¼Œè¿‡æ‹Ÿåˆè¶Šä¸¥é‡
- **æŸå¤±æ›²çº¿è¶‹åŠ¿**ï¼šè®­ç»ƒæŸå¤±ä¸‹é™ä½†éªŒè¯æŸå¤±ä¸Šå‡æ˜¯æ˜æ˜¾çš„è¿‡æ‹Ÿåˆä¿¡å·
- **æ€§èƒ½å·®è·**ï¼šéªŒè¯é›†æ€§èƒ½æ˜¾è‘—ä½äºè®­ç»ƒé›†æ€§èƒ½ï¼ˆé€šå¸¸è¶…è¿‡30%çš„å·®è·ï¼‰

**ä¸‰ç§å…¸å‹çš„è®­ç»ƒæ›²çº¿**ï¼š

1. **æ­£å¸¸è®­ç»ƒ**ï¼šè®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±éƒ½ç¨³å®šä¸‹é™ï¼Œæœ€ç»ˆè¶‹äºå¹³ç¨³ï¼Œä¸¤è€…å·®è·ä¸å¤§ï¼ˆ<10%ï¼‰
2. **è¿‡æ‹Ÿåˆ**ï¼šè®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ï¼Œä½†éªŒè¯æŸå¤±åœ¨æŸä¸ªç‚¹åå¼€å§‹ä¸Šå‡ï¼Œå½¢æˆ"å‰ªåˆ€å·®"
3. **æ¬ æ‹Ÿåˆ**ï¼šè®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±éƒ½å¾ˆé«˜ï¼Œä¸‹é™ç¼“æ…¢ï¼Œæ¨¡å‹å­¦ä¹ èƒ½åŠ›ä¸è¶³

**å®ç”¨çš„è¿‡æ‹Ÿåˆæ£€æµ‹å™¨**ï¼š

```python
class OverfittingDetector:
    """æ£€æµ‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¿‡æ‹Ÿåˆç°è±¡"""
    
    def __init__(self, window_size=5):
        """
        å‚æ•°:
            window_size: ç”¨äºè®¡ç®—è¶‹åŠ¿çš„çª—å£å¤§å°
        """
        self.window_size = window_size
        self.train_losses = []
        self.val_losses = []
        
    def update(self, train_loss, val_loss):
        """æ›´æ–°æŸå¤±å†å²"""
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        
    def is_overfitting(self):
        """åˆ¤æ–­æ˜¯å¦å‡ºç°è¿‡æ‹Ÿåˆ"""
        if len(self.train_losses) < self.window_size:
            return False
        
        # è·å–æœ€è¿‘çš„æŸå¤±å€¼
        recent_train = self.train_losses[-self.window_size:]
        recent_val = self.val_losses[-self.window_size:]
        
        # è®¡ç®—å¹³å‡å€¼
        avg_train = sum(recent_train) / len(recent_train)
        avg_val = sum(recent_val) / len(recent_val)
        
        # åˆ¤æ–­æ ‡å‡†1ï¼šéªŒè¯æŸå¤±æ˜¾è‘—é«˜äºè®­ç»ƒæŸå¤±
        if avg_val > avg_train * 1.3:  # éªŒè¯æŸå¤±æ¯”è®­ç»ƒæŸå¤±é«˜30%ä»¥ä¸Š
            return True
            
        # åˆ¤æ–­æ ‡å‡†2ï¼šè®­ç»ƒæŸå¤±ä¸‹é™ä½†éªŒè¯æŸå¤±ä¸Šå‡
        train_trend = recent_train[-1] - recent_train[0]
        val_trend = recent_val[-1] - recent_val[0]
        
        if train_trend < -0.1 and val_trend > 0.05:  # è®­ç»ƒä¸‹é™ï¼ŒéªŒè¯ä¸Šå‡
            return True
            
        return False
    
    def get_status(self):
        """è·å–å½“å‰è®­ç»ƒçŠ¶æ€çš„è¯¦ç»†æè¿°"""
        if len(self.train_losses) < 2:
            return "è®­ç»ƒåˆæœŸ"
            
        recent_train = sum(self.train_losses[-3:]) / 3
        recent_val = sum(self.val_losses[-3:]) / 3
        
        gap = (recent_val - recent_train) / recent_train * 100
        
        if gap < 10:
            return f"âœ… æ­£å¸¸è®­ç»ƒ (å·®è·: {gap:.1f}%)"
        elif gap < 30:
            return f"âš ï¸ è½»å¾®è¿‡æ‹Ÿåˆ (å·®è·: {gap:.1f}%)"
        else:
            return f"âŒ ä¸¥é‡è¿‡æ‹Ÿåˆ (å·®è·: {gap:.1f}%)"

# ä½¿ç”¨ç¤ºä¾‹
detector = OverfittingDetector()

for epoch in range(max_epochs):
    train_loss = train_one_epoch()
    val_loss = validate()
    
    detector.update(train_loss, val_loss)
    status = detector.get_status()
    
    print(f"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f} | {status}")
    
    if detector.is_overfitting():
        print("âš ï¸ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼å»ºè®®åœæ­¢è®­ç»ƒæˆ–è°ƒæ•´è¶…å‚æ•°")
```

### é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ ¸å¿ƒç­–ç•¥

**1. æ—©åœï¼ˆEarly Stoppingï¼‰- æœ€é‡è¦çš„é˜²æŠ¤æªæ–½**

æ—©åœæ˜¯æœ€å¸¸ç”¨ä¸”æœ€æœ‰æ•ˆçš„é˜²æ­¢è¿‡æ‹Ÿåˆæ–¹æ³•ã€‚å½“éªŒè¯é›†æ€§èƒ½ä¸å†æå‡æ—¶ï¼Œæå‰åœæ­¢è®­ç»ƒã€‚

- **è€å¿ƒå€¼ï¼ˆPatienceï¼‰**ï¼šå…è®¸éªŒè¯æ€§èƒ½è¿ç»­å¤šå°‘ä¸ªepochä¸æå‡ï¼ˆé€šå¸¸è®¾ä¸º3-5ï¼‰
- **æœ€å°æ”¹å–„ï¼ˆMin Deltaï¼‰**ï¼šåªæœ‰å½“æ”¹å–„è¶…è¿‡è¿™ä¸ªé˜ˆå€¼æ‰ç®—æœ‰æ•ˆæå‡ï¼ˆå¦‚0.001ï¼‰
- **æ¢å¤æœ€ä½³æƒé‡**ï¼šè®­ç»ƒç»“æŸåæ¢å¤åˆ°éªŒè¯æ€§èƒ½æœ€å¥½çš„é‚£ä¸ªepochçš„æƒé‡

```python
class EarlyStopping:
    """æ—©åœæœºåˆ¶ - é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ ¸å¿ƒå·¥å…·"""
    
    def __init__(self, patience=5, min_delta=0.001):
        """
        å‚æ•°:
            patience: å®¹å¿éªŒè¯æ€§èƒ½ä¸æå‡çš„epochæ•°
            min_delta: æœ€å°æ”¹å–„é˜ˆå€¼
        """
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.should_stop = False
        
    def __call__(self, val_loss):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ"""
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss < self.best_loss - self.min_delta:
            # éªŒè¯æŸå¤±æœ‰æ˜æ˜¾æ”¹å–„
            self.best_loss = val_loss
            self.counter = 0
            print(f"âœ“ éªŒè¯æŸå¤±æ”¹å–„: {self.best_loss:.4f}")
        else:
            # éªŒè¯æŸå¤±æ²¡æœ‰æ”¹å–„
            self.counter += 1
            print(f"âš  éªŒè¯æŸå¤±æœªæ”¹å–„ ({self.counter}/{self.patience})")
            if self.counter >= self.patience:
                self.should_stop = True
                
        return self.should_stop

# ä½¿ç”¨ç¤ºä¾‹
early_stopping = EarlyStopping(patience=3, min_delta=0.001)

for epoch in range(max_epochs):
    train_loss = train_one_epoch()
    val_loss = validate()
    
    print(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}")
    
    if early_stopping(val_loss):
        print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
        break
```

**2. å­¦ä¹ ç‡è¡°å‡**

åœ¨è®­ç»ƒåæœŸé™ä½å­¦ä¹ ç‡ï¼Œå¯ä»¥è®©æ¨¡å‹è¿›è¡Œæ›´ç²¾ç»†çš„è°ƒæ•´ï¼Œé¿å…åœ¨æœ€ä¼˜è§£é™„è¿‘éœ‡è¡ã€‚

- **é˜¶æ¢¯è¡°å‡**ï¼šæ¯éš”å›ºå®šepoché™ä½å­¦ä¹ ç‡ï¼ˆå¦‚æ¯3ä¸ªepoché™ä½50%ï¼‰
- **ä½™å¼¦è¡°å‡**ï¼šå­¦ä¹ ç‡æŒ‰ä½™å¼¦æ›²çº¿å¹³æ»‘ä¸‹é™
- **è‡ªé€‚åº”è¡°å‡**ï¼šæ ¹æ®éªŒè¯æ€§èƒ½åŠ¨æ€è°ƒæ•´

**3. æ­£åˆ™åŒ–æŠ€æœ¯**

- **æƒé‡è¡°å‡ï¼ˆWeight Decayï¼‰**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡çš„L2æƒ©ç½šé¡¹ï¼Œé˜²æ­¢æƒé‡è¿‡å¤§
- **Dropout**ï¼šè®­ç»ƒæ—¶éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒï¼Œå¢å¼ºæ¨¡å‹é²æ£’æ€§
- **æ•°æ®å¢å¼º**ï¼šå¢åŠ è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ï¼Œè®©æ¨¡å‹è§åˆ°æ›´å¤šå˜åŒ–

**4. åˆç†çš„è®­ç»ƒè½®æ•°é€‰æ‹©**

ä¸åŒåœºæ™¯ä¸‹æ¨èçš„è®­ç»ƒè½®æ•°ï¼š

| æ•°æ®é›†è§„æ¨¡ | å°æ¨¡å‹(1-3B) | ä¸­æ¨¡å‹(7-13B) | å¤§æ¨¡å‹(30B+) |
|----------|------------|-------------|------------|
| å°(<1K)   | 8-12è½®     | 5-8è½®       | 3-5è½®      |
| ä¸­(1-10K) | 5-8è½®      | 3-5è½®       | 2-3è½®      |
| å¤§(>10K)  | 3-5è½®      | 2-3è½®       | 1-2è½®      |

**ç»éªŒæ³•åˆ™**ï¼š
- æ•°æ®è¶Šå°‘ï¼Œéœ€è¦æ›´å¤šè½®æ•°æ¥å……åˆ†å­¦ä¹ ï¼ˆä½†æ›´å®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
- æ¨¡å‹è¶Šå¤§ï¼Œè¶Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œéœ€è¦æ›´å°‘è½®æ•°
- ä»»åŠ¡è¶Šå¤æ‚ï¼Œéœ€è¦æ›´å¤šè½®æ•°æ¥å­¦ä¹ æ¨¡å¼
- å§‹ç»ˆä½¿ç”¨æ—©åœä½œä¸ºä¿é™©ï¼Œè®¾ç½®çš„æœ€å¤§è½®æ•°å¯ä»¥ç¨å¾®å¤šä¸€äº›

### è®­ç»ƒè½®æ•°æ¨èç³»ç»Ÿ

```python
def recommend_epochs(dataset_size, model_size, task_complexity='medium'):
    """
    æ ¹æ®æ•°æ®é›†å¤§å°ã€æ¨¡å‹è§„æ¨¡å’Œä»»åŠ¡å¤æ‚åº¦æ¨èè®­ç»ƒè½®æ•°
    
    å‚æ•°:
        dataset_size: è®­ç»ƒæ ·æœ¬æ•°é‡
        model_size: æ¨¡å‹è§„æ¨¡ ('small'=1-3B, 'medium'=7-13B, 'large'=30B+)
        task_complexity: ä»»åŠ¡å¤æ‚åº¦ ('simple', 'medium', 'complex')
    
    è¿”å›:
        æ¨èçš„è®­ç»ƒé…ç½®å­—å…¸
    """
    
    # åŸºç¡€è½®æ•°è¡¨
    base_epochs = {
        'simple': {'small': 5, 'medium': 3, 'large': 2},
        'medium': {'small': 8, 'medium': 5, 'large': 3},
        'complex': {'small': 12, 'medium': 8, 'large': 5}
    }
    
    # æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´
    if dataset_size < 1000:
        size_factor = 1.5  # å°æ•°æ®é›†éœ€è¦æ›´å¤šè½®æ•°
        warning = "âš ï¸ æ•°æ®é›†è¾ƒå°ï¼Œè¿‡æ‹Ÿåˆé£é™©é«˜"
    elif dataset_size < 10000:
        size_factor = 1.0
        warning = None
    else:
        size_factor = 0.8  # å¤§æ•°æ®é›†å¯ä»¥å‡å°‘è½®æ•°
        warning = None
    
    recommended = int(base_epochs[task_complexity][model_size] * size_factor)
    
    config = {
        'recommended_epochs': recommended,
        'min_epochs': max(1, recommended - 2),
        'max_epochs': recommended + 3,
        'early_stopping_patience': max(2, recommended // 3),
        'evaluation_steps': max(50, dataset_size // (recommended * 10)),
        'save_steps': max(100, dataset_size // (recommended * 5))
    }
    
    if warning:
        config['warning'] = warning
    
    return config

# ä½¿ç”¨ç¤ºä¾‹
print("=== è®­ç»ƒè½®æ•°æ¨èç³»ç»Ÿ ===\n")

scenarios = [
    (500, 'medium', 'simple', "å°æ•°æ®é›† + ä¸­ç­‰æ¨¡å‹ + ç®€å•ä»»åŠ¡"),
    (5000, 'medium', 'medium', "ä¸­ç­‰æ•°æ®é›† + ä¸­ç­‰æ¨¡å‹ + ä¸­ç­‰ä»»åŠ¡"),
    (20000, 'large', 'complex', "å¤§æ•°æ®é›† + å¤§æ¨¡å‹ + å¤æ‚ä»»åŠ¡"),
]

for dataset_size, model_size, complexity, description in scenarios:
    config = recommend_epochs(dataset_size, model_size, complexity)
    
    print(f"åœºæ™¯: {description}")
    print(f"  æ¨èè®­ç»ƒè½®æ•°: {config['recommended_epochs']}")
    print(f"  è½®æ•°èŒƒå›´: {config['min_epochs']}-{config['max_epochs']}")
    print(f"  æ—©åœè€å¿ƒå€¼: {config['early_stopping_patience']}")
    print(f"  è¯„ä¼°æ­¥æ•°: {config['evaluation_steps']}")
    if 'warning' in config:
        print(f"  {config['warning']}")
    print()
```

## 3.2 è®­ç»ƒè¿‡ç¨‹ç›‘æ§ä¸è°ƒæ•´

### å®æ—¶ç›‘æ§çš„é‡è¦æ€§

è®­ç»ƒå¤§æ¨¡å‹æ˜¯ä¸€ä¸ªèµ„æºå¯†é›†ä¸”è€—æ—¶çš„è¿‡ç¨‹ï¼Œå®æ—¶ç›‘æ§å¯ä»¥å¸®åŠ©æˆ‘ä»¬ï¼š
- åŠæ—¶å‘ç°é—®é¢˜ï¼ˆå¦‚è¿‡æ‹Ÿåˆã€æ¢¯åº¦çˆ†ç‚¸ã€å­¦ä¹ åœæ»ï¼‰
- åŠ¨æ€è°ƒæ•´è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ï¼‰
- é¿å…æµªè´¹è®¡ç®—èµ„æºåœ¨æ— æ•ˆçš„è®­ç»ƒä¸Š
- è®°å½•è®­ç»ƒå†å²ï¼Œä¸ºåç»­ä¼˜åŒ–æä¾›ä¾æ®

### å…³é”®ç›‘æ§æŒ‡æ ‡

**1. æŸå¤±å‡½æ•°ï¼ˆLossï¼‰**
- **è®­ç»ƒæŸå¤±**ï¼šåæ˜ æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„æ‹Ÿåˆç¨‹åº¦
- **éªŒè¯æŸå¤±**ï¼šåæ˜ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
- **ç†æƒ³çŠ¶æ€**ï¼šä¸¤è€…éƒ½ç¨³å®šä¸‹é™ï¼Œå·®è·ä¿æŒåœ¨åˆç†èŒƒå›´ï¼ˆ<20%ï¼‰

**2. å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰**
- ç›‘æ§å­¦ä¹ ç‡çš„å˜åŒ–æ›²çº¿
- ç¡®ä¿å­¦ä¹ ç‡è¡°å‡ç­–ç•¥æ­£å¸¸å·¥ä½œ
- è¿‡é«˜ï¼šè®­ç»ƒä¸ç¨³å®šï¼ŒæŸå¤±éœ‡è¡
- è¿‡ä½ï¼šå­¦ä¹ ç¼“æ…¢ï¼Œæ”¶æ•›å›°éš¾

**3. æ¢¯åº¦èŒƒæ•°ï¼ˆGradient Normï¼‰**
- ç›‘æ§æ¢¯åº¦çš„å¤§å°ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±
- æ­£å¸¸èŒƒå›´ï¼š0.1-10
- æ¢¯åº¦çˆ†ç‚¸ï¼š>100ï¼Œéœ€è¦é™ä½å­¦ä¹ ç‡æˆ–ä½¿ç”¨æ¢¯åº¦è£å‰ª
- æ¢¯åº¦æ¶ˆå¤±ï¼š<0.01ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æ¨¡å‹ç»“æ„

**4. è®­ç»ƒé€Ÿåº¦**
- æ¯ä¸ªepochçš„æ—¶é—´
- æ¯ç§’å¤„ç†çš„æ ·æœ¬æ•°ï¼ˆsamples/secï¼‰
- GPUåˆ©ç”¨ç‡å’Œå†…å­˜ä½¿ç”¨

### å®Œæ•´çš„è®­ç»ƒç›‘æ§ç³»ç»Ÿ

```python
import time
from collections import defaultdict

class TrainingMonitor:
    """è®­ç»ƒè¿‡ç¨‹ç›‘æ§å™¨"""
    
    def __init__(self, log_interval=10):
        self.log_interval = log_interval
        self.history = defaultdict(list)
        self.start_time = None
        self.epoch_start_time = None
        
    def start_training(self):
        """å¼€å§‹è®­ç»ƒ"""
        self.start_time = time.time()
        print("=" * 60)
        print("å¼€å§‹è®­ç»ƒ...")
        print("=" * 60)
        
    def start_epoch(self, epoch):
        """å¼€å§‹æ–°çš„epoch"""
        self.epoch_start_time = time.time()
        self.current_epoch = epoch
        
    def log_step(self, step, metrics):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        for key, value in metrics.items():
            self.history[key].append(value)
            
        if step % self.log_interval == 0:
            log_str = f"Epoch {self.current_epoch} | Step {step}"
            for key, value in metrics.items():
                log_str += f" | {key}: {value:.4f}"
            print(log_str)
    
    def end_epoch(self, train_loss, val_loss, val_metrics=None):
        """ç»“æŸepochå¹¶è®°å½•"""
        epoch_time = time.time() - self.epoch_start_time
        
        self.history['train_loss'].append(train_loss)
        self.history['val_loss'].append(val_loss)
        self.history['epoch_time'].append(epoch_time)
        
        print("-" * 60)
        print(f"Epoch {self.current_epoch} å®Œæˆ | ç”¨æ—¶: {epoch_time:.2f}s")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
        
        if val_metrics:
            for key, value in val_metrics.items():
                print(f"  {key}: {value:.4f}")
                self.history[f'val_{key}'].append(value)
        
        # æ£€æŸ¥è¿‡æ‹Ÿåˆ
        if len(self.history['train_loss']) > 1:
            gap = (val_loss - train_loss) / train_loss * 100
            if gap > 30:
                print(f"  âš ï¸ è­¦å‘Š: è®­ç»ƒ-éªŒè¯å·®è·è¾ƒå¤§ ({gap:.1f}%)")
            elif gap > 50:
                print(f"  âŒ ä¸¥é‡è¿‡æ‹Ÿåˆ! å·®è·: {gap:.1f}%")
        
        print("-" * 60)
        
    def end_training(self):
        """ç»“æŸè®­ç»ƒ"""
        total_time = time.time() - self.start_time
        print("=" * 60)
        print(f"è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time/60:.2f} åˆ†é’Ÿ")
        print("=" * 60)
        
    def get_summary(self):
        """è·å–è®­ç»ƒæ‘˜è¦"""
        summary = {
            'total_epochs': len(self.history['train_loss']),
            'best_val_loss': min(self.history['val_loss']),
            'best_epoch': self.history['val_loss'].index(min(self.history['val_loss'])),
            'final_train_loss': self.history['train_loss'][-1],
            'final_val_loss': self.history['val_loss'][-1],
            'avg_epoch_time': sum(self.history['epoch_time']) / len(self.history['epoch_time'])
        }
        return summary
    
    def plot_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²ï¼ˆéœ€è¦matplotlibï¼‰"""
        try:
            import matplotlib.pyplot as plt
            
            fig, axes = plt.subplots(1, 2, figsize=(15, 5))
            
            # æŸå¤±æ›²çº¿
            epochs = range(1, len(self.history['train_loss']) + 1)
            axes[0].plot(epochs, self.history['train_loss'], 'b-', label='è®­ç»ƒæŸå¤±')
            axes[0].plot(epochs, self.history['val_loss'], 'r-', label='éªŒè¯æŸå¤±')
            axes[0].set_xlabel('Epoch')
            axes[0].set_ylabel('Loss')
            axes[0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
            axes[0].legend()
            axes[0].grid(True)
            
            # è¿‡æ‹Ÿåˆç¨‹åº¦
            gaps = [(v - t) / t * 100 for t, v in 
                   zip(self.history['train_loss'], self.history['val_loss'])]
            axes[1].plot(epochs, gaps, 'g-')
            axes[1].axhline(y=30, color='orange', linestyle='--', label='è½»å¾®è¿‡æ‹Ÿåˆé˜ˆå€¼')
            axes[1].axhline(y=50, color='red', linestyle='--', label='ä¸¥é‡è¿‡æ‹Ÿåˆé˜ˆå€¼')
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('å·®è· (%)')
            axes[1].set_title('è®­ç»ƒ-éªŒè¯å·®è·')
            axes[1].legend()
            axes[1].grid(True)
            
            plt.tight_layout()
            plt.savefig('training_history.png')
            print("è®­ç»ƒå†å²å›¾å·²ä¿å­˜åˆ° training_history.png")
            
        except ImportError:
            print("éœ€è¦å®‰è£…matplotlibæ‰èƒ½ç»˜åˆ¶å›¾è¡¨")

# å®Œæ•´çš„è®­ç»ƒå¾ªç¯ç¤ºä¾‹
def train_with_monitoring():
    """å¸¦ç›‘æ§çš„å®Œæ•´è®­ç»ƒæµç¨‹"""
    
    # åˆå§‹åŒ–
    monitor = TrainingMonitor(log_interval=10)
    early_stopping = EarlyStopping(patience=3)
    detector = OverfittingDetector()
    
    # è·å–è®­ç»ƒé…ç½®
    config = recommend_epochs(
        dataset_size=5000,
        model_size='medium',
        task_complexity='medium'
    )
    
    print(f"è®­ç»ƒé…ç½®: {config['recommended_epochs']} epochs")
    print(f"æ—©åœè€å¿ƒå€¼: {config['early_stopping_patience']}")
    print()
    
    monitor.start_training()
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(config['max_epochs']):
        monitor.start_epoch(epoch)
        
        # è®­ç»ƒé˜¶æ®µ
        train_loss = 0
        for step in range(100):  # å‡è®¾æ¯ä¸ªepochæœ‰100æ­¥
            # æ¨¡æ‹Ÿè®­ç»ƒ
            step_loss = simulate_training_step(epoch, step)
            train_loss += step_loss
            
            monitor.log_step(step, {'loss': step_loss})
        
        train_loss /= 100
        
        # éªŒè¯é˜¶æ®µ
        val_loss = simulate_validation(epoch)
        val_metrics = {'accuracy': 0.85 + epoch * 0.01}
        
        # æ›´æ–°ç›‘æ§
        detector.update(train_loss, val_loss)
        monitor.end_epoch(train_loss, val_loss, val_metrics)
        
        print(f"çŠ¶æ€: {detector.get_status()}")
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_loss):
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
            break
        
        # è¿‡æ‹Ÿåˆè­¦å‘Š
        if detector.is_overfitting():
            print("âš ï¸ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆè¶‹åŠ¿ï¼Œå»ºè®®å…³æ³¨")
    
    monitor.end_training()
    
    # æ‰“å°æ‘˜è¦
    summary = monitor.get_summary()
    print("\nè®­ç»ƒæ‘˜è¦:")
    print(f"  æ€»è½®æ•°: {summary['total_epochs']}")
    print(f"  æœ€ä½³éªŒè¯æŸå¤±: {summary['best_val_loss']:.4f} (ç¬¬{summary['best_epoch']}è½®)")
    print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {summary['final_train_loss']:.4f}")
    print(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {summary['final_val_loss']:.4f}")
    print(f"  å¹³å‡æ¯è½®ç”¨æ—¶: {summary['avg_epoch_time']:.2f}ç§’")
    
    # ç»˜åˆ¶å†å²
    monitor.plot_history()

# è¾…åŠ©å‡½æ•°ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒï¼‰
def simulate_training_step(epoch, step):
    """æ¨¡æ‹Ÿå•æ­¥è®­ç»ƒ"""
    import random
    base_loss = 2.0 / (epoch + 1)
    noise = random.uniform(-0.1, 0.1)
    return base_loss + noise

def simulate_validation(epoch):
    """æ¨¡æ‹ŸéªŒè¯"""
    import random
    # æ¨¡æ‹Ÿè¿‡æ‹Ÿåˆï¼šéªŒè¯æŸå¤±åœ¨åæœŸå¼€å§‹ä¸Šå‡
    if epoch < 3:
        return 2.0 / (epoch + 1) + random.uniform(0, 0.1)
    else:
        return 0.7 + (epoch - 3) * 0.05 + random.uniform(0, 0.1)
```

### å®è·µå»ºè®®æ€»ç»“

åœ¨å¤§æ¨¡å‹å¾®è°ƒçš„å®Œæ•´æµç¨‹ä¸­ï¼Œéœ€è¦åšå¥½ä¸‰ä¸ªé˜¶æ®µçš„å·¥ä½œã€‚**è®­ç»ƒå‰**ï¼Œå‡†å¤‡å……è¶³çš„éªŒè¯é›†ï¼ˆ10-20%ï¼‰ç”¨äºç›‘æ§è¿‡æ‹Ÿåˆï¼Œæ ¹æ®æ•°æ®è§„æ¨¡å’Œæ¨¡å‹å¤§å°è®¾ç½®åˆç†çš„åˆå§‹è®­ç»ƒè½®æ•°ï¼Œé…ç½®å¥½æ—©åœæœºåˆ¶å’Œç›‘æ§ç³»ç»Ÿï¼Œå¹¶å‡†å¤‡åº”å¯¹å¼‚å¸¸æƒ…å†µçš„è°ƒæ•´ç­–ç•¥ã€‚**è®­ç»ƒä¸­**ï¼Œæ¯ä¸ªepochéƒ½è¦è®°å½•è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡ï¼Œå¯†åˆ‡è§‚å¯ŸæŸå¤±æ›²çº¿çš„å˜åŒ–è¶‹åŠ¿ï¼Œç‰¹åˆ«æ˜¯ä¸¤è€…çš„å·®è·å˜åŒ–ï¼Œä½¿ç”¨è¿‡æ‹Ÿåˆæ£€æµ‹å™¨å®æ—¶ç›‘æ§æ¨¡å‹çŠ¶æ€ï¼Œå®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼Œå°¤å…¶è¦ä¿å­˜éªŒè¯æ€§èƒ½æœ€å¥½çš„æ¨¡å‹ç‰ˆæœ¬ã€‚**è®­ç»ƒå**ï¼Œåœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ€§èƒ½ï¼Œåˆ†æè®­ç»ƒå†å²æ‰¾å‡ºæœ€ä½³epochï¼Œæ€»ç»“ç»éªŒæ•™è®­ä¸ºåç»­ä¼˜åŒ–æä¾›å‚è€ƒï¼Œä¿å­˜å®Œæ•´çš„è®­ç»ƒæ—¥å¿—å’Œå¯è§†åŒ–ç»“æœã€‚è®°ä½å…³é”®åŸåˆ™ï¼šå®å¯æ¬ æ‹Ÿåˆä¹Ÿä¸è¦ä¸¥é‡è¿‡æ‹Ÿåˆï¼Œæ—©åœæ˜¯æœ€å¯é çš„é˜²æŠ¤æªæ–½ï¼ŒæŒç»­ç›‘æ§æ¯”ç›²ç›®è®­ç»ƒæ›´é‡è¦ï¼Œå§‹ç»ˆä¿å­˜æœ€ä½³æ¨¡å‹è€Œéæœ€åæ¨¡å‹ï¼Œå¤§æ¨¡å‹é…åˆå°æ•°æ®é›†æ—¶éœ€æ ¼å¤–è°¨æ…ã€‚