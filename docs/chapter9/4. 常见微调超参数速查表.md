# 4. 常见微调超参数速查表

## 4.1 任务特定超参数推荐

基于大量实验和社区经验，我们整理了不同任务类型的超参数推荐表：

```python
def create_hyperparameter_lookup_table():
    """创建超参数速查表"""
    
    hyperparameter_table = {
        'classification': {
            'sentiment_analysis': {
                'learning_rate': {'full': 2e-5, 'lora': 1e-4, 'adapter': 5e-5},
                'batch_size': {'small_gpu': 8, 'medium_gpu': 16, 'large_gpu': 32},
                'epochs': {'small_data': 5, 'medium_data': 3, 'large_data': 2},
                'warmup_ratio': 0.1,
                'weight_decay': 0.01,
                'gradient_accumulation': {'target_batch': 32}
            },
            'text_classification': {
                'learning_rate': {'full': 2e-5, 'lora': 5e-5, 'adapter': 3e-5},
                'batch_size': {'small_gpu': 8, 'medium_gpu': 16, 'large_gpu': 32},
                'epochs': {'small_data': 8, 'medium_data': 5, 'large_data': 3},
                'warmup_ratio': 0.1,
                'weight_decay': 0.01
            }
        },
        'generation': {
            'text_generation': {
                'learning_rate': {'full': 1e-5, 'lora': 2e-5, 'adapter': 1e-5},
                'batch_size': {'small_gpu': 4, 'medium_gpu': 8, 'large_gpu': 16},
                'epochs': {'small_data': 3, 'medium_data': 2, 'large_data': 1},
                'warmup_ratio': 0.05,
                'weight_decay': 0.01,
                'max_length': 512
            },
            'summarization': {
                'learning_rate': {'full': 5e-6, 'lora': 1e-5, 'adapter': 5e-6},
                'batch_size': {'small_gpu': 2, 'medium_gpu': 4, 'large_gpu': 8},
                'epochs': {'small_data': 5, 'medium_data': 3, 'large_data': 2},
                'warmup_ratio': 0.1,
                'weight_decay': 0.01,
                'max_length': 1024
            }
        },
        'qa': {
            'extractive_qa': {
                'learning_rate': {'full': 3e-5, 'lora': 5e-5, 'adapter': 3e-5},
                'batch_size': {'small_gpu': 8, 'medium_gpu': 12, 'large_gpu': 16},
                'epochs': {'small_data': 5, 'medium_data': 3, 'large_data': 2},
                'warmup_ratio': 0.1,
                'weight_decay': 0.01
            },
            'generative_qa': {
                'learning_rate': {'full': 1e-5, 'lora': 2e-5, 'adapter': 1e-5},
                'batch_size': {'small_gpu': 4, 'medium_gpu': 8, 'large_gpu': 12},
                'epochs': {'small_data': 4, 'medium_data': 3, 'large_data': 2},
                'warmup_ratio': 0.05,
                'weight_decay': 0.01
            }
        },
        'dialogue': {
            'chatbot': {
                'learning_rate': {'full': 1e-5, 'lora': 2e-5, 'adapter': 1e-5},
                'batch_size': {'small_gpu': 2, 'medium_gpu': 4, 'large_gpu': 8},
                'epochs': {'small_data': 3, 'medium_data': 2, 'large_data': 1},
                'warmup_ratio': 0.05,
                'weight_decay': 0.01,
                'max_length': 2048
            }
        }
    }
    
    return hyperparameter_table

def get_recommended_hyperparameters(task_type, subtask, model_size='7b', 
                                  data_size='medium', gpu_memory='medium', 
                                  method='lora'):
    """获取推荐的超参数"""
    
    table = create_hyperparameter_lookup_table()
    
    if task_type not in table or subtask not in table[task_type]:
        return None
    
    config = table[task_type][subtask]
    
    # 构建推荐配置
    recommended_config = {
        'learning_rate': config['learning_rate'].get(method, config['learning_rate']['lora']),
        'batch_size': config['batch_size'].get(f'{gpu_memory}_gpu', config['batch_size']['medium_gpu']),
        'epochs': config['epochs'].get(f'{data_size}_data', config['epochs']['medium_data']),
        'warmup_ratio': config.get('warmup_ratio', 0.1),
        'weight_decay': config.get('weight_decay', 0.01),
        'scheduler': 'cosine' if task_type == 'generation' else 'linear',
        'gradient_clipping': 1.0,
        'early_stopping_patience': max(2, config['epochs'].get(f'{data_size}_data', 3) // 2)
    }
    
    # 添加任务特定参数
    if 'max_length' in config:
        recommended_config['max_length'] = config['max_length']
    
    if 'gradient_accumulation' in config:
        target_batch = config['gradient_accumulation']['target_batch']
        actual_batch = recommended_config['batch_size']
        recommended_config['gradient_accumulation_steps'] = max(1, target_batch // actual_batch)
    
    return recommended_config

def print_hyperparameter_table():
    """打印超参数速查表"""
    
    print("=== 大模型微调超参数速查表 ===\n")
    
    # 常见任务配置
    common_tasks = [
        ('classification', 'sentiment_analysis', '情感分析'),
        ('classification', 'text_classification', '文本分类'),
        ('generation', 'text_generation', '文本生成'),
        ('generation', 'summarization', '文本摘要'),
        ('qa', 'extractive_qa', '抽取式问答'),
        ('qa', 'generative_qa', '生成式问答'),
        ('dialogue', 'chatbot', '对话系统')
    ]
    
    methods = ['full', 'lora', 'adapter']
    
    for task_type, subtask, task_name in common_tasks:
        print(f"### {task_name} ({task_type}/{subtask})")
        print("| 微调方法 | 学习率 | 批次大小 | 训练轮数 | 预热比例 |")
        print("|---------|--------|----------|----------|----------|")
        
        for method in methods:
            config = get_recommended_hyperparameters(
                task_type, subtask, method=method
            )
            if config:
                print(f"| {method.upper()} | {config['learning_rate']:.0e} | "
                      f"{config['batch_size']} | {config['epochs']} | "
                      f"{config['warmup_ratio']} |")
        print()

# 使用示例
print_hyperparameter_table()

# 获取特定任务的推荐配置
print("=== 特定任务配置示例 ===")
config = get_recommended_hyperparameters(
    task_type='classification',
    subtask='sentiment_analysis',
    model_size='7b',
    data_size='small',
    gpu_memory='medium',
    method='lora'
)

print("情感分析任务推荐配置:")
for key, value in config.items():
    print(f"  {key}: {value}")
```

## 4.2 调参最佳实践

**从粗到细的调参策略**：先确定学习率的数量级，再调整批次大小，最后微调其他参数。

**使用验证集指导调参**：始终基于验证集性能来选择超参数，避免在测试集上调参。

**记录实验结果**：使用工具如Weights & Biases记录所有实验，便于分析和复现。

**考虑计算资源限制**：在资源受限的情况下，优先保证学习率和批次大小的合理性。

通过系统的超参数调优，我们可以显著提升大模型微调的效果。关键是要理解每个参数的作用机制，结合具体任务特点和资源限制，制定合理的调优策略。记住，超参数调优是一个迭代过程，需要耐心和系统性的方法。

```python
import json
import time
from datetime import datetime
from pathlib import Path
import pandas as pd

class HyperparameterTuningBestPractices:
    """超参数调优最佳实践指南"""
    
    def __init__(self, project_name, save_dir="./tuning_logs"):
        self.project_name = project_name
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
        self.experiment_log = []
        
    def coarse_to_fine_tuning_strategy(self):
        """从粗到细的调参策略"""
        
        strategy = {
            "phase_1_coarse": {
                "description": "粗调阶段 - 确定参数数量级",
                "parameters": {
                    "learning_rate": {
                        "search_space": [1e-6, 1e-5, 1e-4, 1e-3],
                        "priority": "highest",
                        "method": "grid_search"
                    },
                    "batch_size": {
                        "search_space": [4, 8, 16, 32],
                        "priority": "high",
                        "method": "grid_search"
                    },
                    "epochs": {
                        "search_space": [1, 3, 5],
                        "priority": "medium",
                        "method": "fixed_choice"
                    }
                },
                "evaluation_criteria": "validation_loss",
                "max_trials": 20,
                "early_stopping": True
            },
            
            "phase_2_medium": {
                "description": "中调阶段 - 在最佳区间内细化",
                "parameters": {
                    "learning_rate": {
                        "search_space": "around_best_from_phase1",
                        "range_factor": 0.5,  # 在最佳值的±50%范围内搜索
                        "method": "random_search"
                    },
                    "warmup_ratio": {
                        "search_space": [0.0, 0.05, 0.1, 0.15],
                        "priority": "medium",
                        "method": "grid_search"
                    },
                    "weight_decay": {
                        "search_space": [0.0, 0.01, 0.1],
                        "priority": "medium",
                        "method": "grid_search"
                    }
                },
                "evaluation_criteria": ["validation_loss", "validation_accuracy"],
                "max_trials": 30,
                "early_stopping": True
            },
            
            "phase_3_fine": {
                "description": "精调阶段 - 微调细节参数",
                "parameters": {
                    "gradient_clipping": {
                        "search_space": [0.5, 1.0, 2.0],
                        "priority": "low",
                        "method": "grid_search"
                    },
                    "scheduler_type": {
                        "search_space": ["linear", "cosine", "polynomial"],
                        "priority": "low",
                        "method": "grid_search"
                    },
                    "dropout_rate": {
                        "search_space": [0.0, 0.1, 0.2],
                        "priority": "low",
                        "method": "grid_search"
                    }
                },
                "evaluation_criteria": ["validation_f1", "validation_accuracy"],
                "max_trials": 15,
                "cross_validation": True
            }
        }
        
        return strategy
    
    def resource_aware_tuning_plan(self, available_gpu_hours, gpu_memory_gb):
        """基于资源限制的调参计划"""
        
        # 根据可用资源制定调参计划
        if available_gpu_hours < 10:
            plan = "minimal"
        elif available_gpu_hours < 50:
            plan = "standard"
        else:
            plan = "comprehensive"
        
        tuning_plans = {
            "minimal": {
                "description": "资源受限方案（<10 GPU小时）",
                "focus_parameters": ["learning_rate", "batch_size"],
                "max_trials": 10,
                "evaluation_method": "single_validation",
                "early_stopping_patience": 2,
                "recommendations": [
                    "只调优最关键的学习率和批次大小",
                    "使用经验值作为其他参数的起点",
                    "采用aggressive的早停策略"
                ]
            },
            
            "standard": {
                "description": "标准方案（10-50 GPU小时）",
                "focus_parameters": ["learning_rate", "batch_size", "epochs", "warmup_ratio"],
                "max_trials": 30,
                "evaluation_method": "validation_with_holdout",
                "early_stopping_patience": 3,
                "recommendations": [
                    "分两阶段调参：粗调+精调",
                    "使用验证集进行参数选择",
                    "记录所有实验结果"
                ]
            },
            
            "comprehensive": {
                "description": "全面方案（>50 GPU小时）",
                "focus_parameters": "all",
                "max_trials": 100,
                "evaluation_method": "cross_validation",
                "early_stopping_patience": 5,
                "recommendations": [
                    "三阶段调参：粗调+中调+精调",
                    "使用交叉验证确保结果稳定性",
                    "进行多目标优化",
                    "测试多个随机种子"
                ]
            }
        }
        
        selected_plan = tuning_plans[plan]
        
        # 根据GPU显存调整批次大小搜索空间
        if gpu_memory_gb < 12:
            max_batch_size = 8
        elif gpu_memory_gb < 24:
            max_batch_size = 16
        else:
            max_batch_size = 32
        
        selected_plan["batch_size_limit"] = max_batch_size
        
        return selected_plan
    
    def validation_based_selection_guide(self):
        """基于验证集的参数选择指南"""
        
        guide = {
            "validation_split_strategy": {
                "small_dataset": {
                    "condition": "< 1000 samples",
                    "strategy": "stratified_k_fold",
                    "k": 5,
                    "reason": "小数据集需要充分利用每个样本"
                },
                "medium_dataset": {
                    "condition": "1000-10000 samples",
                    "strategy": "holdout_validation",
                    "split_ratio": 0.8,
                    "reason": "标准的训练验证划分"
                },
                "large_dataset": {
                    "condition": "> 10000 samples",
                    "strategy": "holdout_with_early_stopping",
                    "split_ratio": 0.85,
                    "reason": "大数据集可以用更多数据训练"
                }
            },
            
            "evaluation_metrics": {
                "classification": {
                    "primary": "f1_score",
                    "secondary": ["accuracy", "precision", "recall"],
                    "selection_criteria": "highest_f1_with_stable_loss"
                },
                "generation": {
                    "primary": "perplexity",
                    "secondary": ["bleu_score", "rouge_score"],
                    "selection_criteria": "lowest_perplexity_with_good_generation_quality"
                },
                "qa": {
                    "primary": "exact_match",
                    "secondary": ["f1_score", "answer_similarity"],
                    "selection_criteria": "highest_exact_match_with_good_f1"
                }
            },
            
            "selection_rules": [
                "永远不要在测试集上选择超参数",
                "使用验证集选择参数，测试集仅用于最终评估",
                "如果验证集性能相近，选择更简单的模型",
                "考虑性能的稳定性，不只看单次最佳结果",
                "记录每次实验的随机种子，确保可复现性"
            ]
        }
        
        return guide
    
    def experiment_logging_system(self):
        """实验记录系统"""
        
        def log_experiment(self, params, results, metadata=None):
            """记录单次实验"""
            
            experiment_record = {
                "timestamp": datetime.now().isoformat(),
                "experiment_id": f"{self.project_name}_{len(self.experiment_log)}",
                "parameters": params,
                "results": results,
                "metadata": metadata or {},
                "status": "completed"
            }
            
            self.experiment_log.append(experiment_record)
            
            # 保存到文件
            log_file = self.save_dir / f"{self.project_name}_experiments.json"
            with open(log_file, 'w') as f:
                json.dump(self.experiment_log, f, indent=2)
            
            return experiment_record["experiment_id"]
        
        def analyze_experiments(self):
            """分析实验结果"""
            
            if not self.experiment_log:
                return "No experiments recorded"
            
            df = pd.DataFrame(self.experiment_log)
            
            analysis = {
                "total_experiments": len(df),
                "best_experiment": None,
                "parameter_importance": {},
                "convergence_analysis": {}
            }
            
            # 找到最佳实验
            if 'results' in df.columns:
                best_idx = df['results'].apply(
                    lambda x: x.get('validation_score', 0) if isinstance(x, dict) else 0
                ).idxmax()
                
                analysis["best_experiment"] = {
                    "experiment_id": df.loc[best_idx, 'experiment_id'],
                    "parameters": df.loc[best_idx, 'parameters'],
                    "results": df.loc[best_idx, 'results']
                }
            
            # 参数重要性分析（简化版）
            for param in ['learning_rate', 'batch_size', 'epochs']:
                if param in str(df['parameters'].values):
                    param_values = []
                    scores = []
                    
                    for _, row in df.iterrows():
                        if isinstance(row['parameters'], dict) and param in row['parameters']:
                            param_values.append(row['parameters'][param])
                            if isinstance(row['results'], dict):
                                scores.append(row['results'].get('validation_score', 0))
                    
                    if param_values and scores:
                        correlation = np.corrcoef(param_values, scores)[0, 1]
                        analysis["parameter_importance"][param] = abs(correlation)
            
            return analysis
        
        # 将方法绑定到类
        self.log_experiment = log_experiment.__get__(self, type(self))
        self.analyze_experiments = analyze_experiments.__get__(self, type(self))
        
        return {
            "logging_template": {
                "required_fields": ["parameters", "results", "timestamp"],
                "recommended_fields": ["experiment_id", "metadata", "status"],
                "metadata_suggestions": [
                    "model_architecture",
                    "dataset_info",
                    "hardware_info",
                    "random_seed",
                    "training_time"
                ]
            },
            "analysis_features": [
                "best_experiment_identification",
                "parameter_importance_ranking",
                "convergence_analysis",
                "resource_usage_tracking"
            ]
        }
    
    def generate_tuning_checklist(self):
        """生成调参检查清单"""
        
        checklist = {
            "pre_tuning": [
                "✓ 确定任务类型和评估指标",
                "✓ 准备训练、验证、测试数据集",
                "✓ 估算可用计算资源",
                "✓ 设置实验记录系统",
                "✓ 确定基线模型性能"
            ],
            
            "during_tuning": [
                "✓ 从学习率开始调参",
                "✓ 监控训练和验证损失",
                "✓ 使用早停避免过拟合",
                "✓ 记录每次实验结果",
                "✓ 定期分析参数重要性"
            ],
            
            "post_tuning": [
                "✓ 在测试集上评估最佳模型",
                "✓ 验证结果的可重现性",
                "✓ 分析失败的实验",
                "✓ 总结调参经验",
                "✓ 保存最佳模型和配置"
            ],
            
            "common_pitfalls": [
                "❌ 在测试集上选择超参数",
                "❌ 忽略随机种子的影响",
                "❌ 过度拟合验证集",
                "❌ 忽略计算资源限制",
                "❌ 不记录实验过程"
            ]
        }
        
        return checklist

# 使用示例
def demonstrate_best_practices():
    """演示调参最佳实践"""
    
    # 创建最佳实践指南
    tuning_guide = HyperparameterTuningBestPractices("sentiment_analysis_project")
    
    print("=== 超参数调优最佳实践指南 ===\n")
    
    # 1. 从粗到细的调参策略
    print("1. 从粗到细的调参策略:")
    strategy = tuning_guide.coarse_to_fine_tuning_strategy()
    for phase, config in strategy.items():
        print(f"\n{phase.upper()}:")
        print(f"  描述: {config['description']}")
        print(f"  最大试验次数: {config['max_trials']}")
        print(f"  关键参数: {list(config['parameters'].keys())}")
    
    # 2. 资源感知的调参计划
    print("\n\n2. 资源感知的调参计划:")
    for gpu_hours in [5, 25, 100]:
        plan = tuning_guide.resource_aware_tuning_plan(gpu_hours, 16)
        print(f"\n{gpu_hours} GPU小时方案:")
        print(f"  类型: {plan['description']}")
        print(f"  关键参数: {plan['focus_parameters']}")
        print(f"  最大试验: {plan['max_trials']}")
    
    # 3. 验证集选择指南
    print("\n\n3. 验证集选择指南:")
    validation_guide = tuning_guide.validation_based_selection_guide()
    print("数据集划分策略:")
    for dataset_type, strategy in validation_guide["validation_split_strategy"].items():
        print(f"  {dataset_type}: {strategy['strategy']} - {strategy['reason']}")
    
    # 4. 实验记录系统
    print("\n\n4. 实验记录系统:")
    logging_system = tuning_guide.experiment_logging_system()
    print("必需字段:", logging_system["logging_template"]["required_fields"])
    print("推荐字段:", logging_system["logging_template"]["recommended_fields"])
    
    # 5. 调参检查清单
    print("\n\n5. 调参检查清单:")
    checklist = tuning_guide.generate_tuning_checklist()
    for phase, items in checklist.items():
        print(f"\n{phase.upper()}:")
        for item in items:
            print(f"  {item}")
    
    # 模拟记录实验
    print("\n\n6. 实验记录示例:")
    sample_params = {
        "learning_rate": 2e-5,
        "batch_size": 16,
        "epochs": 3,
        "warmup_ratio": 0.1
    }
    
    sample_results = {
        "validation_score": 0.85,
        "validation_loss": 0.45,
        "training_time": 1200
    }
    
    experiment_id = tuning_guide.log_experiment(sample_params, sample_results)
    print(f"记录实验: {experiment_id}")
    
    # 分析实验结果
    analysis = tuning_guide.analyze_experiments()
    print(f"实验分析: {analysis}")

# 运行演示
demonstrate_best_practices()
```

通过系统的超参数调优最佳实践，我们可以更高效地找到最优配置，避免常见陷阱，并建立可重现的实验流程。关键要点包括：

1. **分阶段调优**：从粗调到精调，逐步缩小搜索空间
2. **资源感知**：根据可用计算资源制定合理的调优计划
3. **验证驱动**：始终基于验证集性能进行参数选择
4. **完整记录**：记录所有实验过程，便于分析和复现
5. **避免陷阱**：不在测试集上调参，注意随机种子影响

这些实践经验来自于大量的实际项目，遵循这些指导原则可以显著提高调参效率和最终模型性能。

