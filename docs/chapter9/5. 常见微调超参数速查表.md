
## 5. 常见微调超参数速查表

### 5.1 任务特定超参数推荐

基于大量实验和社区经验，我们整理了不同任务类型的超参数推荐表：

```python
def create_hyperparameter_lookup_table():
    """创建超参数速查表"""
    
    hyperparameter_table = {
        'classification': {
            'sentiment_analysis': {
                'learning_rate': {'full': 2e-5, 'lora': 1e-4, 'adapter': 5e-5},
                'batch_size': {'small_gpu': 8, 'medium_gpu': 16, 'large_gpu': 32},
                'epochs': {'small_data': 5, 'medium_data': 3, 'large_data': 2},
                'warmup_ratio': 0.1,
                'weight_decay': 0.01,
                'gradient_accumulation': {'target_batch': 32}
            },
            'text_classification': {
                'learning_rate': {'full': 2e-5, 'lora': 5e-5, 'adapter': 3e-5},
                'batch_size': {'small_gpu': 8, 'medium_gpu': 16, 'large_gpu': 32},
                'epochs': {'small_data': 8, 'medium_data': 5, 'large_data': 3},
                'warmup_ratio': 0.1,
                'weight_decay': 0.01
            }
        },
        'generation': {
            'text_generation': {
                'learning_rate': {'full': 1e-5, 'lora': 2e-5, 'adapter': 1e-5},
                'batch_size': {'small_gpu': 4, 'medium_gpu': 8, 'large_gpu': 16},
                'epochs': {'small_data': 3, 'medium_data': 2, 'large_data': 1},
                'warmup_ratio': 0.05,
                'weight_decay': 0.01,
                'max_length': 512
            },
            'summarization': {
                'learning_rate': {'full': 5e-6, 'lora': 1e-5, 'adapter': 5e-6},
                'batch_size': {'small_gpu': 2, 'medium_gpu': 4, 'large_gpu': 8},
                'epochs': {'small_data': 5, 'medium_data': 3, 'large_data': 2},
                'warmup_ratio': 0.1,
                'weight_decay': 0.01,
                'max_length': 1024
            }
        },
        'qa': {
            'extractive_qa': {
                'learning_rate': {'full': 3e-5, 'lora': 5e-5, 'adapter': 3e-5},
                'batch_size': {'small_gpu': 8, 'medium_gpu': 12, 'large_gpu': 16},
                'epochs': {'small_data': 5, 'medium_data': 3, 'large_data': 2},
                'warmup_ratio': 0.1,
                'weight_decay': 0.01
            },
            'generative_qa': {
                'learning_rate': {'full': 1e-5, 'lora': 2e-5, 'adapter': 1e-5},
                'batch_size': {'small_gpu': 4, 'medium_gpu': 8, 'large_gpu': 12},
                'epochs': {'small_data': 4, 'medium_data': 3, 'large_data': 2},
                'warmup_ratio': 0.05,
                'weight_decay': 0.01
            }
        },
        'dialogue': {
            'chatbot': {
                'learning_rate': {'full': 1e-5, 'lora': 2e-5, 'adapter': 1e-5},
                'batch_size': {'small_gpu': 2, 'medium_gpu': 4, 'large_gpu': 8},
                'epochs': {'small_data': 3, 'medium_data': 2, 'large_data': 1},
                'warmup_ratio': 0.05,
                'weight_decay': 0.01,
                'max_length': 2048
            }
        }
    }
    
    return hyperparameter_table

def get_recommended_hyperparameters(task_type, subtask, model_size='7b', 
                                  data_size='medium', gpu_memory='medium', 
                                  method='lora'):
    """获取推荐的超参数"""
    
    table = create_hyperparameter_lookup_table()
    
    if task_type not in table or subtask not in table[task_type]:
        return None
    
    config = table[task_type][subtask]
    
    # 构建推荐配置
    recommended_config = {
        'learning_rate': config['learning_rate'].get(method, config['learning_rate']['lora']),
        'batch_size': config['batch_size'].get(f'{gpu_memory}_gpu', config['batch_size']['medium_gpu']),
        'epochs': config['epochs'].get(f'{data_size}_data', config['epochs']['medium_data']),
        'warmup_ratio': config.get('warmup_ratio', 0.1),
        'weight_decay': config.get('weight_decay', 0.01),
        'scheduler': 'cosine' if task_type == 'generation' else 'linear',
        'gradient_clipping': 1.0,
        'early_stopping_patience': max(2, config['epochs'].get(f'{data_size}_data', 3) // 2)
    }
    
    # 添加任务特定参数
    if 'max_length' in config:
        recommended_config['max_length'] = config['max_length']
    
    if 'gradient_accumulation' in config:
        target_batch = config['gradient_accumulation']['target_batch']
        actual_batch = recommended_config['batch_size']
        recommended_config['gradient_accumulation_steps'] = max(1, target_batch // actual_batch)
    
    return recommended_config

def print_hyperparameter_table():
    """打印超参数速查表"""
    
    print("=== 大模型微调超参数速查表 ===\n")
    
    # 常见任务配置
    common_tasks = [
        ('classification', 'sentiment_analysis', '情感分析'),
        ('classification', 'text_classification', '文本分类'),
        ('generation', 'text_generation', '文本生成'),
        ('generation', 'summarization', '文本摘要'),
        ('qa', 'extractive_qa', '抽取式问答'),
        ('qa', 'generative_qa', '生成式问答'),
        ('dialogue', 'chatbot', '对话系统')
    ]
    
    methods = ['full', 'lora', 'adapter']
    
    for task_type, subtask, task_name in common_tasks:
        print(f"### {task_name} ({task_type}/{subtask})")
        print("| 微调方法 | 学习率 | 批次大小 | 训练轮数 | 预热比例 |")
        print("|---------|--------|----------|----------|----------|")
        
        for method in methods:
            config = get_recommended_hyperparameters(
                task_type, subtask, method=method
            )
            if config:
                print(f"| {method.upper()} | {config['learning_rate']:.0e} | "
                      f"{config['batch_size']} | {config['epochs']} | "
                      f"{config['warmup_ratio']} |")
        print()

# 使用示例
print_hyperparameter_table()

# 获取特定任务的推荐配置
print("=== 特定任务配置示例 ===")
config = get_recommended_hyperparameters(
    task_type='classification',
    subtask='sentiment_analysis',
    model_size='7b',
    data_size='small',
    gpu_memory='medium',
    method='lora'
)

print("情感分析任务推荐配置:")
for key, value in config.items():
    print(f"  {key}: {value}")
```

### 5.2 调参最佳实践

**从粗到细的调参策略**：先确定学习率的数量级，再调整批次大小，最后微调其他参数。

**使用验证集指导调参**：始终基于验证集性能来选择超参数，避免在测试集上调参。

**记录实验结果**：使用工具如Weights & Biases记录所有实验，便于分析和复现。

**考虑计算资源限制**：在资源受限的情况下，优先保证学习率和批次大小的合理性。

通过系统的超参数调优，我们可以显著提升大模型微调的效果。关键是要理解每个参数的作用机制，结合具体任务特点和资源限制，制定合理的调优策略。记住，超参数调优是一个迭代过程，需要耐心和系统性的方法。

```python
import json
import time
from datetime import datetime
from pathlib import Path
import pandas as pd

class HyperparameterTuningBestPractices:
    """超参数调优最佳实践指南"""
    
    def __init__(self, project_name, save_dir="./tuning_logs"):
        self.project_name = project_name
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
        self.experiment_log = []
        
    def coarse_to_fine_tuning_strategy(self):
        """从粗到细的调参策略"""
        
        strategy = {
            "phase_1_coarse": {
                "description": "粗调阶段 - 确定参数数量级",
                "parameters": {
                    "learning_rate": {
                        "search_space": [1e-6, 1e-5, 1e-4, 1e-3],
                        "priority": "highest",
                        "method": "grid_search"
                    },
                    "batch_size": {
                        "search_space": [4, 8, 16, 32],
                        "priority": "high",
                        "method": "grid_search"
                    },
                    "epochs": {
                        "search_space": [1, 3, 5],
                        "priority": "medium",
                        "method": "fixed_choice"
                    }
                },
                "evaluation_criteria": "validation_loss",
                "max_trials": 20,
                "early_stopping": True
            },
            
            "phase_2_medium": {
                "description": "中调阶段 - 在最佳区间内细化",
                "parameters": {
                    "learning_rate": {
                        "search_space": "around_best_from_phase1",
                        "range_factor": 0.5,  # 在最佳值的±50%范围内搜索
                        "method": "random_search"
                    },
                    "warmup_ratio": {
                        "search_space": [0.0, 0.05, 0.1, 0.15],
                        "priority": "medium",
                        "method": "grid_search"
                    },
                    "weight_decay": {
                        "search_space": [0.0, 0.01, 0.1],
                        "priority": "medium",
                        "method": "grid_search"
                    }
                },
                "evaluation_criteria": ["validation_loss", "validation_accuracy"],
                "max_trials": 30,
                "early_stopping": True
            },
            
            "phase_3_fine": {
                "description": "精调阶段 - 微调细节参数",
                "parameters": {
                    "gradient_clipping": {
                        "search_space": [0.5, 1.0, 2.0],
                        "priority": "low",
                        "method": "grid_search"
                    },
                    "scheduler_type": {
                        "search_space": ["linear", "cosine", "polynomial"],
                        "priority": "low",
                        "method": "grid_search"
                    },
                    "dropout_rate": {
                        "search_space": [0.0, 0.1, 0.2],
                        "priority": "low",
                        "method": "grid_search"
                    }
                },
                "evaluation_criteria": ["validation_f1", "validation_accuracy"],
                "max_trials": 15,
                "cross_validation": True
            }
        }
        
        return strategy
    
    def resource_aware_tuning_plan(self, available_gpu_hours, gpu_memory_gb):
        """基于资源限制的调参计划"""
        
        # 根据可用资源制定调参计划
        if available_gpu_hours < 10:
            plan = "minimal"
        elif available_gpu_hours < 50:
            plan = "standard"
        else:
            plan = "comprehensive"
        
        tuning_plans = {
            "minimal": {
                "description": "资源受限方案（<10 GPU小时）",
                "focus_parameters": ["learning_rate", "batch_size"],
                "max_trials": 10,
                "evaluation_method": "single_validation",
                "early_stopping_patience": 2,
                "recommendations": [
                    "只调优最关键的学习率和批次大小",
                    "使用经验值作为其他参数的起点",
                    "采用aggressive的早停策略"
                ]
            },
            
            "standard": {
                "description": "标准方案（10-50 GPU小时）",
                "focus_parameters": ["learning_rate", "batch_size", "epochs", "warmup_ratio"],
                "max_trials": 30,
                "evaluation_method": "validation_with_holdout",
                "early_stopping_patience": 3,
                "recommendations": [
                    "分两阶段调参：粗调+精调",
                    "使用验证集进行参数选择",
                    "记录所有实验结果"
                ]
            },
            
            "comprehensive": {
                "description": "全面方案（>50 GPU小时）",
                "focus_parameters": "all",
                "max_trials": 100,
                "evaluation_method": "cross_validation",
                "early_stopping_patience": 5,
                "recommendations": [
                    "三阶段调参：粗调+中调+精调",
                    "使用交叉验证确保结果稳定性",
                    "进行多目标优化",
                    "测试多个随机种子"
                ]
            }
        }
        
        selected_plan = tuning_plans[plan]
        
        # 根据GPU显存调整批次大小搜索空间
        if gpu_memory_gb < 12:
            max_batch_size = 8
        elif gpu_memory_gb < 24:
            max_batch_size = 16
        else:
            max_batch_size = 32
        
        selected_plan["batch_size_limit"] = max_batch_size
        
        return selected_plan
    
    def validation_based_selection_guide(self):
        """基于验证集的参数选择指南"""
        
        guide = {
            "validation_split_strategy": {
                "small_dataset": {
                    "condition": "< 1000 samples",
                    "strategy": "stratified_k_fold",
                    "k": 5,
                    "reason": "小数据集需要充分利用每个样本"
                },
                "medium_dataset": {
                    "condition": "1000-10000 samples",
                    "strategy": "holdout_validation",
                    "split_ratio": 0.8,
                    "reason": "标准的训练验证划分"
                },
                "large_dataset": {
                    "condition": "> 10000 samples",
                    "strategy": "holdout_with_early_stopping",
                    "split_ratio": 0.85,
                    "reason": "大数据集可以用更多数据训练"
                }
            },
            
            "evaluation_metrics": {
                "classification": {
                    "primary": "f1_score",
                    "secondary": ["accuracy", "precision", "recall"],
                    "selection_criteria": "highest_f1_with_stable_loss"
                },
                "generation": {
                    "primary": "perplexity",
                    "secondary": ["bleu_score", "rouge_score"],
                    "selection_criteria": "lowest_perplexity_with_good_generation_quality"
                },
                "qa": {
                    "primary": "exact_match",
                    "secondary": ["f1_score", "answer_similarity"],
                    "selection_criteria": "highest_exact_match_with_good_f1"
                }
            },
            
            "selection_rules": [
                "永远不要在测试集上选择超参数",
                "使用验证集选择参数，测试集仅用于最终评估",
                "如果验证集性能相近，选择更简单的模型",
                "考虑性能的稳定性，不只看单次最佳结果",
                "记录每次实验的随机种子，确保可复现性"
            ]
        }
        
        return guide
    
    def experiment_logging_system(self):
        """实验记录系统"""
        
        def log_experiment(self, params, results, metadata=None):
            """记录单次实验"""
            
            experiment_record = {
                "timestamp": datetime.now().isoformat(),
                "experiment_id": f"{self.project_name}_{len(self.experiment_log)}",
                "parameters": params,
                "results": results,
                "metadata": metadata or {},
                "status": "completed"
            }
            
            self.experiment_log.append(experiment_record)
            
            # 保存到文件
            log_file = self.save_dir / f"{self.project_name}_experiments.json"
            with open(log_file, 'w') as f:
                json.dump(self.experiment_log, f, indent=2)
            
            return experiment_record["experiment_id"]
        
        def analyze_experiments(self):
            """分析实验结果"""
            
            if not self.experiment_log:
                return "No experiments recorded"
            
            df = pd.DataFrame(self.experiment_log)
            
            analysis = {
                "total_experiments": len(df),
                "best_experiment": None,
                "parameter_importance": {},
                "convergence_analysis": {}
            }
            
            # 找到最佳实验
            if 'results' in df.columns:
                best_idx = df['results'].apply(
                    lambda x: x.get('validation_score', 0) if isinstance(x, dict) else 0
                ).idxmax()
                
                analysis["best_experiment"] = {
                    "experiment_id": df.loc[best_idx, 'experiment_id'],
                    "parameters": df.loc[best_idx, 'parameters'],
                    "results": df.loc[best_idx, 'results']
                }
            
            # 参数重要性分析（简化版）
            for param in ['learning_rate', 'batch_size', 'epochs']:
                if param in str(df['parameters'].values):
                    param_values = []
                    scores = []
                    
                    for _, row in df.iterrows():
                        if isinstance(row['parameters'], dict) and param in row['parameters']:
                            param_values.append(row['parameters'][param])
                            if isinstance(row['results'], dict):
                                scores.append(row['results'].get('validation_score', 0))
                    
                    if param_values and scores:
                        correlation = np.corrcoef(param_values, scores)[0, 1]
                        analysis["parameter_importance"][param] = abs(correlation)
            
            return analysis
        
        # 将方法绑定到类
        self.log_experiment = log_experiment.__get__(self, type(self))
        self.analyze_experiments = analyze_experiments.__get__(self, type(self))
        
        return {
            "logging_template": {
                "required_fields": ["parameters", "results", "timestamp"],
                "recommended_fields": ["experiment_id", "metadata", "status"],
                "metadata_suggestions": [
                    "model_architecture",
                    "dataset_info",
                    "hardware_info",
                    "random_seed",
                    "training_time"
                ]
            },
            "analysis_features": [
                "best_experiment_identification",
                "parameter_importance_ranking",
                "convergence_analysis",
                "resource_usage_tracking"
            ]
        }
    
    def generate_tuning_checklist(self):
        """生成调参检查清单"""
        
        checklist = {
            "pre_tuning": [
                "✓ 确定任务类型和评估指标",
                "✓ 准备训练、验证、测试数据集",
                "✓ 估算可用计算资源",
                "✓ 设置实验记录系统",
                "✓ 确定基线模型性能"
            ],
            
            "during_tuning": [
                "✓ 从学习率开始调参",
                "✓ 监控训练和验证损失",
                "✓ 使用早停避免过拟合",
                "✓ 记录每次实验结果",
                "✓ 定期分析参数重要性"
            ],
            
            "post_tuning": [
                "✓ 在测试集上评估最佳模型",
                "✓ 验证结果的可重现性",
                "✓ 分析失败的实验",
                "✓ 总结调参经验",
                "✓ 保存最佳模型和配置"
            ],
            
            "common_pitfalls": [
                "❌ 在测试集上选择超参数",
                "❌ 忽略随机种子的影响",
                "❌ 过度拟合验证集",
                "❌ 忽略计算资源限制",
                "❌ 不记录实验过程"
            ]
        }
        
        return checklist

# 使用示例
def demonstrate_best_practices():
    """演示调参最佳实践"""
    
    # 创建最佳实践指南
    tuning_guide = HyperparameterTuningBestPractices("sentiment_analysis_project")
    
    print("=== 超参数调优最佳实践指南 ===\n")
    
    # 1. 从粗到细的调参策略
    print("1. 从粗到细的调参策略:")
    strategy = tuning_guide.coarse_to_fine_tuning_strategy()
    for phase, config in strategy.items():
        print(f"\n{phase.upper()}:")
        print(f"  描述: {config['description']}")
        print(f"  最大试验次数: {config['max_trials']}")
        print(f"  关键参数: {list(config['parameters'].keys())}")
    
    # 2. 资源感知的调参计划
    print("\n\n2. 资源感知的调参计划:")
    for gpu_hours in [5, 25, 100]:
        plan = tuning_guide.resource_aware_tuning_plan(gpu_hours, 16)
        print(f"\n{gpu_hours} GPU小时方案:")
        print(f"  类型: {plan['description']}")
        print(f"  关键参数: {plan['focus_parameters']}")
        print(f"  最大试验: {plan['max_trials']}")
    
    # 3. 验证集选择指南
    print("\n\n3. 验证集选择指南:")
    validation_guide = tuning_guide.validation_based_selection_guide()
    print("数据集划分策略:")
    for dataset_type, strategy in validation_guide["validation_split_strategy"].items():
        print(f"  {dataset_type}: {strategy['strategy']} - {strategy['reason']}")
    
    # 4. 实验记录系统
    print("\n\n4. 实验记录系统:")
    logging_system = tuning_guide.experiment_logging_system()
    print("必需字段:", logging_system["logging_template"]["required_fields"])
    print("推荐字段:", logging_system["logging_template"]["recommended_fields"])
    
    # 5. 调参检查清单
    print("\n\n5. 调参检查清单:")
    checklist = tuning_guide.generate_tuning_checklist()
    for phase, items in checklist.items():
        print(f"\n{phase.upper()}:")
        for item in items:
            print(f"  {item}")
    
    # 模拟记录实验
    print("\n\n6. 实验记录示例:")
    sample_params = {
        "learning_rate": 2e-5,
        "batch_size": 16,
        "epochs": 3,
        "warmup_ratio": 0.1
    }
    
    sample_results = {
        "validation_score": 0.85,
        "validation_loss": 0.45,
        "training_time": 1200
    }
    
    experiment_id = tuning_guide.log_experiment(sample_params, sample_results)
    print(f"记录实验: {experiment_id}")
    
    # 分析实验结果
    analysis = tuning_guide.analyze_experiments()
    print(f"实验分析: {analysis}")

# 运行演示
demonstrate_best_practices()
```

通过系统的超参数调优最佳实践，我们可以更高效地找到最优配置，避免常见陷阱，并建立可重现的实验流程。关键要点包括：

1. **分阶段调优**：从粗调到精调，逐步缩小搜索空间
2. **资源感知**：根据可用计算资源制定合理的调优计划
3. **验证驱动**：始终基于验证集性能进行参数选择
4. **完整记录**：记录所有实验过程，便于分析和复现
5. **避免陷阱**：不在测试集上调参，注意随机种子影响

这些实践经验来自于大量的实际项目，遵循这些指导原则可以显著提高调参效率和最终模型性能。


### 6.1 自动化超参数搜索

手动调参虽然能够积累经验，但效率较低且容易陷入局部最优。自动化超参数搜索技术可以更系统地探索参数空间，找到更好的配置。

**网格搜索（Grid Search）**：在预定义的参数网格中穷举所有组合。适用于参数空间较小的情况。

**随机搜索（Random Search）**：在参数空间中随机采样。相比网格搜索，随机搜索在高维空间中通常更有效。

**贝叶斯优化（Bayesian Optimization）**：基于先验知识和历史实验结果，智能地选择下一组参数。适用于评估成本高的场景。

**超带算法（Hyperband）**：结合了随机搜索和早停策略，能够高效地分配计算资源。

```python
import optuna
import numpy as np
from sklearn.model_selection import cross_val_score
import joblib
from typing import Dict, Any, Callable

class HyperparameterOptimizer:
    """超参数优化器"""
    
    def __init__(self, objective_function: Callable, direction='minimize'):
        self.objective_function = objective_function
        self.direction = direction
        self.study = None
        self.best_params = None
        
    def define_search_space(self, trial):
        """定义搜索空间"""
        # 学习率搜索空间（对数尺度）
        learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-3, log=True)
        
        # 批次大小搜索空间（2的幂次）
        batch_size = trial.suggest_categorical('batch_size', [4, 8, 16, 32, 64])
        
        # 训练轮数
        epochs = trial.suggest_int('epochs', 1, 10)
        
        # 预热比例
        warmup_ratio = trial.suggest_float('warmup_ratio', 0.0, 0.2)
        
        # 权重衰减
        weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-1, log=True)
        
        # LoRA特定参数（如果使用LoRA）
        lora_r = trial.suggest_categorical('lora_r', [4, 8, 16, 32, 64])
        lora_alpha = trial.suggest_categorical('lora_alpha', [8, 16, 32, 64, 128])
        lora_dropout = trial.suggest_float('lora_dropout', 0.0, 0.3)
        
        # 梯度累积步数
        gradient_accumulation_steps = trial.suggest_categorical(
            'gradient_accumulation_steps', [1, 2, 4, 8]
        )
        
        return {
            'learning_rate': learning_rate,
            'batch_size': batch_size,
            'epochs': epochs,
            'warmup_ratio': warmup_ratio,
            'weight_decay': weight_decay,
            'lora_r': lora_r,
            'lora_alpha': lora_alpha,
            'lora_dropout': lora_dropout,
            'gradient_accumulation_steps': gradient_accumulation_steps
        }
    
    def optimize(self, n_trials=50, timeout=None):
        """执行超参数优化"""
        
        def objective(trial):
            # 获取当前试验的参数
            params = self.define_search_space(trial)
            
            # 调用目标函数
            try:
                score = self.objective_function(params)
                return score
            except Exception as e:
                print(f"Trial failed with params {params}: {e}")
                # 返回一个较差的分数
                return float('inf') if self.direction == 'minimize' else float('-inf')
        
        # 创建研究对象
        self.study = optuna.create_study(direction=self.direction)
        
        # 执行优化
        self.study.optimize(objective, n_trials=n_trials, timeout=timeout)
        
        # 保存最佳参数
        self.best_params = self.study.best_params
        
        return self.best_params, self.study.best_value
    
    def get_optimization_history(self):
        """获取优化历史"""
        if self.study is None:
            return None
        
        trials = self.study.trials
        history = {
            'trial_numbers': [t.number for t in trials],
            'values': [t.value for t in trials if t.value is not None],
            'params': [t.params for t in trials]
        }
        
        return history
    
    def plot_optimization_history(self):
        """绘制优化历史"""
        if self.study is None:
            print("No optimization study found. Run optimize() first.")
            return
        
        # 使用optuna内置的可视化功能
        try:
            import optuna.visualization as vis
            
            # 优化历史
            fig1 = vis.plot_optimization_history(self.study)
            fig1.show()
            
            # 参数重要性
            fig2 = vis.plot_param_importances(self.study)
            fig2.show()
            
            # 参数关系
            fig3 = vis.plot_parallel_coordinate(self.study)
            fig3.show()
            
        except ImportError:
            print("Please install plotly for visualization: pip install plotly")

class AdaptiveLearningRateScheduler:
    """自适应学习率调度器"""
    
    def __init__(self, optimizer, patience=5, factor=0.5, min_lr=1e-7):
        self.optimizer = optimizer
        self.patience = patience
        self.factor = factor
        self.min_lr = min_lr
        self.best_loss = float('inf')
        self.wait = 0
        self.lr_history = []
        
    def step(self, val_loss):
        """根据验证损失调整学习率"""
        current_lr = self.optimizer.param_groups[0]['lr']
        self.lr_history.append(current_lr)
        
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.wait = 0
        else:
            self.wait += 1
            
            if self.wait >= self.patience:
                new_lr = max(current_lr * self.factor, self.min_lr)
                
                if new_lr < current_lr:
                    print(f"Reducing learning rate from {current_lr:.2e} to {new_lr:.2e}")
                    
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = new_lr
                    
                    self.wait = 0
                
                return True  # 学习率已调整
        
        return False  # 学习率未调整

def create_hyperparameter_search_example():
    """创建超参数搜索示例"""
    
    def mock_objective_function(params):
        """模拟目标函数（实际应用中这里是完整的训练和评估过程）"""
        
        # 模拟训练过程的评估指标
        # 在实际应用中，这里应该是完整的模型训练和验证过程
        
        lr = params['learning_rate']
        batch_size = params['batch_size']
        epochs = params['epochs']
        
        # 简单的模拟函数：学习率和批次大小的组合效果
        # 实际中这里应该调用真实的训练函数
        simulated_score = (
            0.1 * np.log10(lr) +  # 学习率影响
            0.05 * np.log2(batch_size) +  # 批次大小影响
            0.02 * epochs +  # 训练轮数影响
            np.random.normal(0, 0.01)  # 添加噪声
        )
        
        # 转换为准确率（0-1之间）
        accuracy = 1 / (1 + np.exp(-simulated_score))
        
        return -accuracy  # 返回负值因为我们要最小化
    
    # 创建优化器
    optimizer = HyperparameterOptimizer(mock_objective_function, direction='minimize')
    
    # 执行优化
    print("开始超参数搜索...")
    best_params, best_score = optimizer.optimize(n_trials=30)
    
    print(f"\n最佳参数: {best_params}")
    print(f"最佳分数: {-best_score:.4f}")  # 转换回正值
    
    # 获取优化历史
    history = optimizer.get_optimization_history()
    
    # 分析结果
    print(f"\n优化历史:")
    print(f"总试验次数: {len(history['trial_numbers'])}")
    print(f"最佳试验: {np.argmin(history['values'])}")
    
    return optimizer

# 运行示例
print("=== 自动化超参数搜索演示 ===")
optimizer_example = create_hyperparameter_search_example()
```

### 6.2 多目标优化

在实际应用中，我们往往需要在多个目标之间进行权衡，如准确率vs推理速度、性能vs显存使用等。多目标优化可以帮助我们找到这些目标之间的最佳平衡点。

```python
import numpy as np
from typing import List, Tuple
import matplotlib.pyplot as plt

class MultiObjectiveOptimizer:
    """多目标优化器"""
    
    def __init__(self, objectives: List[str], directions: List[str]):
        """
        objectives: 目标名称列表
        directions: 优化方向列表 ('minimize' 或 'maximize')
        """
        self.objectives = objectives
        self.directions = directions
        self.pareto_front = []
        self.all_solutions = []
    
    def is_dominated(self, solution1: Dict, solution2: Dict) -> bool:
        """检查solution1是否被solution2支配"""
        
        better_in_all = True
        better_in_at_least_one = False
        
        for obj, direction in zip(self.objectives, self.directions):
            val1 = solution1['scores'][obj]
            val2 = solution2['scores'][obj]
            
            if direction == 'minimize':
                if val1 < val2:
                    better_in_at_least_one = True
                elif val1 > val2:
                    better_in_all = False
            else:  # maximize
                if val1 > val2:
                    better_in_at_least_one = True
                elif val1 < val2:
                    better_in_all = False
        
        return better_in_all and better_in_at_least_one
    
    def update_pareto_front(self, new_solution: Dict):
        """更新帕累托前沿"""
        
        # 检查新解是否被现有解支配
        is_dominated_by_existing = False
        for existing_solution in self.pareto_front:
            if self.is_dominated(new_solution, existing_solution):
                is_dominated_by_existing = True
                break
        
        if not is_dominated_by_existing:
            # 移除被新解支配的现有解
            self.pareto_front = [
                sol for sol in self.pareto_front 
                if not self.is_dominated(sol, new_solution)
            ]
            
            # 添加新解
            self.pareto_front.append(new_solution)
        
        # 记录所有解
        self.all_solutions.append(new_solution)
    
    def optimize_multi_objective(self, objective_function: Callable, 
                                search_space_generator: Callable, 
                                n_trials: int = 100):
        """执行多目标优化"""
        
        for trial in range(n_trials):
            # 生成参数
            params = search_space_generator()
            
            try:
                # 评估多个目标
                scores = objective_function(params)
                
                solution = {
                    'params': params,
                    'scores': scores,
                    'trial': trial
                }
                
                # 更新帕累托前沿
                self.update_pareto_front(solution)
                
                if trial % 10 == 0:
                    print(f"Trial {trial}: Pareto front size = {len(self.pareto_front)}")
                    
            except Exception as e:
                print(f"Trial {trial} failed: {e}")
                continue
        
        return self.pareto_front
    
    def plot_pareto_front(self, obj1_idx: int = 0, obj2_idx: int = 1):
        """绘制帕累托前沿（二维）"""
        
        if len(self.objectives) < 2:
            print("Need at least 2 objectives for plotting")
            return
        
        obj1_name = self.objectives[obj1_idx]
        obj2_name = self.objectives[obj2_idx]
        
        # 提取所有解的目标值
        all_obj1 = [sol['scores'][obj1_name] for sol in self.all_solutions]
        all_obj2 = [sol['scores'][obj2_name] for sol in self.all_solutions]
        
        # 提取帕累托前沿的目标值
        pareto_obj1 = [sol['scores'][obj1_name] for sol in self.pareto_front]
        pareto_obj2 = [sol['scores'][obj2_name] for sol in self.pareto_front]
        
        plt.figure(figsize=(10, 8))
        
        # 绘制所有解
        plt.scatter(all_obj1, all_obj2, alpha=0.6, c='lightblue', label='All solutions')
        
        # 绘制帕累托前沿
        plt.scatter(pareto_obj1, pareto_obj2, c='red', s=100, label='Pareto front')
        
        # 连接帕累托前沿点
        if len(pareto_obj1) > 1:
            # 按第一个目标排序
            sorted_indices = np.argsort(pareto_obj1)
            sorted_obj1 = [pareto_obj1[i] for i in sorted_indices]
            sorted_obj2 = [pareto_obj2[i] for i in sorted_indices]
            plt.plot(sorted_obj1, sorted_obj2, 'r--', alpha=0.7)
        
        plt.xlabel(f'{obj1_name} ({self.directions[obj1_idx]})')
        plt.ylabel(f'{obj2_name} ({self.directions[obj2_idx]})')
        plt.title('Multi-Objective Optimization Results')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def get_best_compromise_solution(self, weights: Dict[str, float] = None):
        """获取最佳折中解"""
        
        if not self.pareto_front:
            return None
        
        if weights is None:
            # 默认等权重
            weights = {obj: 1.0 for obj in self.objectives}
        
        best_solution = None
        best_weighted_score = float('-inf')
        
        for solution in self.pareto_front:
            weighted_score = 0
            
            for obj, direction in zip(self.objectives, self.directions):
                score = solution['scores'][obj]
                weight = weights.get(obj, 1.0)
                
                if direction == 'maximize':
                    weighted_score += weight * score
                else:  # minimize
                    weighted_score -= weight * score
            
            if weighted_score > best_weighted_score:
                best_weighted_score = weighted_score
                best_solution = solution
        
        return best_solution

def demonstrate_multi_objective_optimization():
    """演示多目标优化"""
    
    def multi_objective_function(params):
        """多目标函数：准确率 vs 推理速度 vs 显存使用"""
        
        lr = params['learning_rate']
        batch_size = params['batch_size']
        model_size = params['model_size']
        
        # 模拟准确率（要最大化）
        accuracy = 0.8 + 0.1 * np.log10(lr / 1e-5) + 0.05 * np.log2(batch_size / 8)
        accuracy = max(0.5, min(0.99, accuracy + np.random.normal(0, 0.02)))
        
        # 模拟推理速度（要最大化，这里用倒数表示时间）
        inference_speed = 100 / (model_size * batch_size**0.5)
        inference_speed = max(1, inference_speed + np.random.normal(0, 2))
        
        # 模拟显存使用（要最小化）
        memory_usage = model_size * batch_size * 0.1
        memory_usage = max(1, memory_usage + np.random.normal(0, 0.5))
        
        return {
            'accuracy': accuracy,
            'inference_speed': inference_speed,
            'memory_usage': memory_usage
        }
    
    def generate_search_space():
        """生成搜索空间"""
        return {
            'learning_rate': np.random.uniform(1e-6, 1e-3),
            'batch_size': np.random.choice([4, 8, 16, 32]),
            'model_size': np.random.choice([1, 3, 7, 13])  # 模型大小（B参数）
        }
    
    # 创建多目标优化器
    optimizer = MultiObjectiveOptimizer(
        objectives=['accuracy', 'inference_speed', 'memory_usage'],
        directions=['maximize', 'maximize', 'minimize']
    )
    
    # 执行优化
    print("开始多目标优化...")
    pareto_front = optimizer.optimize_multi_objective(
        multi_objective_function, 
        generate_search_space, 
        n_trials=200
    )
    
    print(f"\n帕累托前沿包含 {len(pareto_front)} 个解")
    
    # 显示帕累托前沿的解
    print("\n帕累托前沿解:")
    for i, solution in enumerate(pareto_front[:5]):  # 只显示前5个
        params = solution['params']
        scores = solution['scores']
        print(f"解 {i+1}:")
        print(f"  参数: LR={params['learning_rate']:.2e}, "
              f"Batch={params['batch_size']}, Model={params['model_size']}B")
        print(f"  目标: 准确率={scores['accuracy']:.3f}, "
              f"推理速度={scores['inference_speed']:.1f}, "
              f"显存={scores['memory_usage']:.1f}GB")
        print()
    
    # 绘制帕累托前沿
    optimizer.plot_pareto_front(0, 2)  # 准确率 vs 显存使用
    
    # 获取最佳折中解
    best_compromise = optimizer.get_best_compromise_solution({
        'accuracy': 0.5,
        'inference_speed': 0.3,
        'memory_usage': 0.2
    })
    
    if best_compromise:
        print("最佳折中解:")
        print(f"  参数: {best_compromise['params']}")
        print(f"  目标: {best_compromise['scores']}")
    
    return optimizer

# 运行多目标优化演示
print("\n=== 多目标优化演示 ===")
multi_obj_optimizer = demonstrate_multi_objective_optimization()
```

### 6.3 超参数调优的实用建议

**阶段性调优策略**：
1. **粗调阶段**：使用较大的搜索范围，快速确定参数的大致区间
2. **精调阶段**：在粗调结果的基础上，缩小搜索范围进行精细调优
3. **验证阶段**：在多个随机种子下验证最佳参数的稳定性

**资源分配策略**：
- 将80%的计算资源用于搜索学习率和批次大小
- 将15%的资源用于调优训练轮数和正则化参数
- 将5%的资源用于其他细节参数

**经验性规则**：
- 学习率通常是最重要的超参数，优先调优
- 批次大小的选择要考虑显存限制和梯度累积
- 训练轮数要结合早停策略，避免过拟合
- 不同任务类型有不同的参数敏感性

通过系统的超参数调优，结合自动化搜索技术和多目标优化方法，我们可以在有限的计算资源下找到最适合特定任务的参数配置。关键是要理解每个参数的作用机制，合理分配调优资源，并建立有效的实验管理流程。