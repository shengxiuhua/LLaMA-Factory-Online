# 1. Learning Rate (学习率) 设置技巧

## 1.1 学习率的本质与影响机制

学习率是控制模型参数更新幅度的关键超参数，它直接影响训练的收敛速度和最终性能。在大模型微调中，学习率的设置尤为关键，因为我们需要在保持预训练知识和学习新任务之间找到平衡。

**学习率过大的问题**：会导致参数更新步长过大，可能跳过最优解，甚至导致训练发散。在微调场景下，过大的学习率还可能破坏预训练模型已经学到的有用特征。

**学习率过小的问题**：会导致训练收敛缓慢，可能陷入局部最优解，或者在有限的训练时间内无法充分学习新任务。

**微调中的学习率特点**：相比从头训练，微调通常使用更小的学习率，因为预训练模型已经在一个相对较好的参数空间中，只需要进行微调整。典型的微调学习率比预训练学习率小1-2个数量级。

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup

class LearningRateScheduler:
    """学习率调度器"""
    
    def __init__(self, optimizer, scheduler_type='linear', **kwargs):
        self.optimizer = optimizer
        self.scheduler_type = scheduler_type
        self.scheduler = self._create_scheduler(**kwargs)
        self.lr_history = []
    
    def _create_scheduler(self, **kwargs):
        """创建学习率调度器"""
        if self.scheduler_type == 'linear':
            return get_linear_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=kwargs.get('num_warmup_steps', 0),
                num_training_steps=kwargs.get('num_training_steps', 1000)
            )
        elif self.scheduler_type == 'cosine':
            return get_cosine_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=kwargs.get('num_warmup_steps', 0),
                num_training_steps=kwargs.get('num_training_steps', 1000)
            )
        elif self.scheduler_type == 'exponential':
            return torch.optim.lr_scheduler.ExponentialLR(
                self.optimizer,
                gamma=kwargs.get('gamma', 0.95)
            )
        elif self.scheduler_type == 'step':
            return torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=kwargs.get('step_size', 100),
                gamma=kwargs.get('gamma', 0.1)
            )
        else:
            return None
    
    def step(self):
        """更新学习率"""
        if self.scheduler:
            self.scheduler.step()
        
        # 记录当前学习率
        current_lr = self.optimizer.param_groups[0]['lr']
        self.lr_history.append(current_lr)
    
    def get_current_lr(self):
        """获取当前学习率"""
        return self.optimizer.param_groups[0]['lr']
    
    def plot_lr_schedule(self, steps=None):
        """绘制学习率变化曲线"""
        if steps is None:
            steps = range(len(self.lr_history))
        
        plt.figure(figsize=(10, 6))
        plt.plot(steps, self.lr_history)
        plt.xlabel('Training Steps')
        plt.ylabel('Learning Rate')
        plt.title(f'{self.scheduler_type.capitalize()} Learning Rate Schedule')
        plt.grid(True)
        plt.show()

class LearningRateFinder:
    """学习率寻找器"""
    
    def __init__(self, model, optimizer, criterion, device):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
    
    def find_lr(self, train_loader, start_lr=1e-7, end_lr=10, num_iter=100):
        """寻找最优学习率"""
        # 保存原始状态
        original_state = self.model.state_dict()
        original_optimizer_state = self.optimizer.state_dict()
        
        # 设置学习率范围
        lr_mult = (end_lr / start_lr) ** (1 / num_iter)
        lr = start_lr
        self.optimizer.param_groups[0]['lr'] = lr
        
        # 记录数据
        lrs = []
        losses = []
        
        # 训练循环
        self.model.train()
        for i, (inputs, targets) in enumerate(train_loader):
            if i >= num_iter:
                break
            
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            
            # 前向传播
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
            
            # 记录
            lrs.append(lr)
            losses.append(loss.item())
            
            # 反向传播
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            # 更新学习率
            lr *= lr_mult
            self.optimizer.param_groups[0]['lr'] = lr
        
        # 恢复原始状态
        self.model.load_state_dict(original_state)
        self.optimizer.load_state_dict(original_optimizer_state)
        
        return lrs, losses
    
    def plot_lr_find(self, lrs, losses):
        """绘制学习率寻找结果"""
        plt.figure(figsize=(12, 4))
        
        # 损失vs学习率
        plt.subplot(1, 2, 1)
        plt.plot(lrs, losses)
        plt.xscale('log')
        plt.xlabel('Learning Rate')
        plt.ylabel('Loss')
        plt.title('Loss vs Learning Rate')
        plt.grid(True)
        
        # 损失变化率vs学习率
        plt.subplot(1, 2, 2)
        # 计算损失的变化率
        loss_changes = np.gradient(losses)
        plt.plot(lrs, loss_changes)
        plt.xscale('log')
        plt.xlabel('Learning Rate')
        plt.ylabel('Loss Change Rate')
        plt.title('Loss Change Rate vs Learning Rate')
        plt.grid(True)
        
        plt.tight_layout()
        plt.show()
        
        # 推荐学习率
        min_loss_idx = np.argmin(losses)
        recommended_lr = lrs[min_loss_idx] / 10  # 通常选择最小损失对应学习率的1/10
        
        print(f"推荐学习率: {recommended_lr:.2e}")
        return recommended_lr

def get_layer_wise_lr(model, base_lr=2e-5, decay_factor=0.9):
    """分层学习率设置"""
    layer_wise_params = []
    
    # 获取模型层数
    if hasattr(model, 'transformer'):
        # Transformer模型
        num_layers = len(model.transformer.h)
        
        # 为不同层设置不同学习率
        for i, layer in enumerate(model.transformer.h):
            lr = base_lr * (decay_factor ** (num_layers - i - 1))
            layer_wise_params.append({
                'params': layer.parameters(),
                'lr': lr
            })
        
        # 输出层使用基础学习率
        if hasattr(model, 'lm_head'):
            layer_wise_params.append({
                'params': model.lm_head.parameters(),
                'lr': base_lr
            })
    
    return layer_wise_params

# 使用示例
def demonstrate_lr_scheduling():
    """演示学习率调度"""
    # 创建示例模型和优化器
    model = nn.Linear(100, 10)
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    
    # 不同类型的学习率调度器
    schedulers = {
        'linear': LearningRateScheduler(
            optimizer, 'linear', 
            num_warmup_steps=100, 
            num_training_steps=1000
        ),
        'cosine': LearningRateScheduler(
            optimizer, 'cosine',
            num_warmup_steps=100,
            num_training_steps=1000
        ),
        'exponential': LearningRateScheduler(
            optimizer, 'exponential',
            gamma=0.95
        )
    }
    
    # 模拟训练过程
    for scheduler_name, scheduler in schedulers.items():
        print(f"\n{scheduler_name.upper()} 调度器:")
        
        # 重置优化器
        for param_group in optimizer.param_groups:
            param_group['lr'] = 2e-5
        
        scheduler.lr_history = []  # 重置历史记录
        
        # 模拟1000步训练
        for step in range(1000):
            scheduler.step()
            
            if step % 200 == 0:
                print(f"Step {step}: LR = {scheduler.get_current_lr():.2e}")

# 微调任务的学习率推荐
def get_task_specific_lr(task_type, model_size, method='lora'):
    """根据任务类型推荐学习率"""
    
    base_lrs = {
        'classification': {
            'full': {'small': 2e-5, 'medium': 1e-5, 'large': 5e-6},
            'lora': {'small': 1e-4, 'medium': 5e-5, 'large': 2e-5},
            'adapter': {'small': 1e-4, 'medium': 5e-5, 'large': 2e-5}
        },
        'generation': {
            'full': {'small': 1e-5, 'medium': 5e-6, 'large': 2e-6},
            'lora': {'small': 5e-5, 'medium': 2e-5, 'large': 1e-5},
            'adapter': {'small': 5e-5, 'medium': 2e-5, 'large': 1e-5}
        },
        'qa': {
            'full': {'small': 3e-5, 'medium': 1e-5, 'large': 5e-6},
            'lora': {'small': 1e-4, 'medium': 5e-5, 'large': 2e-5},
            'adapter': {'small': 1e-4, 'medium': 5e-5, 'large': 2e-5}
        }
    }
    
    # 根据模型参数量确定大小类别
    size_category = 'small' if '1b' in model_size or '3b' in model_size else \
                   'medium' if '7b' in model_size or '13b' in model_size else 'large'
    
    recommended_lr = base_lrs.get(task_type, {}).get(method, {}).get(size_category, 2e-5)
    
    return {
        'recommended_lr': recommended_lr,
        'lr_range': (recommended_lr * 0.1, recommended_lr * 10),
        'warmup_ratio': 0.1 if method == 'full' else 0.05,
        'scheduler': 'cosine' if task_type == 'generation' else 'linear'
    }

# 使用示例
print("=== 学习率调度演示 ===")
demonstrate_lr_scheduling()

print("\n=== 任务特定学习率推荐 ===")
tasks = ['classification', 'generation', 'qa']
models = ['7b', '13b']
methods = ['full', 'lora']

for task in tasks:
    for model in models:
        for method in methods:
            lr_config = get_task_specific_lr(task, model, method)
            print(f"{task} + {model} + {method}: {lr_config['recommended_lr']:.2e}")
```

## 1.2 学习率调度策略

**Warmup策略**：在训练初期使用较小的学习率，然后逐渐增加到目标学习率。这种策略可以避免训练初期的不稳定，特别适用于大模型微调。

**线性衰减**：在warmup阶段后，学习率线性衰减到零。这是最常用的策略，简单有效。

**余弦衰减**：学习率按照余弦函数衰减，相比线性衰减，余弦衰减在训练后期提供更平滑的学习率变化。

**分层学习率**：对模型的不同层使用不同的学习率，通常底层（接近输入）使用较小的学习率，顶层（接近输出）使用较大的学习率。

