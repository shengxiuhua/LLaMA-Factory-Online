
## 4. Epochs 与 Overfitting (过拟合)

### 4.1 训练轮数的选择策略

训练轮数（Epochs）的选择需要在充分学习和避免过拟合之间找到平衡。在大模型微调中，由于模型容量大而训练数据相对较少，过拟合是一个常见问题。

**过拟合的表现**：训练损失持续下降，但验证损失开始上升；训练准确率很高，但验证准确率停滞或下降；模型在训练集上表现完美，但在测试集上表现不佳。

**早停策略**：监控验证集性能，当验证性能不再提升时停止训练。

**学习率衰减**：在训练后期降低学习率，进行精细调整。

```python
import numpy as np
import matplotlib.pyplot as plt
from collections import deque

class EarlyStopping:
    """早停机制"""
    
    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1
            
        if self.counter >= self.patience:
            if self.restore_best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False
    
    def save_checkpoint(self, model):
        """保存最佳权重"""
        self.best_weights = model.state_dict().copy()

class OverfittingDetector:
    """过拟合检测器"""
    
    def __init__(self, window_size=5, threshold=0.1):
        self.window_size = window_size
        self.threshold = threshold
        self.train_losses = deque(maxlen=window_size)
        self.val_losses = deque(maxlen=window_size)
        
    def update(self, train_loss, val_loss):
        """更新损失历史"""
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        
    def is_overfitting(self):
        """检测是否过拟合"""
        if len(self.train_losses) < self.window_size:
            return False
            
        # 计算训练损失和验证损失的趋势
        train_trend = np.polyfit(range(self.window_size), list(self.train_losses), 1)[0]
        val_trend = np.polyfit(range(self.window_size), list(self.val_losses), 1)[0]
        
        # 训练损失下降但验证损失上升
        if train_trend < -self.threshold and val_trend > self.threshold:
            return True
            
        # 验证损失显著高于训练损失
        recent_train_loss = np.mean(list(self.train_losses)[-3:])
        recent_val_loss = np.mean(list(self.val_losses)[-3:])
        
        if recent_val_loss > recent_train_loss * 1.5:
            return True
            
        return False
    
    def get_overfitting_score(self):
        """获取过拟合程度评分"""
        if len(self.train_losses) < self.window_size:
            return 0.0
            
        recent_train_loss = np.mean(list(self.train_losses)[-3:])
        recent_val_loss = np.mean(list(self.val_losses)[-3:])
        
        # 计算过拟合评分
        if recent_train_loss > 0:
            overfitting_score = max(0, (recent_val_loss - recent_train_loss) / recent_train_loss)
        else:
            overfitting_score = 0.0
            
        return min(1.0, overfitting_score)

def recommend_epochs(dataset_size, model_size, task_complexity='medium'):
    """推荐训练轮数"""
    
    # 基础轮数推荐
    base_epochs = {
        'simple': {'small': 5, 'medium': 3, 'large': 2},
        'medium': {'small': 8, 'medium': 5, 'large': 3},
        'complex': {'small': 12, 'medium': 8, 'large': 5}
    }
    
    # 根据数据集大小调整
    if dataset_size < 1000:
        size_factor = 1.5  # 小数据集需要更多轮数
    elif dataset_size < 10000:
        size_factor = 1.0
    else:
        size_factor = 0.8  # 大数据集可以减少轮数
    
    # 确定模型大小类别
    if '1b' in model_size or '3b' in model_size:
        model_category = 'small'
    elif '7b' in model_size or '13b' in model_size:
        model_category = 'medium'
    else:
        model_category = 'large'
    
    recommended_epochs = int(base_epochs[task_complexity][model_category] * size_factor)
    
    return {
        'recommended_epochs': recommended_epochs,
        'min_epochs': max(1, recommended_epochs - 2),
        'max_epochs': recommended_epochs + 3,
        'early_stopping_patience': max(2, recommended_epochs // 3),
        'lr_decay_start': max(1, recommended_epochs // 2)
    }

def simulate_training_curves():
    """模拟训练曲线以演示过拟合检测"""
    
    # 模拟不同的训练场景
    scenarios = {
        'normal': {
            'train_losses': [2.0, 1.5, 1.2, 1.0, 0.9, 0.85, 0.82, 0.80, 0.79, 0.78],
            'val_losses': [2.1, 1.6, 1.3, 1.1, 1.0, 0.95, 0.93, 0.92, 0.91, 0.90]
        },
        'overfitting': {
            'train_losses': [2.0, 1.5, 1.0, 0.7, 0.5, 0.3, 0.2, 0.1, 0.05, 0.02],
            'val_losses': [2.1, 1.6, 1.2, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7]
        },
        'underfitting': {
            'train_losses': [2.0, 1.9, 1.8, 1.75, 1.7, 1.68, 1.66, 1.65, 1.64, 1.63],
            'val_losses': [2.1, 2.0, 1.9, 1.85, 1.8, 1.78, 1.76, 1.75, 1.74, 1.73]
        }
    }
    
    plt.figure(figsize=(15, 5))
    
    for i, (scenario_name, losses) in enumerate(scenarios.items()):
        plt.subplot(1, 3, i+1)
        
        epochs = range(1, len(losses['train_losses']) + 1)
        plt.plot(epochs, losses['train_losses'], 'b-', label='训练损失')
        plt.plot(epochs, losses['val_losses'], 'r-', label='验证损失')
        
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title(f'{scenario_name.capitalize()} Training')
        plt.legend()
        plt.grid(True)
        
        # 使用过拟合检测器
        detector = OverfittingDetector()
        for epoch in range(len(losses['train_losses'])):
            detector.update(losses['train_losses'][epoch], losses['val_losses'][epoch])
            
            if epoch >= 4:  # 至少需要5个数据点
                if detector.is_overfitting():
                    plt.axvline(x=epoch+1, color='orange', linestyle='--', alpha=0.7)
                    plt.text(epoch+1, max(losses['val_losses']), 'Overfitting\nDetected', 
                            rotation=90, verticalalignment='top')
                    break
    
    plt.tight_layout()
    plt.show()

# 使用示例
print("=== 训练轮数推荐 ===")
datasets = [500, 2000, 10000]
models = ['7b', '13b', '30b']
complexities = ['simple', 'medium', 'complex']

for dataset_size in datasets:
    for model_size in models:
        for complexity in complexities:
            recommendation = recommend_epochs(dataset_size, model_size, complexity)
            print(f"数据集{dataset_size}, {model_size}模型, {complexity}任务: "
                  f"{recommendation['recommended_epochs']} epochs "
                  f"(范围: {recommendation['min_epochs']}-{recommendation['max_epochs']})")

print("\n=== 过拟合检测演示 ===")
simulate_training_curves()
```


### 6.1 自动化超参数搜索

手动调参虽然能够积累经验，但效率较低且容易陷入局部最优。自动化超参数搜索技术可以更系统地探索参数空间，找到更好的配置。

**网格搜索（Grid Search）**：在预定义的参数网格中穷举所有组合。适用于参数空间较小的情况。

**随机搜索（Random Search）**：在参数空间中随机采样。相比网格搜索，随机搜索在高维空间中通常更有效。

**贝叶斯优化（Bayesian Optimization）**：基于先验知识和历史实验结果，智能地选择下一组参数。适用于评估成本高的场景。

**超带算法（Hyperband）**：结合了随机搜索和早停策略，能够高效地分配计算资源。

```python
import optuna
import numpy as np
from sklearn.model_selection import cross_val_score
import joblib
from typing import Dict, Any, Callable

class HyperparameterOptimizer:
    """超参数优化器"""
    
    def __init__(self, objective_function: Callable, direction='minimize'):
        self.objective_function = objective_function
        self.direction = direction
        self.study = None
        self.best_params = None
        
    def define_search_space(self, trial):
        """定义搜索空间"""
        # 学习率搜索空间（对数尺度）
        learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-3, log=True)
        
        # 批次大小搜索空间（2的幂次）
        batch_size = trial.suggest_categorical('batch_size', [4, 8, 16, 32, 64])
        
        # 训练轮数
        epochs = trial.suggest_int('epochs', 1, 10)
        
        # 预热比例
        warmup_ratio = trial.suggest_float('warmup_ratio', 0.0, 0.2)
        
        # 权重衰减
        weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-1, log=True)
        
        # LoRA特定参数（如果使用LoRA）
        lora_r = trial.suggest_categorical('lora_r', [4, 8, 16, 32, 64])
        lora_alpha = trial.suggest_categorical('lora_alpha', [8, 16, 32, 64, 128])
        lora_dropout = trial.suggest_float('lora_dropout', 0.0, 0.3)
        
        # 梯度累积步数
        gradient_accumulation_steps = trial.suggest_categorical(
            'gradient_accumulation_steps', [1, 2, 4, 8]
        )
        
        return {
            'learning_rate': learning_rate,
            'batch_size': batch_size,
            'epochs': epochs,
            'warmup_ratio': warmup_ratio,
            'weight_decay': weight_decay,
            'lora_r': lora_r,
            'lora_alpha': lora_alpha,
            'lora_dropout': lora_dropout,
            'gradient_accumulation_steps': gradient_accumulation_steps
        }
    
    def optimize(self, n_trials=50, timeout=None):
        """执行超参数优化"""
        
        def objective(trial):
            # 获取当前试验的参数
            params = self.define_search_space(trial)
            
            # 调用目标函数
            try:
                score = self.objective_function(params)
                return score
            except Exception as e:
                print(f"Trial failed with params {params}: {e}")
                # 返回一个较差的分数
                return float('inf') if self.direction == 'minimize' else float('-inf')
        
        # 创建研究对象
        self.study = optuna.create_study(direction=self.direction)
        
        # 执行优化
        self.study.optimize(objective, n_trials=n_trials, timeout=timeout)
        
        # 保存最佳参数
        self.best_params = self.study.best_params
        
        return self.best_params, self.study.best_value
    
    def get_optimization_history(self):
        """获取优化历史"""
        if self.study is None:
            return None
        
        trials = self.study.trials
        history = {
            'trial_numbers': [t.number for t in trials],
            'values': [t.value for t in trials if t.value is not None],
            'params': [t.params for t in trials]
        }
        
        return history
    
    def plot_optimization_history(self):
        """绘制优化历史"""
        if self.study is None:
            print("No optimization study found. Run optimize() first.")
            return
        
        # 使用optuna内置的可视化功能
        try:
            import optuna.visualization as vis
            
            # 优化历史
            fig1 = vis.plot_optimization_history(self.study)
            fig1.show()
            
            # 参数重要性
            fig2 = vis.plot_param_importances(self.study)
            fig2.show()
            
            # 参数关系
            fig3 = vis.plot_parallel_coordinate(self.study)
            fig3.show()
            
        except ImportError:
            print("Please install plotly for visualization: pip install plotly")

class AdaptiveLearningRateScheduler:
    """自适应学习率调度器"""
    
    def __init__(self, optimizer, patience=5, factor=0.5, min_lr=1e-7):
        self.optimizer = optimizer
        self.patience = patience
        self.factor = factor
        self.min_lr = min_lr
        self.best_loss = float('inf')
        self.wait = 0
        self.lr_history = []
        
    def step(self, val_loss):
        """根据验证损失调整学习率"""
        current_lr = self.optimizer.param_groups[0]['lr']
        self.lr_history.append(current_lr)
        
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.wait = 0
        else:
            self.wait += 1
            
            if self.wait >= self.patience:
                new_lr = max(current_lr * self.factor, self.min_lr)
                
                if new_lr < current_lr:
                    print(f"Reducing learning rate from {current_lr:.2e} to {new_lr:.2e}")
                    
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = new_lr
                    
                    self.wait = 0
                
                return True  # 学习率已调整
        
        return False  # 学习率未调整

def create_hyperparameter_search_example():
    """创建超参数搜索示例"""
    
    def mock_objective_function(params):
        """模拟目标函数（实际应用中这里是完整的训练和评估过程）"""
        
        # 模拟训练过程的评估指标
        # 在实际应用中，这里应该是完整的模型训练和验证过程
        
        lr = params['learning_rate']
        batch_size = params['batch_size']
        epochs = params['epochs']
        
        # 简单的模拟函数：学习率和批次大小的组合效果
        # 实际中这里应该调用真实的训练函数
        simulated_score = (
            0.1 * np.log10(lr) +  # 学习率影响
            0.05 * np.log2(batch_size) +  # 批次大小影响
            0.02 * epochs +  # 训练轮数影响
            np.random.normal(0, 0.01)  # 添加噪声
        )
        
        # 转换为准确率（0-1之间）
        accuracy = 1 / (1 + np.exp(-simulated_score))
        
        return -accuracy  # 返回负值因为我们要最小化
    
    # 创建优化器
    optimizer = HyperparameterOptimizer(mock_objective_function, direction='minimize')
    
    # 执行优化
    print("开始超参数搜索...")
    best_params, best_score = optimizer.optimize(n_trials=30)
    
    print(f"\n最佳参数: {best_params}")
    print(f"最佳分数: {-best_score:.4f}")  # 转换回正值
    
    # 获取优化历史
    history = optimizer.get_optimization_history()
    
    # 分析结果
    print(f"\n优化历史:")
    print(f"总试验次数: {len(history['trial_numbers'])}")
    print(f"最佳试验: {np.argmin(history['values'])}")
    
    return optimizer

# 运行示例
print("=== 自动化超参数搜索演示 ===")
optimizer_example = create_hyperparameter_search_example()
```

### 6.2 多目标优化

在实际应用中，我们往往需要在多个目标之间进行权衡，如准确率vs推理速度、性能vs显存使用等。多目标优化可以帮助我们找到这些目标之间的最佳平衡点。

```python
import numpy as np
from typing import List, Tuple
import matplotlib.pyplot as plt

class MultiObjectiveOptimizer:
    """多目标优化器"""
    
    def __init__(self, objectives: List[str], directions: List[str]):
        """
        objectives: 目标名称列表
        directions: 优化方向列表 ('minimize' 或 'maximize')
        """
        self.objectives = objectives
        self.directions = directions
        self.pareto_front = []
        self.all_solutions = []
    
    def is_dominated(self, solution1: Dict, solution2: Dict) -> bool:
        """检查solution1是否被solution2支配"""
        
        better_in_all = True
        better_in_at_least_one = False
        
        for obj, direction in zip(self.objectives, self.directions):
            val1 = solution1['scores'][obj]
            val2 = solution2['scores'][obj]
            
            if direction == 'minimize':
                if val1 < val2:
                    better_in_at_least_one = True
                elif val1 > val2:
                    better_in_all = False
            else:  # maximize
                if val1 > val2:
                    better_in_at_least_one = True
                elif val1 < val2:
                    better_in_all = False
        
        return better_in_all and better_in_at_least_one
    
    def update_pareto_front(self, new_solution: Dict):
        """更新帕累托前沿"""
        
        # 检查新解是否被现有解支配
        is_dominated_by_existing = False
        for existing_solution in self.pareto_front:
            if self.is_dominated(new_solution, existing_solution):
                is_dominated_by_existing = True
                break
        
        if not is_dominated_by_existing:
            # 移除被新解支配的现有解
            self.pareto_front = [
                sol for sol in self.pareto_front 
                if not self.is_dominated(sol, new_solution)
            ]
            
            # 添加新解
            self.pareto_front.append(new_solution)
        
        # 记录所有解
        self.all_solutions.append(new_solution)
    
    def optimize_multi_objective(self, objective_function: Callable, 
                                search_space_generator: Callable, 
                                n_trials: int = 100):
        """执行多目标优化"""
        
        for trial in range(n_trials):
            # 生成参数
            params = search_space_generator()
            
            try:
                # 评估多个目标
                scores = objective_function(params)
                
                solution = {
                    'params': params,
                    'scores': scores,
                    'trial': trial
                }
                
                # 更新帕累托前沿
                self.update_pareto_front(solution)
                
                if trial % 10 == 0:
                    print(f"Trial {trial}: Pareto front size = {len(self.pareto_front)}")
                    
            except Exception as e:
                print(f"Trial {trial} failed: {e}")
                continue
        
        return self.pareto_front
    
    def plot_pareto_front(self, obj1_idx: int = 0, obj2_idx: int = 1):
        """绘制帕累托前沿（二维）"""
        
        if len(self.objectives) < 2:
            print("Need at least 2 objectives for plotting")
            return
        
        obj1_name = self.objectives[obj1_idx]
        obj2_name = self.objectives[obj2_idx]
        
        # 提取所有解的目标值
        all_obj1 = [sol['scores'][obj1_name] for sol in self.all_solutions]
        all_obj2 = [sol['scores'][obj2_name] for sol in self.all_solutions]
        
        # 提取帕累托前沿的目标值
        pareto_obj1 = [sol['scores'][obj1_name] for sol in self.pareto_front]
        pareto_obj2 = [sol['scores'][obj2_name] for sol in self.pareto_front]
        
        plt.figure(figsize=(10, 8))
        
        # 绘制所有解
        plt.scatter(all_obj1, all_obj2, alpha=0.6, c='lightblue', label='All solutions')
        
        # 绘制帕累托前沿
        plt.scatter(pareto_obj1, pareto_obj2, c='red', s=100, label='Pareto front')
        
        # 连接帕累托前沿点
        if len(pareto_obj1) > 1:
            # 按第一个目标排序
            sorted_indices = np.argsort(pareto_obj1)
            sorted_obj1 = [pareto_obj1[i] for i in sorted_indices]
            sorted_obj2 = [pareto_obj2[i] for i in sorted_indices]
            plt.plot(sorted_obj1, sorted_obj2, 'r--', alpha=0.7)
        
        plt.xlabel(f'{obj1_name} ({self.directions[obj1_idx]})')
        plt.ylabel(f'{obj2_name} ({self.directions[obj2_idx]})')
        plt.title('Multi-Objective Optimization Results')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def get_best_compromise_solution(self, weights: Dict[str, float] = None):
        """获取最佳折中解"""
        
        if not self.pareto_front:
            return None
        
        if weights is None:
            # 默认等权重
            weights = {obj: 1.0 for obj in self.objectives}
        
        best_solution = None
        best_weighted_score = float('-inf')
        
        for solution in self.pareto_front:
            weighted_score = 0
            
            for obj, direction in zip(self.objectives, self.directions):
                score = solution['scores'][obj]
                weight = weights.get(obj, 1.0)
                
                if direction == 'maximize':
                    weighted_score += weight * score
                else:  # minimize
                    weighted_score -= weight * score
            
            if weighted_score > best_weighted_score:
                best_weighted_score = weighted_score
                best_solution = solution
        
        return best_solution

def demonstrate_multi_objective_optimization():
    """演示多目标优化"""
    
    def multi_objective_function(params):
        """多目标函数：准确率 vs 推理速度 vs 显存使用"""
        
        lr = params['learning_rate']
        batch_size = params['batch_size']
        model_size = params['model_size']
        
        # 模拟准确率（要最大化）
        accuracy = 0.8 + 0.1 * np.log10(lr / 1e-5) + 0.05 * np.log2(batch_size / 8)
        accuracy = max(0.5, min(0.99, accuracy + np.random.normal(0, 0.02)))
        
        # 模拟推理速度（要最大化，这里用倒数表示时间）
        inference_speed = 100 / (model_size * batch_size**0.5)
        inference_speed = max(1, inference_speed + np.random.normal(0, 2))
        
        # 模拟显存使用（要最小化）
        memory_usage = model_size * batch_size * 0.1
        memory_usage = max(1, memory_usage + np.random.normal(0, 0.5))
        
        return {
            'accuracy': accuracy,
            'inference_speed': inference_speed,
            'memory_usage': memory_usage
        }
    
    def generate_search_space():
        """生成搜索空间"""
        return {
            'learning_rate': np.random.uniform(1e-6, 1e-3),
            'batch_size': np.random.choice([4, 8, 16, 32]),
            'model_size': np.random.choice([1, 3, 7, 13])  # 模型大小（B参数）
        }
    
    # 创建多目标优化器
    optimizer = MultiObjectiveOptimizer(
        objectives=['accuracy', 'inference_speed', 'memory_usage'],
        directions=['maximize', 'maximize', 'minimize']
    )
    
    # 执行优化
    print("开始多目标优化...")
    pareto_front = optimizer.optimize_multi_objective(
        multi_objective_function, 
        generate_search_space, 
        n_trials=200
    )
    
    print(f"\n帕累托前沿包含 {len(pareto_front)} 个解")
    
    # 显示帕累托前沿的解
    print("\n帕累托前沿解:")
    for i, solution in enumerate(pareto_front[:5]):  # 只显示前5个
        params = solution['params']
        scores = solution['scores']
        print(f"解 {i+1}:")
        print(f"  参数: LR={params['learning_rate']:.2e}, "
              f"Batch={params['batch_size']}, Model={params['model_size']}B")
        print(f"  目标: 准确率={scores['accuracy']:.3f}, "
              f"推理速度={scores['inference_speed']:.1f}, "
              f"显存={scores['memory_usage']:.1f}GB")
        print()
    
    # 绘制帕累托前沿
    optimizer.plot_pareto_front(0, 2)  # 准确率 vs 显存使用
    
    # 获取最佳折中解
    best_compromise = optimizer.get_best_compromise_solution({
        'accuracy': 0.5,
        'inference_speed': 0.3,
        'memory_usage': 0.2
    })
    
    if best_compromise:
        print("最佳折中解:")
        print(f"  参数: {best_compromise['params']}")
        print(f"  目标: {best_compromise['scores']}")
    
    return optimizer

# 运行多目标优化演示
print("\n=== 多目标优化演示 ===")
multi_obj_optimizer = demonstrate_multi_objective_optimization()
```

### 6.3 超参数调优的实用建议

**阶段性调优策略**：
1. **粗调阶段**：使用较大的搜索范围，快速确定参数的大致区间
2. **精调阶段**：在粗调结果的基础上，缩小搜索范围进行精细调优
3. **验证阶段**：在多个随机种子下验证最佳参数的稳定性

**资源分配策略**：
- 将80%的计算资源用于搜索学习率和批次大小
- 将15%的资源用于调优训练轮数和正则化参数
- 将5%的资源用于其他细节参数

**经验性规则**：
- 学习率通常是最重要的超参数，优先调优
- 批次大小的选择要考虑显存限制和梯度累积
- 训练轮数要结合早停策略，避免过拟合
- 不同任务类型有不同的参数敏感性

通过系统的超参数调优，结合自动化搜索技术和多目标优化方法，我们可以在有限的计算资源下找到最适合特定任务的参数配置。关键是要理解每个参数的作用机制，合理分配调优资源，并建立有效的实验管理流程。