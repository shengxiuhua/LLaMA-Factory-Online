
## 6. 高级超参数调优技术

### 6.1 自动化超参数搜索

手动调参虽然能够积累经验，但效率较低且容易陷入局部最优。自动化超参数搜索技术可以更系统地探索参数空间，找到更好的配置。

**网格搜索（Grid Search）**：在预定义的参数网格中穷举所有组合。适用于参数空间较小的情况。

**随机搜索（Random Search）**：在参数空间中随机采样。相比网格搜索，随机搜索在高维空间中通常更有效。

**贝叶斯优化（Bayesian Optimization）**：基于先验知识和历史实验结果，智能地选择下一组参数。适用于评估成本高的场景。

**超带算法（Hyperband）**：结合了随机搜索和早停策略，能够高效地分配计算资源。

```python
import optuna
import numpy as np
from sklearn.model_selection import cross_val_score
import joblib
from typing import Dict, Any, Callable

class HyperparameterOptimizer:
    """超参数优化器"""
    
    def __init__(self, objective_function: Callable, direction='minimize'):
        self.objective_function = objective_function
        self.direction = direction
        self.study = None
        self.best_params = None
        
    def define_search_space(self, trial):
        """定义搜索空间"""
        # 学习率搜索空间（对数尺度）
        learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-3, log=True)
        
        # 批次大小搜索空间（2的幂次）
        batch_size = trial.suggest_categorical('batch_size', [4, 8, 16, 32, 64])
        
        # 训练轮数
        epochs = trial.suggest_int('epochs', 1, 10)
        
        # 预热比例
        warmup_ratio = trial.suggest_float('warmup_ratio', 0.0, 0.2)
        
        # 权重衰减
        weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-1, log=True)
        
        # LoRA特定参数（如果使用LoRA）
        lora_r = trial.suggest_categorical('lora_r', [4, 8, 16, 32, 64])
        lora_alpha = trial.suggest_categorical('lora_alpha', [8, 16, 32, 64, 128])
        lora_dropout = trial.suggest_float('lora_dropout', 0.0, 0.3)
        
        # 梯度累积步数
        gradient_accumulation_steps = trial.suggest_categorical(
            'gradient_accumulation_steps', [1, 2, 4, 8]
        )
        
        return {
            'learning_rate': learning_rate,
            'batch_size': batch_size,
            'epochs': epochs,
            'warmup_ratio': warmup_ratio,
            'weight_decay': weight_decay,
            'lora_r': lora_r,
            'lora_alpha': lora_alpha,
            'lora_dropout': lora_dropout,
            'gradient_accumulation_steps': gradient_accumulation_steps
        }
    
    def optimize(self, n_trials=50, timeout=None):
        """执行超参数优化"""
        
        def objective(trial):
            # 获取当前试验的参数
            params = self.define_search_space(trial)
            
            # 调用目标函数
            try:
                score = self.objective_function(params)
                return score
            except Exception as e:
                print(f"Trial failed with params {params}: {e}")
                # 返回一个较差的分数
                return float('inf') if self.direction == 'minimize' else float('-inf')
        
        # 创建研究对象
        self.study = optuna.create_study(direction=self.direction)
        
        # 执行优化
        self.study.optimize(objective, n_trials=n_trials, timeout=timeout)
        
        # 保存最佳参数
        self.best_params = self.study.best_params
        
        return self.best_params, self.study.best_value
    
    def get_optimization_history(self):
        """获取优化历史"""
        if self.study is None:
            return None
        
        trials = self.study.trials
        history = {
            'trial_numbers': [t.number for t in trials],
            'values': [t.value for t in trials if t.value is not None],
            'params': [t.params for t in trials]
        }
        
        return history
    
    def plot_optimization_history(self):
        """绘制优化历史"""
        if self.study is None:
            print("No optimization study found. Run optimize() first.")
            return
        
        # 使用optuna内置的可视化功能
        try:
            import optuna.visualization as vis
            
            # 优化历史
            fig1 = vis.plot_optimization_history(self.study)
            fig1.show()
            
            # 参数重要性
            fig2 = vis.plot_param_importances(self.study)
            fig2.show()
            
            # 参数关系
            fig3 = vis.plot_parallel_coordinate(self.study)
            fig3.show()
            
        except ImportError:
            print("Please install plotly for visualization: pip install plotly")

class AdaptiveLearningRateScheduler:
    """自适应学习率调度器"""
    
    def __init__(self, optimizer, patience=5, factor=0.5, min_lr=1e-7):
        self.optimizer = optimizer
        self.patience = patience
        self.factor = factor
        self.min_lr = min_lr
        self.best_loss = float('inf')
        self.wait = 0
        self.lr_history = []
        
    def step(self, val_loss):
        """根据验证损失调整学习率"""
        current_lr = self.optimizer.param_groups[0]['lr']
        self.lr_history.append(current_lr)
        
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.wait = 0
        else:
            self.wait += 1
            
            if self.wait >= self.patience:
                new_lr = max(current_lr * self.factor, self.min_lr)
                
                if new_lr < current_lr:
                    print(f"Reducing learning rate from {current_lr:.2e} to {new_lr:.2e}")
                    
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = new_lr
                    
                    self.wait = 0
                
                return True  # 学习率已调整
        
        return False  # 学习率未调整

def create_hyperparameter_search_example():
    """创建超参数搜索示例"""
    
    def mock_objective_function(params):
        """模拟目标函数（实际应用中这里是完整的训练和评估过程）"""
        
        # 模拟训练过程的评估指标
        # 在实际应用中，这里应该是完整的模型训练和验证过程
        
        lr = params['learning_rate']
        batch_size = params['batch_size']
        epochs = params['epochs']
        
        # 简单的模拟函数：学习率和批次大小的组合效果
        # 实际中这里应该调用真实的训练函数
        simulated_score = (
            0.1 * np.log10(lr) +  # 学习率影响
            0.05 * np.log2(batch_size) +  # 批次大小影响
            0.02 * epochs +  # 训练轮数影响
            np.random.normal(0, 0.01)  # 添加噪声
        )
        
        # 转换为准确率（0-1之间）
        accuracy = 1 / (1 + np.exp(-simulated_score))
        
        return -accuracy  # 返回负值因为我们要最小化
    
    # 创建优化器
    optimizer = HyperparameterOptimizer(mock_objective_function, direction='minimize')
    
    # 执行优化
    print("开始超参数搜索...")
    best_params, best_score = optimizer.optimize(n_trials=30)
    
    print(f"\n最佳参数: {best_params}")
    print(f"最佳分数: {-best_score:.4f}")  # 转换回正值
    
    # 获取优化历史
    history = optimizer.get_optimization_history()
    
    # 分析结果
    print(f"\n优化历史:")
    print(f"总试验次数: {len(history['trial_numbers'])}")
    print(f"最佳试验: {np.argmin(history['values'])}")
    
    return optimizer

# 运行示例
print("=== 自动化超参数搜索演示 ===")
optimizer_example = create_hyperparameter_search_example()
```

### 6.2 多目标优化

在实际应用中，我们往往需要在多个目标之间进行权衡，如准确率vs推理速度、性能vs显存使用等。多目标优化可以帮助我们找到这些目标之间的最佳平衡点。

```python
import numpy as np
from typing import List, Tuple
import matplotlib.pyplot as plt

class MultiObjectiveOptimizer:
    """多目标优化器"""
    
    def __init__(self, objectives: List[str], directions: List[str]):
        """
        objectives: 目标名称列表
        directions: 优化方向列表 ('minimize' 或 'maximize')
        """
        self.objectives = objectives
        self.directions = directions
        self.pareto_front = []
        self.all_solutions = []
    
    def is_dominated(self, solution1: Dict, solution2: Dict) -> bool:
        """检查solution1是否被solution2支配"""
        
        better_in_all = True
        better_in_at_least_one = False
        
        for obj, direction in zip(self.objectives, self.directions):
            val1 = solution1['scores'][obj]
            val2 = solution2['scores'][obj]
            
            if direction == 'minimize':
                if val1 < val2:
                    better_in_at_least_one = True
                elif val1 > val2:
                    better_in_all = False
            else:  # maximize
                if val1 > val2:
                    better_in_at_least_one = True
                elif val1 < val2:
                    better_in_all = False
        
        return better_in_all and better_in_at_least_one
    
    def update_pareto_front(self, new_solution: Dict):
        """更新帕累托前沿"""
        
        # 检查新解是否被现有解支配
        is_dominated_by_existing = False
        for existing_solution in self.pareto_front:
            if self.is_dominated(new_solution, existing_solution):
                is_dominated_by_existing = True
                break
        
        if not is_dominated_by_existing:
            # 移除被新解支配的现有解
            self.pareto_front = [
                sol for sol in self.pareto_front 
                if not self.is_dominated(sol, new_solution)
            ]
            
            # 添加新解
            self.pareto_front.append(new_solution)
        
        # 记录所有解
        self.all_solutions.append(new_solution)
    
    def optimize_multi_objective(self, objective_function: Callable, 
                                search_space_generator: Callable, 
                                n_trials: int = 100):
        """执行多目标优化"""
        
        for trial in range(n_trials):
            # 生成参数
            params = search_space_generator()
            
            try:
                # 评估多个目标
                scores = objective_function(params)
                
                solution = {
                    'params': params,
                    'scores': scores,
                    'trial': trial
                }
                
                # 更新帕累托前沿
                self.update_pareto_front(solution)
                
                if trial % 10 == 0:
                    print(f"Trial {trial}: Pareto front size = {len(self.pareto_front)}")
                    
            except Exception as e:
                print(f"Trial {trial} failed: {e}")
                continue
        
        return self.pareto_front
    
    def plot_pareto_front(self, obj1_idx: int = 0, obj2_idx: int = 1):
        """绘制帕累托前沿（二维）"""
        
        if len(self.objectives) < 2:
            print("Need at least 2 objectives for plotting")
            return
        
        obj1_name = self.objectives[obj1_idx]
        obj2_name = self.objectives[obj2_idx]
        
        # 提取所有解的目标值
        all_obj1 = [sol['scores'][obj1_name] for sol in self.all_solutions]
        all_obj2 = [sol['scores'][obj2_name] for sol in self.all_solutions]
        
        # 提取帕累托前沿的目标值
        pareto_obj1 = [sol['scores'][obj1_name] for sol in self.pareto_front]
        pareto_obj2 = [sol['scores'][obj2_name] for sol in self.pareto_front]
        
        plt.figure(figsize=(10, 8))
        
        # 绘制所有解
        plt.scatter(all_obj1, all_obj2, alpha=0.6, c='lightblue', label='All solutions')
        
        # 绘制帕累托前沿
        plt.scatter(pareto_obj1, pareto_obj2, c='red', s=100, label='Pareto front')
        
        # 连接帕累托前沿点
        if len(pareto_obj1) > 1:
            # 按第一个目标排序
            sorted_indices = np.argsort(pareto_obj1)
            sorted_obj1 = [pareto_obj1[i] for i in sorted_indices]
            sorted_obj2 = [pareto_obj2[i] for i in sorted_indices]
            plt.plot(sorted_obj1, sorted_obj2, 'r--', alpha=0.7)
        
        plt.xlabel(f'{obj1_name} ({self.directions[obj1_idx]})')
        plt.ylabel(f'{obj2_name} ({self.directions[obj2_idx]})')
        plt.title('Multi-Objective Optimization Results')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def get_best_compromise_solution(self, weights: Dict[str, float] = None):
        """获取最佳折中解"""
        
        if not self.pareto_front:
            return None
        
        if weights is None:
            # 默认等权重
            weights = {obj: 1.0 for obj in self.objectives}
        
        best_solution = None
        best_weighted_score = float('-inf')
        
        for solution in self.pareto_front:
            weighted_score = 0
            
            for obj, direction in zip(self.objectives, self.directions):
                score = solution['scores'][obj]
                weight = weights.get(obj, 1.0)
                
                if direction == 'maximize':
                    weighted_score += weight * score
                else:  # minimize
                    weighted_score -= weight * score
            
            if weighted_score > best_weighted_score:
                best_weighted_score = weighted_score
                best_solution = solution
        
        return best_solution

def demonstrate_multi_objective_optimization():
    """演示多目标优化"""
    
    def multi_objective_function(params):
        """多目标函数：准确率 vs 推理速度 vs 显存使用"""
        
        lr = params['learning_rate']
        batch_size = params['batch_size']
        model_size = params['model_size']
        
        # 模拟准确率（要最大化）
        accuracy = 0.8 + 0.1 * np.log10(lr / 1e-5) + 0.05 * np.log2(batch_size / 8)
        accuracy = max(0.5, min(0.99, accuracy + np.random.normal(0, 0.02)))
        
        # 模拟推理速度（要最大化，这里用倒数表示时间）
        inference_speed = 100 / (model_size * batch_size**0.5)
        inference_speed = max(1, inference_speed + np.random.normal(0, 2))
        
        # 模拟显存使用（要最小化）
        memory_usage = model_size * batch_size * 0.1
        memory_usage = max(1, memory_usage + np.random.normal(0, 0.5))
        
        return {
            'accuracy': accuracy,
            'inference_speed': inference_speed,
            'memory_usage': memory_usage
        }
    
    def generate_search_space():
        """生成搜索空间"""
        return {
            'learning_rate': np.random.uniform(1e-6, 1e-3),
            'batch_size': np.random.choice([4, 8, 16, 32]),
            'model_size': np.random.choice([1, 3, 7, 13])  # 模型大小（B参数）
        }
    
    # 创建多目标优化器
    optimizer = MultiObjectiveOptimizer(
        objectives=['accuracy', 'inference_speed', 'memory_usage'],
        directions=['maximize', 'maximize', 'minimize']
    )
    
    # 执行优化
    print("开始多目标优化...")
    pareto_front = optimizer.optimize_multi_objective(
        multi_objective_function, 
        generate_search_space, 
        n_trials=200
    )
    
    print(f"\n帕累托前沿包含 {len(pareto_front)} 个解")
    
    # 显示帕累托前沿的解
    print("\n帕累托前沿解:")
    for i, solution in enumerate(pareto_front[:5]):  # 只显示前5个
        params = solution['params']
        scores = solution['scores']
        print(f"解 {i+1}:")
        print(f"  参数: LR={params['learning_rate']:.2e}, "
              f"Batch={params['batch_size']}, Model={params['model_size']}B")
        print(f"  目标: 准确率={scores['accuracy']:.3f}, "
              f"推理速度={scores['inference_speed']:.1f}, "
              f"显存={scores['memory_usage']:.1f}GB")
        print()
    
    # 绘制帕累托前沿
    optimizer.plot_pareto_front(0, 2)  # 准确率 vs 显存使用
    
    # 获取最佳折中解
    best_compromise = optimizer.get_best_compromise_solution({
        'accuracy': 0.5,
        'inference_speed': 0.3,
        'memory_usage': 0.2
    })
    
    if best_compromise:
        print("最佳折中解:")
        print(f"  参数: {best_compromise['params']}")
        print(f"  目标: {best_compromise['scores']}")
    
    return optimizer

# 运行多目标优化演示
print("\n=== 多目标优化演示 ===")
multi_obj_optimizer = demonstrate_multi_objective_optimization()
```

### 6.3 超参数调优的实用建议

**阶段性调优策略**：
1. **粗调阶段**：使用较大的搜索范围，快速确定参数的大致区间
2. **精调阶段**：在粗调结果的基础上，缩小搜索范围进行精细调优
3. **验证阶段**：在多个随机种子下验证最佳参数的稳定性

**资源分配策略**：
- 将80%的计算资源用于搜索学习率和批次大小
- 将15%的资源用于调优训练轮数和正则化参数
- 将5%的资源用于其他细节参数

**经验性规则**：
- 学习率通常是最重要的超参数，优先调优
- 批次大小的选择要考虑显存限制和梯度累积
- 训练轮数要结合早停策略，避免过拟合
- 不同任务类型有不同的参数敏感性

通过系统的超参数调优，结合自动化搜索技术和多目标优化方法，我们可以在有限的计算资源下找到最适合特定任务的参数配置。关键是要理解每个参数的作用机制，合理分配调优资源，并建立有效的实验管理流程。