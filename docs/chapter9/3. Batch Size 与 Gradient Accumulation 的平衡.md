
## 3. Batch Size 与 Gradient Accumulation 的平衡

### 3.1 批次大小的影响机制

批次大小（Batch Size）是另一个关键的超参数，它不仅影响训练的稳定性和收敛速度，还直接关系到显存使用和训练效率。在大模型微调中，由于显存限制，我们往往无法使用理想的批次大小，这时就需要通过梯度累积（Gradient Accumulation）来模拟大批次训练。

**大批次的优势**：梯度估计更准确，训练更稳定；可以更好地利用并行计算资源；减少梯度噪声，有助于收敛。

**大批次的劣势**：需要更多显存；可能导致泛化能力下降；训练时间更长（如果不能充分并行）。

**小批次的特点**：显存需求小；梯度噪声大，可能有正则化效果；训练不够稳定，可能需要更小的学习率。

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import time
import psutil
import GPUtil

class BatchSizeOptimizer:
    """批次大小优化器"""
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.memory_usage_history = []
    
    def find_optimal_batch_size(self, dataset, start_batch_size=1, max_batch_size=128):
        """寻找最优批次大小"""
        optimal_batch_size = start_batch_size
        
        for batch_size in [2**i for i in range(int(np.log2(start_batch_size)), int(np.log2(max_batch_size)) + 1)]:
            try:
                # 测试当前批次大小
                success = self._test_batch_size(dataset, batch_size)
                if success:
                    optimal_batch_size = batch_size
                else:
                    break
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"批次大小 {batch_size} 超出显存限制")
                    break
                else:
                    raise e
        
        return optimal_batch_size
    
    def _test_batch_size(self, dataset, batch_size):
        """测试特定批次大小"""
        try:
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
            
            # 清空显存缓存
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # 测试一个批次
            batch = next(iter(dataloader))
            inputs, targets = batch
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            
            # 前向传播
            outputs = self.model(inputs)
            loss = nn.CrossEntropyLoss()(outputs, targets)
            
            # 反向传播
            loss.backward()
            
            # 记录显存使用
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**3  # GB
                self.memory_usage_history.append((batch_size, memory_used))
                print(f"批次大小 {batch_size}: 显存使用 {memory_used:.2f} GB")
            
            # 清理梯度
            self.model.zero_grad()
            
            return True
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                return False
            else:
                raise e
    
    def calculate_gradient_accumulation(self, target_batch_size, actual_batch_size):
        """计算梯度累积步数"""
        accumulation_steps = target_batch_size // actual_batch_size
        effective_batch_size = actual_batch_size * accumulation_steps
        
        return {
            'accumulation_steps': accumulation_steps,
            'effective_batch_size': effective_batch_size,
            'batch_size_difference': target_batch_size - effective_batch_size
        }

class GradientAccumulator:
    """梯度累积器"""
    
    def __init__(self, model, optimizer, accumulation_steps):
        self.model = model
        self.optimizer = optimizer
        self.accumulation_steps = accumulation_steps
        self.current_step = 0
        self.accumulated_loss = 0.0
    
    def step(self, loss):
        """执行一步梯度累积"""
        # 缩放损失
        scaled_loss = loss / self.accumulation_steps
        scaled_loss.backward()
        
        self.accumulated_loss += scaled_loss.item()
        self.current_step += 1
        
        # 检查是否需要更新参数
        if self.current_step % self.accumulation_steps == 0:
            # 梯度裁剪
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # 更新参数
            self.optimizer.step()
            self.optimizer.zero_grad()
            
            # 返回平均损失
            avg_loss = self.accumulated_loss
            self.accumulated_loss = 0.0
            
            return True, avg_loss
        
        return False, None
    
    def finalize(self):
        """处理剩余的梯度"""
        if self.current_step % self.accumulation_steps != 0:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            self.optimizer.zero_grad()

def demonstrate_gradient_accumulation():
    """演示梯度累积的使用"""
    
    # 创建示例数据
    class DummyDataset(Dataset):
        def __init__(self, size=1000):
            self.size = size
            self.data = torch.randn(size, 100)
            self.labels = torch.randint(0, 10, (size,))
        
        def __len__(self):
            return self.size
        
        def __getitem__(self, idx):
            return self.data[idx], self.labels[idx]
    
    # 创建模型
    model = nn.Linear(100, 10)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    dataset = DummyDataset()
    
    # 不同的批次大小配置
    configs = [
        {'batch_size': 32, 'accumulation_steps': 1, 'name': '直接批次32'},
        {'batch_size': 8, 'accumulation_steps': 4, 'name': '批次8+累积4'},
        {'batch_size': 4, 'accumulation_steps': 8, 'name': '批次4+累积8'},
    ]
    
    for config in configs:
        print(f"\n=== {config['name']} ===")
        
        dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)
        accumulator = GradientAccumulator(model, optimizer, config['accumulation_steps'])
        
        # 训练几个批次
        total_loss = 0
        num_updates = 0
        
        for i, (inputs, targets) in enumerate(dataloader):
            if i >= 10:  # 只训练10个批次
                break
            
            outputs = model(inputs)
            loss = nn.CrossEntropyLoss()(outputs, targets)
            
            updated, avg_loss = accumulator.step(loss)
            
            if updated:
                num_updates += 1
                total_loss += avg_loss
                print(f"更新 {num_updates}: 平均损失 = {avg_loss:.4f}")
        
        # 处理剩余梯度
        accumulator.finalize()
        
        print(f"总更新次数: {num_updates}")
        print(f"平均损失: {total_loss / max(num_updates, 1):.4f}")

# 批次大小推荐函数
def recommend_batch_size(model_size, available_memory_gb, task_type='classification'):
    """推荐批次大小"""
    
    # 基于模型大小和可用显存的经验公式
    memory_per_sample = {
        '1b': 0.1,   # GB per sample
        '3b': 0.2,
        '7b': 0.4,
        '13b': 0.8,
        '30b': 1.5,
        '70b': 3.0
    }
    
    base_memory = memory_per_sample.get(model_size, 0.4)
    
    # 任务类型调整
    task_multiplier = {
        'classification': 1.0,
        'generation': 1.5,  # 生成任务需要更多显存
        'qa': 1.2
    }
    
    adjusted_memory = base_memory * task_multiplier.get(task_type, 1.0)
    
    # 计算推荐批次大小（保留一些显存余量）
    usable_memory = available_memory_gb * 0.8  # 80%的显存用于批次
    recommended_batch_size = max(1, int(usable_memory / adjusted_memory))
    
    # 调整为2的幂次
    recommended_batch_size = 2 ** int(np.log2(recommended_batch_size))
    
    return {
        'recommended_batch_size': recommended_batch_size,
        'memory_per_sample': adjusted_memory,
        'estimated_memory_usage': recommended_batch_size * adjusted_memory,
        'gradient_accumulation_for_32': max(1, 32 // recommended_batch_size),
        'gradient_accumulation_for_64': max(1, 64 // recommended_batch_size)
    }

# 使用示例
print("=== 梯度累积演示 ===")
demonstrate_gradient_accumulation()

print("\n=== 批次大小推荐 ===")
model_sizes = ['7b', '13b', '30b']
memory_sizes = [8, 16, 24, 40]

for model_size in model_sizes:
    for memory_gb in memory_sizes:
        recommendation = recommend_batch_size(model_size, memory_gb, 'classification')
        print(f"{model_size} 模型, {memory_gb}GB 显存: "
              f"推荐批次大小 {recommendation['recommended_batch_size']}, "
              f"累积步数(目标32) {recommendation['gradient_accumulation_for_32']}")
```

### 3.2 显存优化策略

**梯度检查点**：通过重新计算来节省显存，以时间换空间。

**混合精度训练**：使用FP16或BF16来减少显存使用。

**优化器状态分片**：使用DeepSpeed等工具将优化器状态分布到多个设备。

**动态批次大小**：根据序列长度动态调整批次大小。

