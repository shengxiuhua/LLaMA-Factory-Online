# 第三章 预训练语言模型

## 1. 预训练语言模型的革命性意义

预训练语言模型（Pre-trained Language Models, PLMs）的出现标志着自然语言处理领域的一个重要转折点。从2018年BERT的横空出世，到如今GPT系列模型的广泛应用，预训练语言模型已经成为现代NLP系统的基石。这些模型通过在大规模无标注文本上进行预训练，学习到了丰富的语言知识和表示能力，为各种下游任务提供了强大的基础。

**预训练语言模型的核心思想**在于"预训练+微调"的两阶段范式。在预训练阶段，模型在大规模无标注文本上学习语言的通用表示；在微调阶段，模型在特定任务的标注数据上进行调整，以适应具体的应用需求。这种范式不仅大大降低了各种NLP任务的数据需求，还显著提升了模型的性能表现。

**架构分类的重要性**：根据Transformer架构的不同组合方式，预训练语言模型可以分为三大类：Encoder-only模型（如BERT）、Encoder-Decoder模型（如T5）、和Decoder-only模型（如GPT）。每种架构都有其独特的优势和适用场景，理解这些差异对于选择合适的模型和设计有效的微调策略至关重要。

本章将深入分析这三种主要架构的设计原理、技术特点、优缺点以及最佳应用场景。通过理论分析与实践案例相结合的方式，帮助读者全面理解预训练语言模型的技术内涵，为后续的微调实践奠定坚实基础。