# 1. Encoder-only PLM

## 1.1 架构原理与设计思想

Encoder-only预训练语言模型以BERT（Bidirectional Encoder Representations from Transformers）为代表，其核心设计思想是通过双向注意力机制来学习文本的深层表示。与传统的单向语言模型不同，Encoder-only模型能够同时利用上下文信息，从而获得更加丰富和准确的语言理解能力。

**双向注意力机制的优势**：传统的语言模型只能利用前文信息来预测下一个词，这种单向的信息流限制了模型对语言的理解深度。Encoder-only模型通过掩码语言建模（Masked Language Modeling, MLM）任务，允许模型在预测被掩码的词时同时利用前文和后文信息。这种双向的信息整合使得模型能够学习到更加完整和准确的词汇表示。

**掩码语言建模的核心机制**：在预训练过程中，模型随机掩盖输入序列中的一部分词汇（通常是15%），然后要求模型根据上下文信息来预测这些被掩盖的词汇。这种训练方式迫使模型学习词汇之间的深层语义关系，而不仅仅是表面的统计共现模式。

**下一句预测任务的补充作用**：除了MLM任务外，BERT还引入了下一句预测（Next Sentence Prediction, NSP）任务，要求模型判断两个句子是否在原文中相邻。虽然后续研究表明NSP任务的效果有限，但它体现了早期研究者对句子级别语义理解的重视。

## 1.2 代表性模型分析

**BERT系列的演进**：BERT 的模型架构是取了 Transformer 的 Encoder 部分堆叠而成，其主要结构如下图所示。
![](../images/12.png)

BERT作为Encoder-only模型的开山之作，其成功催生了一系列改进版本。BERT-Large通过增加模型参数提升了性能；RoBERTa通过优化训练策略（去除NSP任务、使用更大的批次大小、更长的训练时间）进一步提升了效果；ALBERT通过参数共享和因式分解技术在保持性能的同时大幅减少了参数量。

**领域特化的发展**：基于BERT的架构，研究者们开发了许多领域特化的模型。SciBERT专门针对科学文献进行预训练；ClinicalBERT专注于医疗文本；FinBERT则针对金融领域进行了优化。这些模型通过在特定领域的文本上进行预训练，获得了更好的领域适应性。

**多语言扩展**：mBERT（multilingual BERT）和XLM-R等模型将Encoder-only架构扩展到多语言场景，通过在多种语言的文本上进行联合预训练，实现了跨语言的知识迁移和理解能力。

```python
import torch
from transformers import BertModel, BertTokenizer, BertForSequenceClassification
from transformers import TrainingArguments, Trainer
import numpy as np

class BERTFineTuningExample:
    """BERT微调示例类"""
    
    def __init__(self, model_name='bert-base-uncased'):
        self.model_name = model_name
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = None
        
    def load_model_for_classification(self, num_labels=2):
        """加载用于分类任务的BERT模型"""
        self.model = BertForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=num_labels
        )
        return self.model
    
    def prepare_data(self, texts, labels, max_length=512):
        """准备训练数据"""
        encodings = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=max_length,
            return_tensors='pt'
        )
        
        class TextDataset(torch.utils.data.Dataset):
            def __init__(self, encodings, labels):
                self.encodings = encodings
                self.labels = labels
            
            def __getitem__(self, idx):
                item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
                item['labels'] = torch.tensor(self.labels[idx])
                return item
            
            def __len__(self):
                return len(self.labels)
        
        return TextDataset(encodings, labels)
    
    def fine_tune(self, train_dataset, eval_dataset=None, output_dir='./bert_finetuned'):
        """执行微调"""
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=64,
            warmup_steps=500,
            weight_decay=0.01,
            logging_dir='./logs',
            logging_steps=10,
            evaluation_strategy="epoch" if eval_dataset else "no",
            save_strategy="epoch",
            load_best_model_at_end=True if eval_dataset else False,
            metric_for_best_model="eval_loss" if eval_dataset else None,
        )
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
        )
        
        trainer.train()
        return trainer

# 使用示例
def demonstrate_bert_finetuning():
    """演示BERT微调过程"""
    
    # 示例数据
    train_texts = [
        "This movie is fantastic!",
        "I hate this film.",
        "Great acting and storyline.",
        "Boring and predictable plot."
    ]
    train_labels = [1, 0, 1, 0]  # 1: positive, 0: negative
    
    # 创建微调实例
    bert_tuner = BERTFineTuningExample()
    
    # 加载分类模型
    model = bert_tuner.load_model_for_classification(num_labels=2)
    
    # 准备数据
    train_dataset = bert_tuner.prepare_data(train_texts, train_labels)
    
    # 执行微调
    trainer = bert_tuner.fine_tune(train_dataset)
    
    print("BERT微调完成！")
    return trainer

# 运行演示
trainer = demonstrate_bert_finetuning()
```

## 1.3 优势与局限性分析

**显著优势**：

**强大的语言理解能力**：Encoder-only模型通过双向注意力机制，能够深度理解文本的语义内容。在各种理解类任务（如文本分类、情感分析、命名实体识别等）上表现优异。

**高效的微调性能**：由于预训练阶段已经学习了丰富的语言表示，Encoder-only模型在下游任务上通常只需要少量的标注数据就能达到很好的效果。这种高效的迁移学习能力大大降低了NLP应用的门槛。

**稳定的训练过程**：相比于生成式模型，Encoder-only模型的训练过程更加稳定，不容易出现模式崩塌或训练发散等问题。

**主要局限性**：

**生成能力有限**：Encoder-only模型主要设计用于理解任务，在文本生成方面能力有限。虽然可以通过一些技巧（如掩码填充）实现简单的生成，但效果远不如专门的生成模型。

**序列长度限制**：由于自注意力机制的计算复杂度是序列长度的平方，Encoder-only模型在处理长文本时面临计算和内存的双重挑战。

**单向推理限制**：虽然在训练时使用双向信息，但在实际应用中，某些任务（如实时对话）需要单向的推理能力，这时Encoder-only模型就显得不够灵活。

## 1.4 最佳应用场景与微调策略

**理想应用场景**：

**文本分类任务**：包括情感分析、主题分类、垃圾邮件检测等。Encoder-only模型在这类任务上表现出色，通常只需要在预训练模型的基础上添加一个分类头即可。

**序列标注任务**：如命名实体识别、词性标注、语义角色标注等。模型可以为序列中的每个位置输出相应的标签。

**句子对任务**：如文本相似度计算、自然语言推理、问答匹配等。通过将两个句子拼接输入模型，可以学习它们之间的关系。

**信息抽取任务**：如关系抽取、事件抽取等。模型可以识别文本中的关键信息并进行结构化输出。

**微调策略建议**：

**学习率设置**：Encoder-only模型通常使用较小的学习率（如2e-5到5e-5），以避免破坏预训练的权重。

**层次化微调**：可以对模型的不同层使用不同的学习率，底层使用更小的学习率以保持通用特征，顶层使用相对较大的学习率以快速适应任务。

**渐进式解冻**：先冻结大部分层只训练分类头，然后逐步解冻更多层进行微调。这种策略可以避免过拟合并提高训练稳定性。

```python
def bert_advanced_finetuning_strategies():
    """BERT高级微调策略示例"""
    
    from transformers import AdamW, get_linear_schedule_with_warmup
    
    # 层次化学习率设置
    def setup_layered_learning_rates(model, base_lr=2e-5, decay_factor=0.9):
        """为不同层设置不同的学习率"""
        
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = []
        
        # 获取层数
        num_layers = len(model.bert.encoder.layer)
        
        # 为每一层设置学习率
        for layer_idx in range(num_layers):
            layer_lr = base_lr * (decay_factor ** (num_layers - layer_idx - 1))
            
            # 当前层的参数
            layer_params = [
                {
                    "params": [p for n, p in model.bert.encoder.layer[layer_idx].named_parameters() 
                              if not any(nd in n for nd in no_decay)],
                    "weight_decay": 0.01,
                    "lr": layer_lr
                },
                {
                    "params": [p for n, p in model.bert.encoder.layer[layer_idx].named_parameters() 
                              if any(nd in n for nd in no_decay)],
                    "weight_decay": 0.0,
                    "lr": layer_lr
                }
            ]
            optimizer_grouped_parameters.extend(layer_params)
        
        # 分类头使用基础学习率
        classifier_params = [
            {
                "params": [p for n, p in model.classifier.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                "weight_decay": 0.01,
                "lr": base_lr
            },
            {
                "params": [p for n, p in model.classifier.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
                "lr": base_lr
            }
        ]
        optimizer_grouped_parameters.extend(classifier_params)
        
        return AdamW(optimizer_grouped_parameters, eps=1e-8)
    
    # 渐进式解冻策略
    def progressive_unfreezing(model, trainer, unfreeze_schedule):
        """渐进式解冻训练"""
        
        # 初始时冻结所有BERT层
        for param in model.bert.parameters():
            param.requires_grad = False
        
        # 只训练分类头
        for param in model.classifier.parameters():
            param.requires_grad = True
        
        for epoch, layers_to_unfreeze in unfreeze_schedule.items():
            if epoch > 0:
                # 解冻指定层
                for layer_idx in layers_to_unfreeze:
                    for param in model.bert.encoder.layer[layer_idx].parameters():
                        param.requires_grad = True
                
                print(f"Epoch {epoch}: Unfrozen layers {layers_to_unfreeze}")
    
    # 使用示例
    unfreeze_schedule = {
        0: [],  # 只训练分类头
        1: [11, 10],  # 解冻最后两层
        2: [9, 8, 7],  # 解冻更多层
        3: list(range(12))  # 解冻所有层
    }
    
    return setup_layered_learning_rates, progressive_unfreezing, unfreeze_schedule

# 获取高级微调策略
setup_lr, progressive_unfreeze, schedule = bert_advanced_finetuning_strategies()
```

