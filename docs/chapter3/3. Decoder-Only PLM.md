# 3. Decoder-Only PLM

## 3.1 架构演进与设计哲学

Decoder-only预训练语言模型以GPT（Generative Pre-trained Transformer）系列为代表，体现了"简单即美"的设计哲学。这类模型只使用Transformer的Decoder部分，通过自回归的方式进行文本生成，却在各种NLP任务上展现出了惊人的能力。

**自回归生成的核心机制**：Decoder-only模型采用自回归的方式生成文本，即根据前面已生成的所有词来预测下一个词。这种单向的信息流虽然看似简单，但却能够学习到复杂的语言模式和知识结构。

**因果注意力掩码**：为了保证自回归的特性，Decoder-only模型使用因果注意力掩码，确保每个位置只能看到它之前的位置信息。这种设计虽然限制了信息的双向流动，但却使得模型具有了强大的生成能力。

**规模化的威力**：GPT系列模型的发展历程清晰地展示了规模化的威力。从GPT-1的117M参数到GPT-3的175B参数，再到GPT-4的万亿级参数，模型能力随着规模的增长呈现出质的飞跃。

## 3.2 涌现能力与In-Context Learning

**涌现能力的发现**：随着模型规模的增长，Decoder-only模型展现出了许多在小规模时不具备的涌现能力。这些能力包括复杂推理、代码生成、数学计算、创意写作等，这些能力的出现往往是突然的、非线性的。

**In-Context Learning的革命**：GPT-3引入的In-Context Learning（上下文学习）彻底改变了NLP任务的解决方式。模型可以仅通过几个示例就学会新任务，而无需更新参数。这种能力使得模型具有了前所未有的灵活性和适应性。

**Few-Shot Learning的实现**：通过精心设计的提示（prompt），Decoder-only模型可以在零样本或少样本的情况下完成各种任务。这种能力大大降低了NLP应用的门槛，使得非专业用户也能够利用大模型的能力。

```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
import numpy as np

class GPTFineTuningFramework:
    """GPT微调框架"""
    
    def __init__(self, model_name='gpt2'):
        self.model_name = model_name
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        
        # 设置pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def prepare_language_modeling_data(self, texts, max_length=512):
        """准备语言建模数据"""
        
        # 对文本进行编码
        encodings = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=max_length,
            return_tensors='pt'
        )
        
        class LanguageModelingDataset(torch.utils.data.Dataset):
            def __init__(self, encodings):
                self.encodings = encodings
            
            def __getitem__(self, idx):
                return {
                    'input_ids': self.encodings['input_ids'][idx],
                    'attention_mask': self.encodings['attention_mask'][idx]
                }
            
            def __len__(self):
                return len(self.encodings['input_ids'])
        
        return LanguageModelingDataset(encodings)
    
    def fine_tune_for_generation(self, train_dataset, eval_dataset=None, domain_name="custom"):
        """针对特定领域进行生成式微调"""
        
        # 数据整理器
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # GPT使用因果语言建模，不是掩码语言建模
        )
        
        training_args = TrainingArguments(
            output_dir=f'./gpt_{domain_name}',
            overwrite_output_dir=True,
            num_train_epochs=3,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            warmup_steps=1000,
            weight_decay=0.01,
            logging_dir=f'./logs_gpt_{domain_name}',
            logging_steps=100,
            evaluation_strategy="epoch" if eval_dataset else "no",
            save_strategy="epoch",
            load_best_model_at_end=True if eval_dataset else False,
            prediction_loss_only=True,
            gradient_accumulation_steps=4,
            fp16=True,  # 使用混合精度训练
        )
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
        )
        
        trainer.train()
        return trainer
    
    def generate_text(self, prompt, max_length=100, temperature=0.8, top_p=0.9, num_return_sequences=1):
        """生成文本"""
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
        
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                num_return_sequences=num_return_sequences,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        generated_texts = []
        for output in outputs:
            text = self.tokenizer.decode(output, skip_special_tokens=True)
            generated_texts.append(text)
        
        return generated_texts
    
    def few_shot_learning_demo(self, task_examples, query):
        """演示Few-Shot Learning"""
        
        # 构建few-shot prompt
        prompt = ""
        for example in task_examples:
            prompt += f"Input: {example['input']}\nOutput: {example['output']}\n\n"
        
        prompt += f"Input: {query}\nOutput:"
        
        # 生成回答
        responses = self.generate_text(prompt, max_length=len(prompt) + 50, temperature=0.3)
        
        # 提取生成的输出部分
        generated_output = responses[0][len(prompt):].strip()
        
        return generated_output

def demonstrate_gpt_capabilities():
    """演示GPT的各种能力"""
    
    # 创建GPT微调框架
    gpt_tuner = GPTFineTuningFramework()
    
    print("=== GPT能力演示 ===\n")
    
    # 1. 文本生成
    print("1. 文本生成:")
    prompt = "The future of artificial intelligence is"
    generated_texts = gpt_tuner.generate_text(prompt, max_length=80, num_return_sequences=2)
    for i, text in enumerate(generated_texts):
        print(f"生成 {i+1}: {text}")
    print()
    
    # 2. Few-Shot Learning演示
    print("2. Few-Shot Learning - 情感分析:")
    sentiment_examples = [
        {"input": "I love this movie!", "output": "positive"},
        {"input": "This book is terrible.", "output": "negative"},
        {"input": "The weather is okay.", "output": "neutral"}
    ]
    
    query = "This restaurant serves amazing food!"
    result = gpt_tuner.few_shot_learning_demo(sentiment_examples, query)
    print(f"查询: {query}")
    print(f"预测: {result}")
    print()
    
    # 3. 代码生成演示
    print("3. 代码生成:")
    code_prompt = "# Python function to calculate fibonacci numbers\ndef fibonacci(n):"
    code_generated = gpt_tuner.generate_text(code_prompt, max_length=150, temperature=0.2)
    print(code_generated[0])
    print()
    
    # 4. 创意写作
    print("4. 创意写作:")
    creative_prompt = "Once upon a time, in a world where robots and humans lived together,"
    story = gpt_tuner.generate_text(creative_prompt, max_length=120, temperature=0.9)
    print(story[0])
    
    return gpt_tuner

# 运行GPT能力演示
gpt_demo = demonstrate_gpt_capabilities()
```

## 3.3 优势与挑战分析

### 核心优势

**强大的生成能力**：Decoder-only模型在文本生成方面表现出色，能够生成连贯、流畅、富有创意的文本内容。

**任务泛化能力**：通过适当的提示设计，单一的Decoder-only模型可以处理各种不同类型的NLP任务，展现出强大的泛化能力。

**上下文学习**：In-Context Learning能力使得模型可以快速适应新任务，无需参数更新就能学会新技能。

**规模效应明显**：随着模型规模的增长，Decoder-only模型的能力呈现出显著的提升，涌现出许多新的能力。

### 面临的挑战

**计算资源需求**：大规模的Decoder-only模型需要巨大的计算资源进行训练和推理，这限制了其普及应用。

**幻觉问题**：模型有时会生成看似合理但实际错误的内容，这在需要高准确性的应用中是一个严重问题。

**可控性挑战**：如何精确控制模型的输出内容和风格仍然是一个挑战。

**安全性考虑**：强大的生成能力也带来了潜在的安全风险，如生成有害内容、虚假信息等。

## 3.4 微调策略与最佳实践

**指令微调（Instruction Tuning）**：通过在大量指令-回答对上进行微调，提高模型遵循指令的能力。这种方法能够显著提升模型在各种任务上的表现。

**人类反馈强化学习（RLHF）**：通过人类反馈来训练奖励模型，然后使用强化学习来优化语言模型，使其生成更符合人类偏好的内容。

**提示工程（Prompt Engineering）**：设计有效的提示是发挥Decoder-only模型能力的关键。好的提示可以显著提升模型在特定任务上的表现。

```python
def advanced_gpt_finetuning_strategies():
    """GPT高级微调策略"""
    
    # 指令微调数据格式
    instruction_data_format = {
        "instruction_following": [
            {
                "instruction": "Summarize the following text in one sentence.",
                "input": "Long text content here...",
                "output": "Brief summary here."
            },
            {
                "instruction": "Translate the following English text to French.",
                "input": "Hello, how are you?",
                "output": "Bonjour, comment allez-vous?"
            }
        ]
    }
    
    # 提示模板设计
    prompt_templates = {
        "classification": "Classify the sentiment of the following text as positive, negative, or neutral:\nText: {text}\nSentiment:",
        "summarization": "Summarize the following text in {length} words:\nText: {text}\nSummary:",
        "qa": "Answer the following question based on the given context:\nContext: {context}\nQuestion: {question}\nAnswer:",
        "generation": "Continue the following story:\n{prompt}\nContinuation:"
    }
    
    # 微调超参数建议
    finetuning_hyperparameters = {
        "instruction_tuning": {
            "learning_rate": 1e-5,
            "batch_size": 8,
            "epochs": 3,
            "warmup_ratio": 0.1,
            "weight_decay": 0.01,
            "gradient_clipping": 1.0
        },
        "domain_adaptation": {
            "learning_rate": 2e-5,
            "batch_size": 4,
            "epochs": 5,
            "warmup_ratio": 0.15,
            "weight_decay": 0.02,
            "gradient_clipping": 0.5
        }
    }
    
    return instruction_data_format, prompt_templates, finetuning_hyperparameters

# 获取高级策略
instruction_format, templates, hyperparams = advanced_gpt_finetuning_strategies()
print("GPT高级微调策略已准备就绪")
```

