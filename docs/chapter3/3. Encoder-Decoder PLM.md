
## 3. Encoder-Decoder PLM

### 3.1 架构设计与核心机制

Encoder-Decoder预训练语言模型代表了序列到序列学习的最新发展，其中T5（Text-to-Text Transfer Transformer）是这一架构的典型代表。这类模型的核心思想是将所有NLP任务都统一为文本到文本的转换问题，通过一个统一的架构来处理各种不同类型的任务。

**统一的文本到文本框架**：T5的革命性贡献在于提出了"Everything is Text-to-Text"的理念。无论是分类、抽取、生成还是翻译任务，都被重新表述为从输入文本生成输出文本的问题。例如，情感分析任务被表述为"sentiment: This movie is great" → "positive"；翻译任务被表述为"translate English to German: Hello" → "Hallo"。

**Encoder-Decoder的协同工作**：在这种架构中，Encoder负责理解和编码输入文本，生成丰富的上下文表示；Decoder则基于这些表示生成目标输出。两者通过交叉注意力机制进行信息交互，使得模型既具备强大的理解能力，又具备灵活的生成能力。

**预训练任务的设计**：T5采用了"span corruption"预训练任务，即随机掩盖输入文本中的连续片段，然后要求模型重构这些被掩盖的内容。这种任务设计既包含了理解的成分（需要理解上下文），又包含了生成的成分（需要生成被掩盖的文本），完美契合了Encoder-Decoder架构的特点。

### 3.2 技术创新与优化策略

**相对位置编码的改进**：T5引入了相对位置编码机制，相比于绝对位置编码，这种方法能够更好地处理不同长度的序列，并且具有更好的泛化能力。

**多任务预训练的探索**：T5通过在多个任务上同时进行预训练，学习到了更加通用和鲁棒的表示。这种多任务学习的方式使得模型在面对新任务时具有更强的适应能力。

**规模效应的验证**：T5系列从Small（60M参数）到XXL（11B参数）的不同规模验证了模型规模对性能的重要影响，为后续的大模型发展提供了重要参考。

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer
from transformers import TrainingArguments, Trainer
import torch

class T5FineTuningFramework:
    """T5微调框架"""
    
    def __init__(self, model_name='t5-base'):
        self.model_name = model_name
        self.tokenizer = T5Tokenizer.from_pretrained(model_name)
        self.model = T5ForConditionalGeneration.from_pretrained(model_name)
        
    def prepare_seq2seq_data(self, input_texts, target_texts, max_input_length=512, max_target_length=128):
        """准备序列到序列的训练数据"""
        
        # 编码输入文本
        input_encodings = self.tokenizer(
            input_texts,
            truncation=True,
            padding=True,
            max_length=max_input_length,
            return_tensors='pt'
        )
        
        # 编码目标文本
        target_encodings = self.tokenizer(
            target_texts,
            truncation=True,
            padding=True,
            max_length=max_target_length,
            return_tensors='pt'
        )
        
        class Seq2SeqDataset(torch.utils.data.Dataset):
            def __init__(self, input_encodings, target_encodings):
                self.input_encodings = input_encodings
                self.target_encodings = target_encodings
            
            def __getitem__(self, idx):
                item = {
                    'input_ids': self.input_encodings['input_ids'][idx],
                    'attention_mask': self.input_encodings['attention_mask'][idx],
                    'labels': self.target_encodings['input_ids'][idx]
                }
                return item
            
            def __len__(self):
                return len(self.input_encodings['input_ids'])
        
        return Seq2SeqDataset(input_encodings, target_encodings)
    
    def fine_tune_for_task(self, train_dataset, eval_dataset=None, task_name="custom_task"):
        """针对特定任务进行微调"""
        
        training_args = TrainingArguments(
            output_dir=f'./t5_{task_name}',
            num_train_epochs=3,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            warmup_steps=1000,
            weight_decay=0.01,
            logging_dir=f'./logs_{task_name}',
            logging_steps=100,
            evaluation_strategy="epoch" if eval_dataset else "no",
            save_strategy="epoch",
            load_best_model_at_end=True if eval_dataset else False,
            predict_with_generate=True,  # 用于生成任务的评估
            generation_max_length=128,
            generation_num_beams=4,
        )
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
        )
        
        trainer.train()
        return trainer
    
    def generate_text(self, input_text, max_length=128, num_beams=4):
        """生成文本"""
        input_ids = self.tokenizer.encode(input_text, return_tensors='pt')
        
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                max_length=max_length,
                num_beams=num_beams,
                early_stopping=True,
                no_repeat_ngram_size=2
            )
        
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

def demonstrate_t5_multitask_finetuning():
    """演示T5多任务微调"""
    
    # 创建T5微调框架
    t5_tuner = T5FineTuningFramework()
    
    # 准备多任务数据
    # 任务1: 文本摘要
    summarization_inputs = [
        "summarize: The quick brown fox jumps over the lazy dog. This is a common sentence used in typing practice.",
        "summarize: Machine learning is a subset of artificial intelligence that focuses on algorithms."
    ]
    summarization_targets = [
        "A sentence about a fox and dog used for typing practice.",
        "ML is part of AI focusing on algorithms."
    ]
    
    # 任务2: 情感分析
    sentiment_inputs = [
        "sentiment: I love this movie!",
        "sentiment: This book is terrible."
    ]
    sentiment_targets = [
        "positive",
        "negative"
    ]
    
    # 任务3: 问答
    qa_inputs = [
        "question: What is the capital of France? context: Paris is the capital and largest city of France.",
        "question: Who wrote Romeo and Juliet? context: William Shakespeare wrote many famous plays including Romeo and Juliet."
    ]
    qa_targets = [
        "Paris",
        "William Shakespeare"
    ]
    
    # 合并所有任务数据
    all_inputs = summarization_inputs + sentiment_inputs + qa_inputs
    all_targets = summarization_targets + sentiment_targets + qa_targets
    
    # 准备数据集
    train_dataset = t5_tuner.prepare_seq2seq_data(all_inputs, all_targets)
    
    # 执行多任务微调
    trainer = t5_tuner.fine_tune_for_task(train_dataset, task_name="multitask")
    
    # 测试生成效果
    test_inputs = [
        "summarize: Artificial intelligence is transforming various industries.",
        "sentiment: The weather is nice today.",
        "question: What is AI? context: Artificial intelligence refers to machine intelligence."
    ]
    
    print("T5多任务微调结果:")
    for input_text in test_inputs:
        output = t5_tuner.generate_text(input_text)
        print(f"Input: {input_text}")
        print(f"Output: {output}")
        print("-" * 50)
    
    return trainer

# 运行多任务微调演示
multitask_trainer = demonstrate_t5_multitask_finetuning()
```

### 3.3 优势特点与应用价值

**突出优势**：

**任务统一性**：Encoder-Decoder模型最大的优势在于能够用统一的架构处理各种不同类型的NLP任务。这种统一性不仅简化了模型的部署和维护，还使得多任务学习成为可能。

**生成能力强**：相比于Encoder-only模型，Encoder-Decoder模型具有强大的文本生成能力。无论是摘要生成、机器翻译还是对话生成，都能取得优秀的效果。

**长序列处理**：通过Encoder-Decoder的分离设计，模型可以更好地处理长序列的输入和输出，特别适合文档级别的处理任务。

**多任务学习友好**：统一的文本到文本框架使得多任务学习变得自然而高效，模型可以在多个任务上同时学习，实现知识的相互促进。

**应用价值分析**：

**内容生成领域**：在新闻摘要、报告生成、创意写作等内容生成任务中表现出色。

**语言转换任务**：机器翻译、语言风格转换、文本改写等任务的理想选择。

**问答系统**：特别适合生成式问答，能够根据上下文生成自然流畅的答案。

**对话系统**：在任务导向对话和开放域对话中都有良好表现。

### 3.4 局限性与改进方向

**主要局限性**：

**计算复杂度高**：Encoder-Decoder架构的计算复杂度相对较高，特别是在处理长序列时，内存和计算需求都比较大。

**训练稳定性**：生成任务的训练通常比分类任务更加困难，容易出现训练不稳定的情况。

**评估复杂性**：生成任务的评估比分类任务更加复杂，需要考虑多个维度的指标。

**改进方向**：

**效率优化**：通过模型压缩、知识蒸馏等技术提高模型效率。

**训练策略改进**：采用更好的训练策略，如课程学习、对抗训练等。

**评估方法完善**：开发更加全面和准确的生成任务评估方法。


### 5.1 性能对比分析

**理解类任务**：在文本分类、情感分析、命名实体识别等理解类任务上，Encoder-only模型通常表现最佳，因为其双向注意力机制能够充分利用上下文信息。

**生成类任务**：在文本生成、对话系统、创意写作等生成类任务上，Decoder-only模型表现最为出色，其自回归的生成方式更适合这类任务。

**序列转换任务**：在机器翻译、文本摘要、问答系统等序列转换任务上，Encoder-Decoder模型通常是最佳选择，其架构天然适合这类输入输出都是序列的任务。

### 5.2 资源需求对比

**训练资源**：Decoder-only模型通常需要最多的训练资源，特别是大规模模型；Encoder-Decoder模型次之；Encoder-only模型相对较少。

**推理资源**：在推理阶段，Encoder-only模型通常最高效；Encoder-Decoder模型的推理复杂度中等；Decoder-only模型的自回归生成过程相对较慢。

**存储需求**：三种架构的存储需求主要取决于模型规模，但在相同参数量下，存储需求基本相当。

### 5.3 应用场景选择指南

**选择Encoder-only模型的场景**：
- 主要进行文本理解和分析任务
- 对推理速度有较高要求
- 计算资源相对有限
- 需要处理大量短文本

**选择Encoder-Decoder模型的场景**：
- 需要进行序列到序列的转换
- 要求统一的多任务处理能力
- 输入输出长度差异较大
- 需要精确的条件生成

**选择Decoder-only模型的场景**：
- 主要进行文本生成任务
- 需要强大的创意和推理能力
- 希望通过提示工程解决多种任务
- 有充足的计算资源

通过深入理解这三种预训练语言模型架构的特点和适用场景，我们可以根据具体需求选择最合适的模型，并设计相应的微调策略。每种架构都有其独特的价值和应用空间，在实际应用中需要综合考虑任务特点、资源限制和性能要求来做出最佳选择。