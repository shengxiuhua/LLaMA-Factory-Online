# 2. æ•°æ®é¢„å¤„ç†æµç¨‹

æ•°æ®é¢„å¤„ç†æ˜¯å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥æœ‰æ•ˆå­¦ä¹ çš„æ ¼å¼çš„å…³é”®æ­¥éª¤ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ä»…æ¶‰åŠæŠ€æœ¯å±‚é¢çš„æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–ï¼Œæ›´é‡è¦çš„æ˜¯è¦ç†è§£æ•°æ®çš„ç‰¹ç‚¹å’Œä»»åŠ¡çš„éœ€æ±‚ï¼Œåˆ¶å®šåˆé€‚çš„é¢„å¤„ç†ç­–ç•¥ã€‚

## 2.1 æ–‡æœ¬æ¸…æ´—ï¼šå»å™ªä¸æ ‡å‡†åŒ–

æ–‡æœ¬æ¸…æ´—æ˜¯æ•°æ®é¢„å¤„ç†çš„ç¬¬ä¸€æ­¥ï¼Œä¹Ÿæ˜¯æœ€é‡è¦çš„æ­¥éª¤ä¹‹ä¸€ã€‚åŸå§‹æ–‡æœ¬æ•°æ®å¾€å¾€åŒ…å«å„ç§å™ªéŸ³å’Œä¸è§„èŒƒå†…å®¹ï¼Œè¿™äº›å™ªéŸ³å¦‚æœä¸ç»è¿‡å¤„ç†ï¼Œä¼šä¸¥é‡å½±å“æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

**ä¸ºä»€ä¹ˆæ–‡æœ¬æ¸…æ´—å¦‚æ­¤é‡è¦ï¼Ÿ**

æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘ä»¬è¦æ•™ä¸€ä¸ªå­©å­å­¦ä¹ é˜…è¯»ï¼Œç»™ä»–çš„ä¹¦ç±ä¸­å……æ»¡äº†é”™åˆ«å­—ã€ä¹±ç ã€é‡å¤çš„å¥å­ï¼Œé‚£ä¹ˆä»–å¾ˆéš¾å­¦ä¼šæ­£ç¡®çš„è¯­è¨€è¡¨è¾¾ã€‚åŒæ ·ï¼Œæ¨¡å‹ä»æ–‡æœ¬æ•°æ®ä¸­å­¦ä¹ è¯­è¨€æ¨¡å¼æ—¶ï¼Œå™ªéŸ³æ•°æ®ä¼šå¹²æ‰°æ­£å¸¸çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¯¼è‡´æ¨¡å‹å­¦åˆ°é”™è¯¯çš„æ¨¡å¼ã€‚

**å¸¸è§çš„æ–‡æœ¬å™ªéŸ³ç±»å‹ï¼š**

1. **HTMLæ ‡ç­¾å’Œç½‘é¡µå…ƒç´ **ï¼šå¦‚`<div>`, `<p>`, `&nbsp;`ç­‰
2. **ç‰¹æ®Šå­—ç¬¦å’Œä¹±ç **ï¼šå¦‚`Ã¢â‚¬â„¢`, `ÃƒÂ¡`ç­‰ç¼–ç é—®é¢˜
3. **é‡å¤å†…å®¹**ï¼šåŒä¸€æ®µæ–‡å­—é‡å¤å‡ºç°
4. **æ ¼å¼ä¸ä¸€è‡´**ï¼šæ—¥æœŸã€æ•°å­—ã€æ ‡ç‚¹ç¬¦å·æ ¼å¼æ··ä¹±
5. **æ— å…³ä¿¡æ¯**ï¼šå¹¿å‘Šã€ç‰ˆæƒå£°æ˜ã€å¯¼èˆªèœå•ç­‰

**å»é‡å¤„ç†çš„æ·±å…¥åˆ†æ**

å»é‡æ˜¯æ–‡æœ¬æ¸…æ´—ä¸­çš„é‡è¦ç¯èŠ‚ï¼Œä½†éœ€è¦åŒºåˆ†ä¸åŒç±»å‹çš„é‡å¤ï¼š

**å®Œå…¨é‡å¤**ï¼šä¸¤ä¸ªæ–‡æœ¬å®Œå…¨ç›¸åŒï¼Œè¿™ç§æƒ…å†µæ¯”è¾ƒå®¹æ˜“å¤„ç†ï¼Œç›´æ¥åˆ é™¤å³å¯ã€‚

**è¿‘ä¼¼é‡å¤**ï¼šä¸¤ä¸ªæ–‡æœ¬åœ¨å†…å®¹ä¸ŠåŸºæœ¬ç›¸åŒï¼Œä½†å¯èƒ½æœ‰ç»†å¾®å·®åˆ«ï¼Œå¦‚æ ‡ç‚¹ç¬¦å·ã€ç©ºæ ¼ã€å¤§å°å†™ç­‰ã€‚è¿™ç§æƒ…å†µéœ€è¦ä½¿ç”¨ç›¸ä¼¼åº¦ç®—æ³•æ¥è¯†åˆ«ã€‚

**è¯­ä¹‰é‡å¤**ï¼šä¸¤ä¸ªæ–‡æœ¬è¡¨è¾¾çš„æ„æ€ç›¸åŒï¼Œä½†ç”¨è¯ä¸åŒã€‚è¿™ç§æƒ…å†µæœ€éš¾å¤„ç†ï¼Œé€šå¸¸éœ€è¦ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹ã€‚

**å®é™…æ¡ˆä¾‹ï¼šæ–°é—»æ•°æ®å»é‡**

æŸæ–°é—»èšåˆç½‘ç«™æ”¶é›†äº†100ä¸‡æ¡æ–°é—»æ•°æ®ï¼Œå‘ç°å­˜åœ¨å¤§é‡é‡å¤ï¼š
- å®Œå…¨é‡å¤ï¼š15%ï¼ˆä¸»è¦æ˜¯è½¬è½½æ–‡ç« ï¼‰
- è¿‘ä¼¼é‡å¤ï¼š25%ï¼ˆåŒä¸€æ–°é—»çš„ä¸åŒç‰ˆæœ¬ï¼‰
- è¯­ä¹‰é‡å¤ï¼š10%ï¼ˆä¸åŒåª’ä½“å¯¹åŒä¸€äº‹ä»¶çš„æŠ¥é“ï¼‰

ç»è¿‡å»é‡å¤„ç†åï¼Œæœ‰æ•ˆæ•°æ®é‡å‡å°‘åˆ°50ä¸‡æ¡ï¼Œä½†æ¨¡å‹è®­ç»ƒæ•ˆæœæ˜¾è‘—æå‡ã€‚

```python
import re
import html
import hashlib

class TextCleaner:
    """è½»é‡çº§æ–‡æœ¬æ¸…æ´—å™¨"""
    
    def __init__(self, language='zh'):
        self.language = language
        self.seen_hashes = set()

    def clean(self, text):
        """ä¸»æ¸…æ´—å‡½æ•°ï¼šæ¸…ç†HTMLã€URLã€å¤šä½™ç©ºç™½ã€ç¼–ç é—®é¢˜"""
        if not isinstance(text, str) or not text.strip():
            return ""
        
        # 1. HTML è§£ç ä¸æ ‡ç­¾ç§»é™¤
        text = html.unescape(text)
        text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤HTMLæ ‡ç­¾
        
        # 2. ç§»é™¤URLå’Œé‚®ç®±
        text = re.sub(r'http[s]?://\S+|www\.\S+', '', text)
        text = re.sub(r'\S+@\S+\.\S+', '', text)
        
        # 3. ä¿®å¤å¸¸è§ç¼–ç ä¹±ç 
        encoding_map = {
            'Ã¢â‚¬â„¢': "'", 'Ã¢â‚¬Å“': '"', 'Ã¢â‚¬': '"', 
            'ÃƒÂ¡': 'Ã¡', 'ÃƒÂ©': 'Ã©', 'ÃƒÂ­': 'Ã­'
        }
        for bad, good in encoding_map.items():
            text = text.replace(bad, good)
        
        # 4. æ ‡å‡†åŒ–ç©ºç™½ï¼ˆç©ºæ ¼ã€æ¢è¡Œï¼‰
        text = re.sub(r'\s+', ' ', text)  # æ‰€æœ‰ç©ºç™½è½¬ä¸ºå•ç©ºæ ¼
        text = text.strip()
        
        return text

    def remove_duplicates(self, texts, method='hash'):
        """å»é‡ï¼šæ”¯æŒ exact æˆ– hashï¼ˆé»˜è®¤ï¼‰"""
        if method == 'exact':
            seen = set()
            return [t for t in texts if not (t in seen or seen.add(t))]
        
        elif method == 'hash':
            unique = []
            for t in texts:
                h = hashlib.md5(t.encode()).hexdigest()
                if h not in self.seen_hashes:
                    self.seen_hashes.add(h)
                    unique.append(t)
            return unique

            # ç¤ºä¾‹æ•°æ®
texts = [
    "<p>è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„äº§å“ï¼ï¼</p>",
    "è´¨é‡ä¸é”™ ğŸ˜Š http://example.com",
    "æœåŠ¡æ€åº¦å¾ˆå·®çš„çš„ï¼Œä¸æ¨è",
    "<p>è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„äº§å“ï¼ï¼</p>",  # é‡å¤
    "",  # ç©º
    "Ã¢â‚¬Å“This is greatÃ¢â‚¬"
]

# åˆå§‹åŒ–æ¸…æ´—å™¨
cleaner = TextCleaner(language='zh')

# æ¸…æ´—æ¯æ¡æ–‡æœ¬
cleaned = [cleaner.clean(t) for t in texts]
print("æ¸…æ´—å:")
for t in cleaned:
    if t: print(f"â€¢ {t}")

# å»é‡ï¼ˆåŸºäºå“ˆå¸Œï¼‰
unique_texts = cleaner.remove_duplicates([t for t in cleaned if t])
print("\nå»é‡å:")
for t in unique_texts:
    print(f"â€¢ {t}")
```

## 2.2 æ ¼å¼æ ‡å‡†åŒ–ï¼šé€‚é…ä¸åŒä»»åŠ¡ç±»å‹

æ ¼å¼æ ‡å‡†åŒ–æ˜¯å°†æ¸…æ´—åçš„æ•°æ®è½¬æ¢ä¸ºç‰¹å®šä»»åŠ¡æ‰€éœ€æ ¼å¼çš„è¿‡ç¨‹ã€‚ä¸åŒçš„NLPä»»åŠ¡å¯¹æ•°æ®æ ¼å¼æœ‰ä¸åŒçš„è¦æ±‚ï¼Œæ­£ç¡®çš„æ ¼å¼åŒ–å¯¹æ¨¡å‹è®­ç»ƒæ•ˆæœè‡³å…³é‡è¦ã€‚

### ä¸ºä»€ä¹ˆæ ¼å¼æ ‡å‡†åŒ–å¦‚æ­¤é‡è¦ï¼Ÿ

æ¨¡å‹å°±åƒä¸€ä¸ªæŒ‘å‰”çš„è¯»è€…ï¼Œå®ƒæœŸæœ›æ•°æ®ä»¥ç‰¹å®šçš„æ ¼å¼å‘ˆç°ã€‚å¦‚æœæ ¼å¼ä¸ç»Ÿä¸€æˆ–ä¸æ­£ç¡®ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•æ­£ç¡®ç†è§£æ•°æ®çš„å«ä¹‰ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸ä½³ã€‚å°±åƒæˆ‘ä»¬é˜…è¯»ä¸€æœ¬ä¹¦ï¼Œå¦‚æœæ ¼å¼æ··ä¹±ã€ç« èŠ‚ä¸æ¸…ï¼Œç†è§£èµ·æ¥å°±ä¼šå¾ˆå›°éš¾ã€‚

### ä¸åŒä»»åŠ¡çš„æ ¼å¼éœ€æ±‚åˆ†æï¼š

**1. åˆ†ç±»ä»»åŠ¡æ ¼å¼**
åˆ†ç±»ä»»åŠ¡æ˜¯æœ€åŸºç¡€çš„NLPä»»åŠ¡ä¹‹ä¸€ï¼Œéœ€è¦æ–‡æœ¬å’Œå¯¹åº”çš„æ ‡ç­¾ã€‚çœ‹ä¼¼ç®€å•ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­æœ‰å¾ˆå¤šç»†èŠ‚éœ€è¦æ³¨æ„ï¼š

- **å•æ ‡ç­¾åˆ†ç±»**ï¼šæ¯ä¸ªæ–‡æœ¬å¯¹åº”ä¸€ä¸ªæ ‡ç­¾
- **å¤šæ ‡ç­¾åˆ†ç±»**ï¼šæ¯ä¸ªæ–‡æœ¬å¯ä»¥å¯¹åº”å¤šä¸ªæ ‡ç­¾
- **å±‚æ¬¡åˆ†ç±»**ï¼šæ ‡ç­¾ä¹‹é—´å­˜åœ¨å±‚æ¬¡å…³ç³»

**2. ç”Ÿæˆä»»åŠ¡æ ¼å¼**
ç”Ÿæˆä»»åŠ¡éœ€è¦è¾“å…¥æç¤ºï¼ˆpromptï¼‰å’ŒæœŸæœ›çš„è¾“å‡ºã€‚è¿™ç±»ä»»åŠ¡çš„æ ¼å¼è®¾è®¡ç›´æ¥å½±å“æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼š

- **æ–‡æœ¬æ‘˜è¦**ï¼šåŸæ–‡ -> æ‘˜è¦
- **æœºå™¨ç¿»è¯‘**ï¼šæºè¯­è¨€ -> ç›®æ ‡è¯­è¨€
- **å¯¹è¯ç”Ÿæˆ**ï¼šä¸Šä¸‹æ–‡ -> å›å¤

**3. æŒ‡ä»¤å¾®è°ƒæ ¼å¼**
æŒ‡ä»¤å¾®è°ƒæ˜¯å½“å‰å¤§æ¨¡å‹è®­ç»ƒçš„ä¸»æµæ–¹æ³•ï¼Œé€šå¸¸é‡‡ç”¨"æŒ‡ä»¤-è¾“å…¥-è¾“å‡º"çš„ä¸‰æ®µå¼æ ¼å¼ï¼š

```
### Instruction:
è¯·åˆ†æè¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘

### Input:
è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œæˆ‘å¾ˆæ»¡æ„

### Response:
è¿™æ®µæ–‡æœ¬è¡¨è¾¾äº†ç§¯ææ­£é¢çš„æƒ…æ„Ÿã€‚ç”¨æˆ·å¯¹äº§å“è¡¨ç¤ºæ»¡æ„ï¼Œæ•´ä½“æƒ…æ„Ÿå€¾å‘ä¸ºæ­£é¢ã€‚
```

**4. é—®ç­”ä»»åŠ¡æ ¼å¼**
é—®ç­”ä»»åŠ¡éœ€è¦é—®é¢˜ã€ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰å’Œç­”æ¡ˆã€‚æ ¹æ®é—®ç­”ç±»å‹çš„ä¸åŒï¼Œæ ¼å¼ä¹Ÿæœ‰æ‰€å·®å¼‚ï¼š

- **æŠ½å–å¼é—®ç­”**ï¼šç­”æ¡ˆç›´æ¥ä»ä¸Šä¸‹æ–‡ä¸­æŠ½å–
- **ç”Ÿæˆå¼é—®ç­”**ï¼šéœ€è¦åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
- **å¼€æ”¾åŸŸé—®ç­”**ï¼šä¸ä¾èµ–ç‰¹å®šä¸Šä¸‹æ–‡

```python
from typing import List, Dict, Any

class DataFormatter:
    """è½»é‡çº§æ•°æ®æ ¼å¼åŒ–å™¨ â€”â€” æ”¯æŒä¸»æµNLPä»»åŠ¡"""
    
    def format_data(self, task_type: str, data: List[Dict[str, Any]], **kwargs) -> List[Dict[str, Any]]:
        """
        æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨æ ¼å¼åŒ–æ•°æ®
        
        æ”¯æŒçš„ä»»åŠ¡ç±»å‹ï¼š
          - 'classification'       : {'text', 'label'}
          - 'generation'           : {'prompt', 'response'}
          - 'instruction_tuning'   : {'instruction', 'input' (å¯é€‰), 'output'}
          - 'qa'                   : {'question', 'answer', 'context' (å¯é€‰)}
          - 'dialogue'             : {'conversation'} ï¼ˆåˆ—è¡¨æˆ–å­—ç¬¦ä¸²ï¼‰
        """
        if task_type == "classification":
            return self._format_classification(data, **kwargs)
        elif task_type == "generation":
            return self._format_generation(data, **kwargs)
        elif task_type == "instruction_tuning":
            return self._format_instruction(data)
        elif task_type == "qa":
            return self._format_qa(data)
        elif task_type == "dialogue":
            return self._format_dialogue(data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}")

    def _format_classification(self, data, text_field="text", label_field="label"):
        formatted = []
        for item in data:
            if text_field in item and label_field in item:
                formatted.append({
                    "text": str(item[text_field]).strip(),
                    "label": str(item[label_field]).strip(),
                    "task_type": "classification"
                })
        return formatted

    def _format_generation(self, data, prompt_field="prompt", response_field="response"):
        formatted = []
        for item in data:
            if prompt_field in item and response_field in item:
                prompt = str(item[prompt_field]).strip()
                response = str(item[response_field]).strip()
                # æ„å»ºå®Œæ•´è®­ç»ƒæ–‡æœ¬
                text = f"Prompt: {prompt}\nResponse: {response}"
                formatted.append({
                    "text": text,
                    "prompt": prompt,
                    "response": response,
                    "task_type": "generation"
                })
        return formatted

    def _format_instruction(self, data):
        formatted = []
        for item in data:
            if "instruction" in item and "output" in item:
                instr = str(item["instruction"]).strip()
                inp = str(item.get("input", "")).strip()
                out = str(item["output"]).strip()
                
                if inp:
                    text = f"### Instruction:\n{instr}\n\n### Input:\n{inp}\n\n### Response:\n{out}"
                else:
                    text = f"### Instruction:\n{instr}\n\n### Response:\n{out}"
                
                formatted.append({
                    "text": text,
                    "instruction": instr,
                    "input": inp,
                    "output": out,
                    "task_type": "instruction_tuning"
                })
        return formatted

    def _format_qa(self, data):
        formatted = []
        for item in data:
            if "question" in item and "answer" in item:
                q = str(item["question"]).strip()
                a = str(item["answer"]).strip()
                ctx = str(item.get("context", "")).strip()
                
                if ctx:
                    text = f"Context: {ctx}\n\nQuestion: {q}\nAnswer: {a}"
                else:
                    text = f"Question: {q}\nAnswer: {a}"
                
                formatted.append({
                    "text": text,
                    "question": q,
                    "answer": a,
                    "context": ctx,
                    "task_type": "qa"
                })
        return formatted

    def _format_dialogue(self, data):
        formatted = []
        for item in data:
            conv = item.get("conversation")
            if not conv:
                continue
            
            if isinstance(conv, list):
                # è½¬ä¸ºäº¤æ›¿å¯¹è¯æ ¼å¼
                lines = []
                for turn in conv:
                    role = turn.get("role", "unknown")
                    content = turn.get("content", "").strip()
                    lines.append(f"{role.title()}: {content}")
                text = "\n".join(lines)
            else:
                text = str(conv).strip()
            
            formatted.append({
                "text": text,
                "conversation": text,
                "task_type": "dialogue"
            })
        return formatted

        formatter = DataFormatter()

# æƒ…æ„Ÿåˆ†ç±»
data1 = [{"review": "å¾ˆå¥½ï¼", "sentiment": "positive"}]
cls = formatter.format_data("classification", data1, text_field="review", label_field="sentiment")

# æŒ‡ä»¤å¾®è°ƒ
data2 = [{"instruction": "ç¿»è¯‘", "input": "Hello", "output": "ä½ å¥½"}]
inst = formatter.format_data("instruction_tuning", data2)

# æ‰“å°ç»“æœ
print(cls[0]["text"])      # â†’ "å¾ˆå¥½ï¼"
print(inst[0]["text"])     # â†’ æŒ‡ä»¤æ¨¡æ¿æ ¼å¼
```

## 2.3 æ•°æ®å¢å¼ºï¼šè§£å†³å°æ ·æœ¬é—®é¢˜

å½“è®­ç»ƒæ•°æ®ä¸è¶³æ—¶ï¼Œæ•°æ®å¢å¼ºæŠ€æœ¯å¯ä»¥é€šè¿‡ç”Ÿæˆæ–°çš„è®­ç»ƒæ ·æœ¬æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚æ•°æ®å¢å¼ºçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨ä¿æŒè¯­ä¹‰ä¸å˜çš„å‰æä¸‹å¢åŠ æ•°æ®çš„å¤šæ ·æ€§ï¼Œè®©æ¨¡å‹è§åˆ°æ›´å¤šçš„è¡¨è¾¾æ–¹å¼ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

**æ•°æ®å¢å¼ºçš„é‡è¦æ€§**

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé«˜è´¨é‡çš„æ ‡æ³¨æ•°æ®å¾€å¾€ç¨€ç¼ºä¸”æ˜‚è´µã€‚ç‰¹åˆ«æ˜¯åœ¨ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ã€é‡‘èï¼‰ï¼Œè·å–è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®å¯èƒ½éœ€è¦æ•°æœˆæ—¶é—´å’Œå¤§é‡æˆæœ¬ã€‚æ•°æ®å¢å¼ºæŠ€æœ¯å¯ä»¥åœ¨æœ‰é™çš„åŸå§‹æ•°æ®åŸºç¡€ä¸Šï¼Œç”Ÿæˆæ›´å¤šçš„è®­ç»ƒæ ·æœ¬ï¼Œæœ‰æ•ˆç¼“è§£æ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚

**æ•°æ®å¢å¼ºçš„æŒ‘æˆ˜**

ç„¶è€Œï¼Œæ–‡æœ¬æ•°æ®å¢å¼ºæ¯”å›¾åƒæ•°æ®å¢å¼ºæ›´åŠ å›°éš¾ï¼Œå› ä¸ºï¼š
1. **è¯­ä¹‰ä¿æŒ**ï¼šæ–‡æœ¬çš„å¾®å°æ”¹åŠ¨å¯èƒ½å¯¼è‡´è¯­ä¹‰å‘ç”Ÿå˜åŒ–
2. **è¯­æ³•æ­£ç¡®æ€§**ï¼šå¢å¼ºåçš„æ–‡æœ¬å¿…é¡»ä¿æŒè¯­æ³•æ­£ç¡®
3. **ä¸Šä¸‹æ–‡ä¸€è‡´æ€§**ï¼šè¯æ±‡æ›¿æ¢å¿…é¡»è€ƒè™‘ä¸Šä¸‹æ–‡ç¯å¢ƒ
4. **æ ‡ç­¾ä¸€è‡´æ€§**ï¼šå¢å¼ºåçš„æ–‡æœ¬æ ‡ç­¾åº”è¯¥ä¿æŒä¸å˜

**æ•°æ®å¢å¼ºæŠ€æœ¯è¯¦è§£**

**1. åŒä¹‰è¯æ›¿æ¢ï¼ˆSynonym Replacementï¼‰**

è¿™æ˜¯æœ€å¸¸ç”¨çš„æ–‡æœ¬å¢å¼ºæŠ€æœ¯ä¹‹ä¸€ã€‚é€šè¿‡å°†æ–‡æœ¬ä¸­çš„è¯æ±‡æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œå¯ä»¥ç”Ÿæˆè¯­ä¹‰ç›¸ä¼¼ä½†è¡¨è¾¾ä¸åŒçš„æ–‡æœ¬ã€‚

**å®é™…æ¡ˆä¾‹ï¼šç”µå•†è¯„è®ºå¢å¼º**
åŸæ–‡ï¼š"è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½"
å¢å¼ºåï¼š"è¿™ä¸ªå•†å“å“è´¨å¾ˆæ£’"ã€"è¿™ä¸ªç‰©å“è´¨é‡å¾ˆä¼˜ç§€"

éœ€è¦æ³¨æ„çš„é—®é¢˜ï¼š
- åŒä¹‰è¯çš„é€‰æ‹©è¦è€ƒè™‘ä¸Šä¸‹æ–‡
- é¿å…æ›¿æ¢å…³é”®è¯ï¼ˆå¦‚å“ç‰Œåã€ä¸“ä¸šæœ¯è¯­ï¼‰
- ä¿æŒè¯­è¨€é£æ ¼çš„ä¸€è‡´æ€§

**2. éšæœºæ’å…¥ï¼ˆRandom Insertionï¼‰**

åœ¨æ–‡æœ¬ä¸­éšæœºä½ç½®æ’å…¥åŒä¹‰è¯ï¼Œå¢åŠ æ–‡æœ¬çš„ä¸°å¯Œæ€§ã€‚

**3. éšæœºäº¤æ¢ï¼ˆRandom Swapï¼‰**

éšæœºäº¤æ¢æ–‡æœ¬ä¸­ä¸¤ä¸ªè¯çš„ä½ç½®ï¼Œåœ¨ä¸æ”¹å˜æ•´ä½“è¯­ä¹‰çš„å‰æä¸‹å¢åŠ è¡¨è¾¾çš„å¤šæ ·æ€§ã€‚

**4. éšæœºåˆ é™¤ï¼ˆRandom Deletionï¼‰**

éšæœºåˆ é™¤æ–‡æœ¬ä¸­çš„éƒ¨åˆ†è¯æ±‡ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯ä¸­çš„ä¸å®Œæ•´è¡¨è¾¾ã€‚

**5. å›è¯‘æŠ€æœ¯ï¼ˆBack Translationï¼‰**

å°†æ–‡æœ¬ç¿»è¯‘æˆå…¶ä»–è¯­è¨€å†ç¿»è¯‘å›æ¥ï¼Œå¯ä»¥äº§ç”Ÿè¯­ä¹‰ç›¸ä¼¼ä½†è¡¨è¾¾ä¸åŒçš„æ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚åˆç”Ÿæˆè‡ªç„¶ã€æµç•…çš„å¢å¼ºæ–‡æœ¬ã€‚

**å®é™…æ¡ˆä¾‹ï¼šå®¢æœå¯¹è¯å¢å¼º**
åŸæ–‡ï¼š"æˆ‘æƒ³é€€è´§"
è‹±æ–‡ç¿»è¯‘ï¼š"I want to return the product"
å›è¯‘ç»“æœï¼š"æˆ‘æƒ³è¦é€€æ¢å•†å“"

**6. ä¸Šä¸‹æ–‡æ‰©å……ï¼ˆContext Augmentationï¼‰**

ä¸ºåŸæœ‰æ–‡æœ¬æ·»åŠ ç›¸å…³çš„èƒŒæ™¯ä¿¡æ¯æˆ–ä¸Šä¸‹æ–‡ï¼Œå¢åŠ æ•°æ®çš„ä¸°å¯Œæ€§ã€‚

```python
import random
import jieba

class DataAugmenter:
    """è½»é‡çº§æ–‡æœ¬æ•°æ®å¢å¼ºå™¨ï¼ˆæ— ç½‘ç»œã€æ—  heavy ä¾èµ–ï¼‰"""
    
    def __init__(self, language='zh'):
        self.language = language
        self.stop_words = {
            'zh': set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´']),
            'en': set(['the', 'a', 'an', 'and', 'or', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'is', 'are', 'was', 'were'])
        }

    def _tokenize(self, text):
        if self.language == 'zh':
            return list(jieba.cut(text))
        else:
            return text.split()

    def _detokenize(self, tokens):
        if self.language == 'zh':
            return ''.join(tokens)
        else:
            return ' '.join(tokens)

    def random_swap(self, text, n=1):
        """éšæœºäº¤æ¢ä¸¤ä¸ªè¯çš„ä½ç½®"""
        words = self._tokenize(text)
        if len(words) < 2:
            return text
        for _ in range(n):
            i, j = random.sample(range(len(words)), 2)
            words[i], words[j] = words[j], words[i]
        return self._detokenize(words)

    def random_deletion(self, text, p=0.1):
        """ä»¥æ¦‚ç‡ p åˆ é™¤éåœç”¨è¯"""
        words = self._tokenize(text)
        if len(words) <= 1:
            return text
        
        new_words = []
        for word in words:
            if word in self.stop_words.get(self.language, set()):
                new_words.append(word)  # åœç”¨è¯ä¿ç•™
            elif random.uniform(0, 1) > p:
                new_words.append(word)
        
        if not new_words:  # è‡³å°‘ä¿ç•™ä¸€ä¸ªè¯
            new_words = [random.choice(words)]
        
        return self._detokenize(new_words)

    def random_insertion(self, text, n=1):
        """éšæœºæ’å…¥å·²æœ‰è¯ï¼ˆé¿å…ä¾èµ–åŒä¹‰è¯åº“ï¼‰"""
        words = self._tokenize(text)
        if not words:
            return text
        
        for _ in range(n):
            word_to_insert = random.choice(words)
            insert_at = random.randint(0, len(words))
            words.insert(insert_at, word_to_insert)
        
        return self._detokenize(words)

    def augment(self, text, methods=['swap', 'delete', 'insert'], num_variants=3):
        """ç”Ÿæˆå¤šä¸ªå¢å¼ºå˜ä½“"""
        variants = set()
        for _ in range(num_variants * 2):  # å¤šé‡‡æ ·é˜²é‡å¤
            method = random.choice(methods)
            if method == 'swap':
                aug = self.random_swap(text, n=random.randint(1, 2))
            elif method == 'delete':
                aug = self.random_deletion(text, p=random.uniform(0.05, 0.15))
            elif method == 'insert':
                aug = self.random_insertion(text, n=random.randint(1, 2))
            else:
                continue
            
            if aug != text and len(aug.strip()) > 0:
                variants.add(aug)
                if len(variants) >= num_variants:
                    break
        
        return list(variants)

        # ä¸­æ–‡ç¤ºä¾‹
aug = DataAugmenter(language='zh')
text = "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½æˆ‘å¾ˆæ»¡æ„"

print("åŸæ–‡:", text)
print("å¢å¼ºç»“æœ:")
for v in aug.augment(text, num_variants=3):
    print(" â†’", v)

# è‹±æ–‡ç¤ºä¾‹
aug_en = DataAugmenter(language='en')
text_en = "The product quality is very good"
print("\nè‹±æ–‡åŸæ–‡:", text_en)
for v in aug_en.augment(text_en, num_variants=3):
    print(" â†’", v)
```

é€šè¿‡ç³»ç»Ÿçš„æ•°æ®é¢„å¤„ç†æµç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†åŸå§‹çš„ã€æ‚ä¹±çš„æ•°æ®è½¬æ¢ä¸ºé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ã€‚è¿™ä¸ªè¿‡ç¨‹è™½ç„¶ç¹çï¼Œä½†å¯¹äºæ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½è‡³å…³é‡è¦ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå»ºè®®æ ¹æ®å…·ä½“ä»»åŠ¡çš„ç‰¹ç‚¹å’Œæ•°æ®çš„ç‰¹æ€§ï¼Œçµæ´»è°ƒæ•´é¢„å¤„ç†ç­–ç•¥ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯ä¸åŒå¤„ç†æ–¹æ³•çš„æ•ˆæœã€‚

**æ•°æ®é¢„å¤„ç†çš„æœ€ä½³å®è·µå»ºè®®ï¼š**

1. **å»ºç«‹æ•°æ®è´¨é‡æ ‡å‡†**ï¼šåœ¨é¡¹ç›®å¼€å§‹æ—¶å°±æ˜ç¡®æ•°æ®è´¨é‡è¦æ±‚ï¼Œå»ºç«‹æ£€æŸ¥æ¸…å•
2. **ä¿ç•™åŸå§‹æ•°æ®**ï¼šåœ¨æ¯ä¸ªå¤„ç†æ­¥éª¤éƒ½ä¿ç•™åŸå§‹æ•°æ®çš„å¤‡ä»½ï¼Œä¾¿äºå›æº¯å’Œè°ƒè¯•
3. **æ¸è¿›å¼å¤„ç†**ï¼šä¸è¦ä¸€æ¬¡æ€§åº”ç”¨æ‰€æœ‰å¤„ç†æ­¥éª¤ï¼Œé€æ­¥éªŒè¯æ¯ä¸ªæ­¥éª¤çš„æ•ˆæœ
4. **è‡ªåŠ¨åŒ–æµç¨‹**ï¼šå°†å¸¸ç”¨çš„é¢„å¤„ç†æ­¥éª¤è‡ªåŠ¨åŒ–ï¼Œæé«˜æ•ˆç‡å’Œä¸€è‡´æ€§
5. **è´¨é‡ç›‘æ§**ï¼šå»ºç«‹æ•°æ®è´¨é‡ç›‘æ§æœºåˆ¶ï¼ŒåŠæ—¶å‘ç°å’Œå¤„ç†é—®é¢˜æ•°æ®
6. **æ–‡æ¡£è®°å½•**ï¼šè¯¦ç»†è®°å½•æ¯ä¸ªå¤„ç†æ­¥éª¤å’Œå‚æ•°è®¾ç½®ï¼Œç¡®ä¿å®éªŒçš„å¯é‡ç°æ€§

æ•°æ®é¢„å¤„ç†æ˜¯ä¸€ä¸ªéœ€è¦ä¸æ–­ä¼˜åŒ–çš„è¿‡ç¨‹ï¼Œéšç€å¯¹ä»»åŠ¡ç†è§£çš„æ·±å…¥å’Œæ•°æ®ç‰¹ç‚¹çš„å‘ç°ï¼Œé¢„å¤„ç†ç­–ç•¥ä¹Ÿéœ€è¦ç›¸åº”è°ƒæ•´ã€‚åªæœ‰é€šè¿‡ç²¾å¿ƒçš„æ•°æ®é¢„å¤„ç†ï¼Œæ‰èƒ½ä¸ºæ¨¡å‹è®­ç»ƒå¥ å®šåšå®çš„åŸºç¡€ã€‚
                 
                