## 3. æ•°æ®é¢„å¤„ç†æµç¨‹

æ•°æ®é¢„å¤„ç†æ˜¯å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥æœ‰æ•ˆå­¦ä¹ çš„æ ¼å¼çš„å…³é”®æ­¥éª¤ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ä»…æ¶‰åŠæŠ€æœ¯å±‚é¢çš„æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–ï¼Œæ›´é‡è¦çš„æ˜¯è¦ç†è§£æ•°æ®çš„ç‰¹ç‚¹å’Œä»»åŠ¡çš„éœ€æ±‚ï¼Œåˆ¶å®šåˆé€‚çš„é¢„å¤„ç†ç­–ç•¥ã€‚

### 3.1 æ–‡æœ¬æ¸…æ´—ï¼šå»å™ªä¸æ ‡å‡†åŒ–

æ–‡æœ¬æ¸…æ´—æ˜¯æ•°æ®é¢„å¤„ç†çš„ç¬¬ä¸€æ­¥ï¼Œä¹Ÿæ˜¯æœ€é‡è¦çš„æ­¥éª¤ä¹‹ä¸€ã€‚åŸå§‹æ–‡æœ¬æ•°æ®å¾€å¾€åŒ…å«å„ç§å™ªéŸ³å’Œä¸è§„èŒƒå†…å®¹ï¼Œè¿™äº›å™ªéŸ³å¦‚æœä¸ç»è¿‡å¤„ç†ï¼Œä¼šä¸¥é‡å½±å“æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

**ä¸ºä»€ä¹ˆæ–‡æœ¬æ¸…æ´—å¦‚æ­¤é‡è¦ï¼Ÿ**

æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘ä»¬è¦æ•™ä¸€ä¸ªå­©å­å­¦ä¹ é˜…è¯»ï¼Œç»™ä»–çš„ä¹¦ç±ä¸­å……æ»¡äº†é”™åˆ«å­—ã€ä¹±ç ã€é‡å¤çš„å¥å­ï¼Œé‚£ä¹ˆä»–å¾ˆéš¾å­¦ä¼šæ­£ç¡®çš„è¯­è¨€è¡¨è¾¾ã€‚åŒæ ·ï¼Œæ¨¡å‹ä»æ–‡æœ¬æ•°æ®ä¸­å­¦ä¹ è¯­è¨€æ¨¡å¼æ—¶ï¼Œå™ªéŸ³æ•°æ®ä¼šå¹²æ‰°æ­£å¸¸çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¯¼è‡´æ¨¡å‹å­¦åˆ°é”™è¯¯çš„æ¨¡å¼ã€‚

**å¸¸è§çš„æ–‡æœ¬å™ªéŸ³ç±»å‹ï¼š**

1. **HTMLæ ‡ç­¾å’Œç½‘é¡µå…ƒç´ **ï¼šå¦‚`<div>`, `<p>`, `&nbsp;`ç­‰
2. **ç‰¹æ®Šå­—ç¬¦å’Œä¹±ç **ï¼šå¦‚`Ã¢â‚¬â„¢`, `ÃƒÂ¡`ç­‰ç¼–ç é—®é¢˜
3. **é‡å¤å†…å®¹**ï¼šåŒä¸€æ®µæ–‡å­—é‡å¤å‡ºç°
4. **æ ¼å¼ä¸ä¸€è‡´**ï¼šæ—¥æœŸã€æ•°å­—ã€æ ‡ç‚¹ç¬¦å·æ ¼å¼æ··ä¹±
5. **æ— å…³ä¿¡æ¯**ï¼šå¹¿å‘Šã€ç‰ˆæƒå£°æ˜ã€å¯¼èˆªèœå•ç­‰

**å»é‡å¤„ç†çš„æ·±å…¥åˆ†æ**

å»é‡æ˜¯æ–‡æœ¬æ¸…æ´—ä¸­çš„é‡è¦ç¯èŠ‚ï¼Œä½†éœ€è¦åŒºåˆ†ä¸åŒç±»å‹çš„é‡å¤ï¼š

**å®Œå…¨é‡å¤**ï¼šä¸¤ä¸ªæ–‡æœ¬å®Œå…¨ç›¸åŒï¼Œè¿™ç§æƒ…å†µæ¯”è¾ƒå®¹æ˜“å¤„ç†ï¼Œç›´æ¥åˆ é™¤å³å¯ã€‚

**è¿‘ä¼¼é‡å¤**ï¼šä¸¤ä¸ªæ–‡æœ¬åœ¨å†…å®¹ä¸ŠåŸºæœ¬ç›¸åŒï¼Œä½†å¯èƒ½æœ‰ç»†å¾®å·®åˆ«ï¼Œå¦‚æ ‡ç‚¹ç¬¦å·ã€ç©ºæ ¼ã€å¤§å°å†™ç­‰ã€‚è¿™ç§æƒ…å†µéœ€è¦ä½¿ç”¨ç›¸ä¼¼åº¦ç®—æ³•æ¥è¯†åˆ«ã€‚

**è¯­ä¹‰é‡å¤**ï¼šä¸¤ä¸ªæ–‡æœ¬è¡¨è¾¾çš„æ„æ€ç›¸åŒï¼Œä½†ç”¨è¯ä¸åŒã€‚è¿™ç§æƒ…å†µæœ€éš¾å¤„ç†ï¼Œé€šå¸¸éœ€è¦ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹ã€‚

**å®é™…æ¡ˆä¾‹ï¼šæ–°é—»æ•°æ®å»é‡**

æŸæ–°é—»èšåˆç½‘ç«™æ”¶é›†äº†100ä¸‡æ¡æ–°é—»æ•°æ®ï¼Œå‘ç°å­˜åœ¨å¤§é‡é‡å¤ï¼š
- å®Œå…¨é‡å¤ï¼š15%ï¼ˆä¸»è¦æ˜¯è½¬è½½æ–‡ç« ï¼‰
- è¿‘ä¼¼é‡å¤ï¼š25%ï¼ˆåŒä¸€æ–°é—»çš„ä¸åŒç‰ˆæœ¬ï¼‰
- è¯­ä¹‰é‡å¤ï¼š10%ï¼ˆä¸åŒåª’ä½“å¯¹åŒä¸€äº‹ä»¶çš„æŠ¥é“ï¼‰

ç»è¿‡å»é‡å¤„ç†åï¼Œæœ‰æ•ˆæ•°æ®é‡å‡å°‘åˆ°50ä¸‡æ¡ï¼Œä½†æ¨¡å‹è®­ç»ƒæ•ˆæœæ˜¾è‘—æå‡ã€‚

```python
import re
import html
import unicodedata
from difflib import SequenceMatcher
import jieba
from collections import defaultdict
import hashlib

class TextCleaner:
    """æ–‡æœ¬æ¸…æ´—å™¨
    
    æä¾›å…¨é¢çš„æ–‡æœ¬æ¸…æ´—åŠŸèƒ½ï¼ŒåŒ…æ‹¬å»é‡ã€å»å™ªã€æ ‡å‡†åŒ–ç­‰ã€‚
    è¿™ä¸ªç±»è®¾è®¡æ—¶è€ƒè™‘äº†ä¸­è‹±æ–‡çš„å·®å¼‚ï¼Œæä¾›äº†çµæ´»çš„é…ç½®é€‰é¡¹ã€‚
    """
    
    def __init__(self, language='zh'):
        self.language = language
        self.duplicate_threshold = 0.85  # ç›¸ä¼¼ï¿½ï¿½é˜ˆå€¼
        self.seen_hashes = set()  # ç”¨äºå¿«é€Ÿå»é‡çš„å“ˆå¸Œé›†åˆ
        
    def remove_duplicates(self, texts, method='exact'):
        """å»é‡å¤„ç†
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            method: å»é‡æ–¹æ³• ('exact', 'similarity', 'hash')
        
        Returns:
            å»é‡åçš„æ–‡æœ¬åˆ—è¡¨
        """
        if method == 'exact':
            # å®Œå…¨é‡å¤å»é™¤ - æœ€å¿«é€Ÿçš„æ–¹æ³•
            seen = set()
            unique_texts = []
            for text in texts:
                if text not in seen:
                    seen.add(text)
                    unique_texts.append(text)
            return unique_texts
        
        elif method == 'hash':
            # åŸºäºå“ˆå¸Œçš„å¿«é€Ÿå»é‡ - é€‚åˆå¤§è§„æ¨¡æ•°æ®
            unique_texts = []
            for text in texts:
                # æ ‡å‡†åŒ–åè®¡ç®—å“ˆå¸Œ
                normalized_text = self._normalize_for_hash(text)
                text_hash = hashlib.md5(normalized_text.encode()).hexdigest()
                
                if text_hash not in self.seen_hashes:
                    self.seen_hashes.add(text_hash)
                    unique_texts.append(text)
            
            return unique_texts
        
        elif method == 'similarity':
            # åŸºäºç›¸ä¼¼åº¦çš„å»é‡ - æœ€å‡†ç¡®ä½†æœ€æ…¢
            unique_texts = []
            for text in texts:
                is_duplicate = False
                for existing_text in unique_texts:
                    similarity = SequenceMatcher(None, text, existing_text).ratio()
                    if similarity > self.duplicate_threshold:
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    unique_texts.append(text)
            
            return unique_texts
    
    def _normalize_for_hash(self, text):
        """ä¸ºå“ˆå¸Œè®¡ç®—æ ‡å‡†åŒ–æ–‡æœ¬"""
        # è½¬å°å†™
        text = text.lower()
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
        return text.strip()
    
    def clean_html_and_urls(self, text):
        """æ¸…ç†HTMLæ ‡ç­¾å’ŒURL
        
        ç½‘ç»œçˆ¬å–çš„æ•°æ®ç»å¸¸åŒ…å«HTMLæ ‡ç­¾å’ŒURLï¼Œè¿™äº›å†…å®¹å¯¹NLPä»»åŠ¡é€šå¸¸æ˜¯å™ªéŸ³ã€‚
        """
        # è§£ç HTMLå®ä½“ï¼ˆå¦‚&nbsp; &lt; &gt;ç­‰ï¼‰
        text = html.unescape(text)
        
        # ç§»é™¤HTMLæ ‡ç­¾
        # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼å¯ä»¥åŒ¹é…å¤§éƒ¨åˆ†HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # ç§»é™¤å¸¸è§çš„HTMLå®ä½“æ®‹ç•™
        text = re.sub(r'&\w+;', '', text)
        
        # ç§»é™¤URL
        # åŒ¹é…http/https URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # ç§»é™¤wwwå¼€å¤´çš„URL
        text = re.sub(r'www\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # ç§»é™¤é‚®ç®±åœ°å€
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', text)
        
        return text
    
    def normalize_whitespace(self, text):
        """æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        
        ç»Ÿä¸€å¤„ç†å„ç§ç©ºç™½å­—ç¬¦ï¼ŒåŒ…æ‹¬ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦ç­‰ã€‚
        """
        # ç»Ÿä¸€æ¢è¡Œç¬¦ï¼ˆWindows: \r\n, Mac: \r, Unix: \nï¼‰
        text = re.sub(r'\r\n|\r|\n', '\n', text)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦
        text = re.sub(r'[ \t]+', ' ', text)
        
        # ç§»é™¤å¤šä½™æ¢è¡Œï¼ˆä¿ç•™æ®µè½ç»“æ„ï¼‰
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        lines = text.split('\n')
        lines = [line.strip() for line in lines]
        text = '\n'.join(lines)
        
        # å»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()
        
        return text
    
    def handle_special_characters(self, text, keep_emoji=False, keep_punctuation=True):
        """å¤„ç†ç‰¹æ®Šå­—ç¬¦
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            keep_emoji: æ˜¯å¦ä¿ç•™emojiè¡¨æƒ…
            keep_punctuation: æ˜¯å¦ä¿ç•™æ ‡ç‚¹ç¬¦å·
        """
        if not keep_emoji:
            # ç§»é™¤emojiè¡¨æƒ…ç¬¦å·
            # UnicodeèŒƒå›´æ¶µç›–äº†å¤§éƒ¨åˆ†emoji
            text = re.sub(r'[\U0001F600-\U0001F64F]', '', text)  # è¡¨æƒ…ç¬¦å·
            text = re.sub(r'[\U0001F300-\U0001F5FF]', '', text)  # ç¬¦å·å’Œè±¡å½¢æ–‡å­—
            text = re.sub(r'[\U0001F680-\U0001F6FF]', '', text)  # äº¤é€šå’Œåœ°å›¾ç¬¦å·
            text = re.sub(r'[\U0001F1E0-\U0001F1FF]', '', text)  # å›½æ——
            text = re.sub(r'[\U00002702-\U000027B0]', '', text)  # æ‚é¡¹ç¬¦å·
            text = re.sub(r'[\U000024C2-\U0001F251]', '', text)  # å°é—­å­—ç¬¦
        
        if not keep_punctuation:
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆæ ¹æ®è¯­è¨€ä¿ç•™å¿…è¦æ ‡ç‚¹ï¼‰
            if self.language == 'zh':
                # ä¸­æ–‡ï¼šä¿ç•™ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
                text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]', '', text)
            else:
                # è‹±æ–‡ï¼šä¿ç•™åŸºæœ¬æ ‡ç‚¹ç¬¦å·
                text = re.sub(r'[^\w\s.,!?;:()\[\]"\''-]', '', text)
        
        # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆå¦‚åˆ¶è¡¨ç¬¦ã€æ¢é¡µç¬¦ç­‰ï¼‰
        text = ''.join(char for char in text if unicodedata.category(char)[0] != 'C' or char in '\n\t ')
        
        return text
    
    def correct_common_errors(self, text):
        """çº æ­£å¸¸è§é”™è¯¯
        
        åŸºäºè§„åˆ™çº æ­£ä¸€äº›å¸¸è§çš„æ–‡æœ¬é”™è¯¯ï¼Œå¦‚é‡å¤å­—ç¬¦ã€å¸¸è§æ‹¼å†™é”™è¯¯ç­‰ã€‚
        """
        # ä¸­æ–‡å¸¸è§é”™è¯¯çº æ­£
        if self.language == 'zh':
            corrections = {
                # é‡å¤å­—ç¬¦
                'çš„çš„': 'çš„',
                'äº†äº†': 'äº†',
                'æ˜¯æ˜¯': 'æ˜¯',
                'åœ¨åœ¨': 'åœ¨',
                'å’Œå’Œ': 'å’Œ',
                # é‡å¤æ ‡ç‚¹
                'ã€‚ã€‚': 'ã€‚',
                'ï¼Ÿï¼Ÿ': 'ï¼Ÿ',
                'ï¼ï¼': 'ï¼',
                'ï¼Œï¼Œ': 'ï¼Œ',
                # å¸¸è§é”™è¯¯
                'å› ä¸ºæ‰€ä»¥': 'å› ä¸º',  # é€»è¾‘é”™è¯¯
                'è™½ç„¶ä½†æ˜¯': 'è™½ç„¶',  # é€»è¾‘é”™è¯¯
            }
        else:
            # è‹±æ–‡å¸¸è§é”™è¯¯çº æ­£
            corrections = {
                # å¸¸è§æ‹¼å†™é”™è¯¯
                'teh': 'the',
                'adn': 'and',
                'taht': 'that',
                'youre': "you're",
                'dont': "don't",
                'cant': "can't",
                'wont': "won't",
                # é‡å¤å­—ç¬¦
                'goood': 'good',
                'sooo': 'so',
                'reallly': 'really',
            }
        
        for error, correction in corrections.items():
            text = text.replace(error, correction)
        
        # ç§»é™¤é‡å¤çš„æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'([.!?])\1+', r'\1', text)
        text = re.sub(r'([,;:])\1+', r'\1', text)
        
        return text
    
    def segment_chinese_text(self, text):
        """ä¸­æ–‡åˆ†è¯
        
        å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†ã€‚åˆ†è¯å¯¹äºä¸­æ–‡NLPä»»åŠ¡å¾ˆé‡è¦ï¼Œ
        å› ä¸ºä¸­æ–‡æ²¡æœ‰å¤©ç„¶çš„è¯è¾¹ç•Œã€‚
        """
        if self.language == 'zh':
            # ä½¿ç”¨jiebaåˆ†è¯
            words = jieba.cut(text)
            return ' '.join(words)
        return text
    
    def detect_and_fix_encoding_issues(self, text):
        """æ£€æµ‹å’Œä¿®å¤ç¼–ç é—®é¢˜
        
        ç½‘ç»œæ•°æ®ç»å¸¸å­˜åœ¨ç¼–ç é—®é¢˜ï¼Œå¯¼è‡´å‡ºç°ä¹±ç ã€‚
        """
        # å¸¸è§ç¼–ç é—®é¢˜çš„ä¿®å¤æ˜ å°„
        encoding_fixes = {
            'Ã¢â‚¬â„¢': "'",      # å•å¼•å·
            'Ã¢â‚¬Å“': '"',      # å·¦åŒå¼•å·
            'Ã¢â‚¬': '"',       # å³åŒå¼•å·
            'Ã¢â‚¬"': 'â€”',      # é•¿ç ´æŠ˜å·
            'Ã¢â‚¬"': 'â€“',      # çŸ­ç ´æŠ˜å·
            'ÃƒÂ¡': 'Ã¡',      # å¸¦é‡éŸ³çš„a
            'ÃƒÂ©': 'Ã©',      # å¸¦é‡éŸ³çš„e
            'ÃƒÂ­': 'Ã­',      # å¸¦é‡éŸ³çš„i
            'ÃƒÂ³': 'Ã³',      # å¸¦é‡éŸ³çš„o
            'ÃƒÂº': 'Ãº',      # å¸¦é‡éŸ³çš„u
        }
        
        for wrong, correct in encoding_fixes.items():
            text = text.replace(wrong, correct)
        
        return text
    
    def clean_text(self, text, 
                   remove_html=True, 
                   normalize_space=True, 
                   handle_special=True, 
                   correct_errors=True,
                   fix_encoding=True,
                   segment_chinese=False,
                   keep_emoji=False,
                   keep_punctuation=True):
        """ç»¼åˆæ–‡æœ¬æ¸…æ´—
        
        è¿™æ˜¯ä¸»è¦çš„æ¸…æ´—æ¥å£ï¼Œæ•´åˆäº†æ‰€æœ‰æ¸…æ´—åŠŸèƒ½ã€‚
        ç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©å¯ç”¨å“ªäº›æ¸…æ´—æ­¥éª¤ã€‚
        """
        
        if not isinstance(text, str):
            return ""
        
        # ä¿®å¤ç¼–ç é—®é¢˜
        if fix_encoding:
            text = self.detect_and_fix_encoding_issues(text)
        
        # HTMLå’ŒURLæ¸…ç†
        if remove_html:
            text = self.clean_html_and_urls(text)
        
        # ç©ºç™½å­—ç¬¦æ ‡å‡†åŒ–
        if normalize_space:
            text = self.normalize_whitespace(text)
        
        # ç‰¹æ®Šå­—ç¬¦å¤„ç†
        if handle_special:
            text = self.handle_special_characters(text, keep_emoji, keep_punctuation)
        
        # é”™è¯¯çº æ­£
        if correct_errors:
            text = self.correct_common_errors(text)
        
        # ä¸­æ–‡åˆ†è¯
        if segment_chinese and self.language == 'zh':
            text = self.segment_chinese_text(text)
        
        # æœ€ç»ˆæ¸…ç†
        text = self.normalize_whitespace(text)
        
        return text
    
    def batch_clean(self, texts, show_progress=True, **kwargs):
        """æ‰¹é‡æ¸…æ´—
        
        å¯¹å¤§é‡æ–‡æœ¬è¿›è¡Œæ‰¹é‡æ¸…æ´—ï¼Œæ”¯æŒè¿›åº¦æ˜¾ç¤ºã€‚
        """
        cleaned_texts = []
        total = len(texts)
        
        for i, text in enumerate(texts):
            if show_progress and i % 1000 == 0:
                print(f"æ¸…æ´—è¿›åº¦: {i}/{total} ({i/total*100:.1f}%)")
            
            cleaned_text = self.clean_text(text, **kwargs)
            if cleaned_text.strip():  # åªä¿ç•™éç©ºæ–‡æœ¬
                cleaned_texts.append(cleaned_text)
        
        if show_progress:
            print(f"æ¸…æ´—å®Œæˆ: {len(cleaned_texts)}/{total} æ¡æœ‰æ•ˆæ•°æ®")
        
        return cleaned_texts
    
    def generate_cleaning_report(self, original_texts, cleaned_texts):
        """ç”Ÿæˆæ¸…æ´—æŠ¥å‘Š
        
        åˆ†ææ¸…æ´—å‰åçš„æ•°æ®å˜åŒ–ï¼Œå¸®åŠ©è¯„ä¼°æ¸…æ´—æ•ˆæœã€‚
        """
        original_count = len(original_texts)
        cleaned_count = len(cleaned_texts)
        
        # è®¡ç®—é•¿åº¦å˜åŒ–
        original_lengths = [len(text) for text in original_texts if isinstance(text, str)]
        cleaned_lengths = [len(text) for text in cleaned_texts]
        
        report = {
            'data_count': {
                'original': original_count,
                'cleaned': cleaned_count,
                'removed': original_count - cleaned_count,
                'removal_rate': f"{(original_count - cleaned_count) / original_count * 100:.2f}%"
            },
            'text_length': {
                'original_avg': np.mean(original_lengths) if original_lengths else 0,
                'cleaned_avg': np.mean(cleaned_lengths) if cleaned_lengths else 0,
                'length_reduction': f"{(1 - np.mean(cleaned_lengths) / np.mean(original_lengths)) * 100:.2f}%" if original_lengths and cleaned_lengths else "N/A"
            },
            'quality_indicators': {
                'empty_texts_removed': original_count - cleaned_count,
                'avg_length_change': np.mean(cleaned_lengths) - np.mean(original_lengths) if original_lengths and cleaned_lengths else 0
            }
        }
        
        return report

# ä½¿ç”¨ç¤ºä¾‹ï¼šç”µå•†è¯„è®ºæ•°æ®æ¸…æ´—
sample_texts = [
    "<p>è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„äº§å“ï¼ï¼</p>",
    "è´¨é‡ä¸é”™ï¼Œæ¨èè´­ä¹° ğŸ˜Š http://example.com",
    "æœåŠ¡æ€åº¦å¾ˆå·®çš„çš„ï¼Œä¸æ¨è",
    "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„äº§å“ï¼ï¼",  # é‡å¤æ–‡æœ¬
    "ä»·æ ¼åˆç†ï¼Œæ€§ä»·æ¯”é«˜",
    "å¥½å¥½å¥½å¥½å¥½å¥½å¥½å¥½å¥½å¥½å¥½",  # é‡å¤å†…å®¹
    "",  # ç©ºæ–‡æœ¬
    "Ã¢â‚¬Å“This is a great productÃ¢â‚¬",  # ç¼–ç é—®é¢˜
    "<div>å®¢æœæ€åº¦å¾ˆå¥½</div>",
    "è”ç³»æˆ‘ä»¬ï¼šservice@example.com ç”µè¯ï¼š400-123-4567"
]

print("=== æ–‡æœ¬æ¸…æ´—ç¤ºä¾‹ ===")
cleaner = TextCleaner(language='zh')

# å•æ­¥æ¸…æ´—æ¼”ç¤º
print("åŸå§‹æ–‡æœ¬ç¤ºä¾‹:")
for i, text in enumerate(sample_texts[:3]):
    print(f"{i+1}. {text}")

print("\næ¸…æ´—å:")
for i, text in enumerate(sample_texts[:3]):
    cleaned = cleaner.clean_text(text)
    print(f"{i+1}. {cleaned}")

# å»é‡æ¼”ç¤º
print("\n=== å»é‡æ¼”ç¤º ===")
print(f"åŸå§‹æ–‡æœ¬æ•°é‡: {len(sample_texts)}")

# å®Œå…¨é‡å¤å»é™¤
exact_unique = cleaner.remove_duplicates(sample_texts, method='exact')
print(f"å®Œå…¨å»é‡å: {len(exact_unique)} æ¡")

# æ‰¹é‡æ¸…æ´—
print("\n=== æ‰¹é‡æ¸…æ´— ===")
cleaned_texts = cleaner.batch_clean(
    sample_texts,
    show_progress=False,
    remove_html=True,
    normalize_space=True,
    handle_special=True,
    correct_errors=True,
    fix_encoding=True,
    keep_emoji=False,
    keep_punctuation=True
)

# ç”Ÿæˆæ¸…æ´—æŠ¥å‘Š
report = cleaner.generate_cleaning_report(sample_texts, cleaned_texts)
print("æ¸…æ´—æŠ¥å‘Š:")
print(f"æ•°æ®é‡å˜åŒ–: {report['data_count']['original']} -> {report['data_count']['cleaned']} (ç§»é™¤ç‡: {report['data_count']['removal_rate']})")
print(f"å¹³å‡é•¿åº¦å˜åŒ–: {report['text_length']['original_avg']:.1f} -> {report['text_length']['cleaned_avg']:.1f}")
print(f"é•¿åº¦å‡å°‘: {report['text_length']['length_reduction']}")

print("\næ¸…æ´—åçš„æ–‡æœ¬:")
for i, text in enumerate(cleaned_texts):
    print(f"{i+1}. {text}")
```

### 3.2 æ ¼å¼æ ‡å‡†åŒ–ï¼šé€‚é…ä¸åŒä»»åŠ¡ç±»å‹

æ ¼å¼æ ‡å‡†åŒ–æ˜¯å°†æ¸…æ´—åçš„æ•°æ®è½¬æ¢ä¸ºç‰¹å®šä»»åŠ¡æ‰€éœ€æ ¼å¼çš„è¿‡ç¨‹ã€‚ä¸åŒçš„NLPä»»åŠ¡å¯¹æ•°æ®æ ¼å¼æœ‰ä¸åŒçš„è¦æ±‚ï¼Œæ­£ç¡®çš„æ ¼å¼åŒ–å¯¹æ¨¡å‹è®­ç»ƒæ•ˆæœè‡³å…³é‡è¦ã€‚

**ä¸ºä»€ä¹ˆæ ¼å¼æ ‡å‡†åŒ–å¦‚æ­¤é‡è¦ï¼Ÿ**

æ¨¡å‹å°±åƒä¸€ä¸ªæŒ‘å‰”çš„è¯»è€…ï¼Œå®ƒæœŸæœ›æ•°æ®ä»¥ç‰¹å®šçš„æ ¼å¼å‘ˆç°ã€‚å¦‚æœæ ¼å¼ä¸ç»Ÿä¸€æˆ–ä¸æ­£ç¡®ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•æ­£ç¡®ç†è§£æ•°æ®çš„å«ä¹‰ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸ä½³ã€‚å°±åƒæˆ‘ä»¬é˜…è¯»ä¸€æœ¬ä¹¦ï¼Œå¦‚æœæ ¼å¼æ··ä¹±ã€ç« èŠ‚ä¸æ¸…ï¼Œç†è§£èµ·æ¥å°±ä¼šå¾ˆå›°éš¾ã€‚

**ä¸åŒä»»åŠ¡çš„æ ¼å¼éœ€æ±‚åˆ†æï¼š**

**1. åˆ†ç±»ä»»åŠ¡æ ¼å¼**
åˆ†ç±»ä»»åŠ¡æ˜¯æœ€åŸºç¡€çš„NLPä»»åŠ¡ä¹‹ä¸€ï¼Œéœ€è¦æ–‡æœ¬å’Œå¯¹åº”çš„æ ‡ç­¾ã€‚çœ‹ä¼¼ç®€å•ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­æœ‰å¾ˆå¤šç»†èŠ‚éœ€è¦æ³¨æ„ï¼š

- **å•æ ‡ç­¾åˆ†ç±»**ï¼šæ¯ä¸ªæ–‡æœ¬å¯¹åº”ä¸€ä¸ªæ ‡ç­¾
- **å¤šæ ‡ç­¾åˆ†ç±»**ï¼šæ¯ä¸ªæ–‡æœ¬å¯ä»¥å¯¹åº”å¤šä¸ªæ ‡ç­¾
- **å±‚æ¬¡åˆ†ç±»**ï¼šæ ‡ç­¾ä¹‹é—´å­˜åœ¨å±‚æ¬¡å…³ç³»

**å®é™…æ¡ˆä¾‹ï¼šæ–°é—»åˆ†ç±»ç³»ç»Ÿ**
æŸæ–°é—»ç½‘ç«™éœ€è¦å¯¹æ–°é—»è¿›è¡Œè‡ªåŠ¨åˆ†ç±»ï¼Œæ¶‰åŠä»¥ä¸‹æŒ‘æˆ˜ï¼š
- ä¸€çº§åˆ†ç±»ï¼šæ”¿æ²»ã€ç»æµã€ä½“è‚²ã€å¨±ä¹ç­‰
- äºŒçº§åˆ†ç±»ï¼šæ”¿æ²»ä¸‹åˆ†ä¸ºå›½å†…æ”¿æ²»ã€å›½é™…æ”¿æ²»ç­‰
- éƒ¨åˆ†æ–°é—»å¯èƒ½å±äºå¤šä¸ªç±»åˆ«ï¼ˆå¦‚ä½“è‚²ç»æµæ–°é—»ï¼‰

**2. ç”Ÿæˆä»»åŠ¡æ ¼å¼**
ç”Ÿæˆä»»åŠ¡éœ€è¦è¾“å…¥æç¤ºï¼ˆpromptï¼‰å’ŒæœŸæœ›çš„è¾“å‡ºã€‚è¿™ç±»ä»»åŠ¡çš„æ ¼å¼è®¾è®¡ç›´æ¥å½±å“æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼š

- **æ–‡æœ¬æ‘˜è¦**ï¼šåŸæ–‡ -> æ‘˜è¦
- **æœºå™¨ç¿»è¯‘**ï¼šæºè¯­è¨€ -> ç›®æ ‡è¯­è¨€
- **å¯¹è¯ç”Ÿæˆ**ï¼šä¸Šä¸‹æ–‡ -> å›å¤

**3. æŒ‡ä»¤å¾®è°ƒæ ¼å¼**
æŒ‡ä»¤å¾®è°ƒæ˜¯å½“å‰å¤§æ¨¡å‹è®­ç»ƒçš„ä¸»æµæ–¹æ³•ï¼Œé€šå¸¸é‡‡ç”¨"æŒ‡ä»¤-è¾“å…¥-è¾“å‡º"çš„ä¸‰æ®µå¼æ ¼å¼ï¼š

```
### Instruction:
è¯·åˆ†æè¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘

### Input:
è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œæˆ‘å¾ˆæ»¡æ„

### Response:
è¿™æ®µæ–‡æœ¬è¡¨è¾¾äº†ç§¯ææ­£é¢çš„æƒ…æ„Ÿã€‚ç”¨æˆ·å¯¹äº§å“è¡¨ç¤ºæ»¡æ„ï¼Œæ•´ä½“æƒ…æ„Ÿå€¾å‘ä¸ºæ­£é¢ã€‚
```

**4. é—®ç­”ä»»åŠ¡æ ¼å¼**
é—®ç­”ä»»åŠ¡éœ€è¦é—®é¢˜ã€ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰å’Œç­”æ¡ˆã€‚æ ¹æ®é—®ç­”ç±»å‹çš„ä¸åŒï¼Œæ ¼å¼ä¹Ÿæœ‰æ‰€å·®å¼‚ï¼š

- **æŠ½å–å¼é—®ç­”**ï¼šç­”æ¡ˆç›´æ¥ä»ä¸Šä¸‹æ–‡ä¸­æŠ½å–
- **ç”Ÿæˆå¼é—®ç­”**ï¼šéœ€è¦åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
- **å¼€æ”¾åŸŸé—®ç­”**ï¼šä¸ä¾èµ–ç‰¹å®šä¸Šä¸‹æ–‡

```python
import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import pandas as pd

@dataclass
class TaskFormat:
    """ä»»åŠ¡æ ¼å¼å®šä¹‰
    
    å®šä¹‰ä¸åŒNLPä»»åŠ¡çš„æ•°æ®æ ¼å¼è§„èŒƒï¼ŒåŒ…æ‹¬å¿…éœ€å­—æ®µã€å¯é€‰å­—æ®µå’Œæ ¼å¼æ¨¡æ¿ã€‚
    """
    task_type: str
    required_fields: List[str]
    optional_fields: List[str] = None
    format_template: str = ""
    description: str = ""

class DataFormatter:
    """æ•°æ®æ ¼å¼åŒ–å™¨
    
    æä¾›å…¨é¢çš„æ•°æ®æ ¼å¼åŒ–åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§NLPä»»åŠ¡çš„æ•°æ®æ ¼å¼è½¬æ¢ã€‚
    è¿™ä¸ªç±»è®¾è®¡æ—¶è€ƒè™‘äº†æ ¼å¼çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚
    """
    
    def __init__(self):
        self.task_formats = {
            'classification': TaskFormat(
                task_type='classification',
                required_fields=['text', 'label'],
                optional_fields=['confidence', 'metadata'],
                format_template="Text: {text}\nLabel: {label}",
                description="æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œéœ€è¦æ–‡æœ¬å’Œå¯¹åº”çš„æ ‡ç­¾"
            ),
            'multi_label_classification': TaskFormat(
                task_type='multi_label_classification',
                required_fields=['text', 'labels'],
                optional_fields=['label_scores'],
                format_template="Text: {text}\nLabels: {labels}",
                description="å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œä¸€ä¸ªæ–‡æœ¬å¯ä»¥æœ‰å¤šä¸ªæ ‡ç­¾"
            ),
            'generation': TaskFormat(
                task_type='generation',
                required_fields=['prompt', 'response'],
                optional_fields=['context', 'metadata'],
                format_template="Prompt: {prompt}\nResponse: {response}",
                description="æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬æ‘˜è¦ã€ç¿»è¯‘ç­‰"
            ),
            'instruction_tuning': TaskFormat(
                task_type='instruction_tuning',
                required_fields=['instruction', 'output'],
                optional_fields=['input', 'system_prompt'],
                format_template="### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n{output}",
                description="æŒ‡ä»¤å¾®è°ƒæ ¼å¼ï¼Œç”¨äºè®­ç»ƒéµå¾ªæŒ‡ä»¤çš„æ¨¡å‹"
            ),
            'qa': TaskFormat(
                task_type='qa',
                required_fields=['question', 'answer'],
                optional_fields=['context', 'answer_start', 'answer_end'],
                format_template="Question: {question}\nContext: {context}\nAnswer: {answer}",
                description="é—®ç­”ä»»åŠ¡ï¼ŒåŒ…æ‹¬æŠ½å–å¼å’Œç”Ÿæˆå¼é—®ç­”"
            ),
            'dialogue': TaskFormat(
                task_type='dialogue',
                required_fields=['conversation'],
                optional_fields=['speaker_info', 'context'],
                format_template="Conversation:\n{conversation}",
                description="å¯¹è¯ä»»åŠ¡ï¼ŒåŒ…æ‹¬å•è½®å’Œå¤šè½®å¯¹è¯"
            ),
            'ner': TaskFormat(
                task_type='ner',
                required_fields=['text', 'entities'],
                optional_fields=['entity_types'],
                format_template="Text: {text}\nEntities: {entities}",
                description="å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡"
            )
        }
    
    def format_classification_data(self, data: List[Dict], 
                                 text_field='text', 
                                 label_field='label',
                                 multi_label=False):
        """æ ¼å¼åŒ–åˆ†ç±»æ•°æ®
        
        Args:
            data: åŸå§‹æ•°æ®åˆ—è¡¨
            text_field: æ–‡æœ¬å­—æ®µå
            label_field: æ ‡ç­¾å­—æ®µå
            multi_label: æ˜¯å¦ä¸ºå¤šæ ‡ç­¾åˆ†ç±»
        """
        formatted_data = []
        
        for item in data:
            if text_field in item and label_field in item:
                text = str(item[text_field]).strip()
                
                if multi_label:
                    # å¤šæ ‡ç­¾å¤„ç†
                    labels = item[label_field]
                    if isinstance(labels, str):
                        # å¦‚æœæ ‡ç­¾æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•åˆ†å‰²
                        labels = [l.strip() for l in labels.split(',')]
                    elif not isinstance(labels, list):
                        labels = [str(labels)]
                    
                    formatted_item = {
                        'text': text,
                        'labels': labels,
                        'task_type': 'multi_label_classification'
                    }
                else:
                    # å•æ ‡ç­¾å¤„ç†
                    label = str(item[label_field]).strip()
                    formatted_item = {
                        'text': text,
                        'label': label,
                        'task_type': 'classification'
                    }
                
                # æ·»åŠ å¯é€‰å­—æ®µ
                if 'confidence' in item:
                    formatted_item['confidence'] = float(item['confidence'])
                
                formatted_data.append(formatted_item)
        
        return formatted_data
    
    def format_generation_data(self, data: List[Dict], 
                             prompt_field='prompt', 
                             response_field='response',
                             context_field=None):
        """æ ¼å¼åŒ–ç”Ÿæˆæ•°æ®
        
        é€‚ç”¨äºæ–‡æœ¬æ‘˜è¦ã€ç¿»è¯‘ã€åˆ›æ„å†™ä½œç­‰ç”Ÿæˆä»»åŠ¡ã€‚
        """
        formatted_data = []
        
        for item in data:
            if prompt_field in item and response_field in item:
                prompt = str(item[prompt_field]).strip()
                response = str(item[response_field]).strip()
                
                formatted_item = {
                    'prompt': prompt,
                    'response': response,
                    'task_type': 'generation'
                }
                
                # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if context_field and context_field in item:
                    context = str(item[context_field]).strip()
                    formatted_item['context'] = context
                    # æ›´æ–°promptä»¥åŒ…å«ä¸Šä¸‹æ–‡
                    formatted_item['prompt'] = f"Context: {context}\n\nPrompt: {prompt}"
                
                # æ„å»ºå®Œæ•´çš„è®­ç»ƒæ–‡æœ¬
                if 'context' in formatted_item:
                    formatted_item['text'] = f"Context: {formatted_item['context']}\nPrompt: {prompt}\nResponse: {response}"
                else:
                    formatted_item['text'] = f"Prompt: {prompt}\nResponse: {response}"
                
                formatted_data.append(formatted_item)
        
        return formatted_data
    
    def format_instruction_data(self, data: List[Dict]):
        """æ ¼å¼åŒ–æŒ‡ä»¤å¾®è°ƒæ•°æ®
        
        å°†æ•°æ®è½¬æ¢ä¸ºæŒ‡ä»¤å¾®è°ƒçš„æ ‡å‡†æ ¼å¼ã€‚è¿™ç§æ ¼å¼æœ‰åŠ©äºæ¨¡å‹å­¦ä¹ éµå¾ªæŒ‡ä»¤ã€‚
        """
        formatted_data = []
        
        for item in data:
            if 'instruction' in item and 'output' in item:
                instruction = str(item['instruction']).strip()
                output = str(item['output']).strip()
                input_text = str(item.get('input', '')).strip()
                
                # æ„å»ºæŒ‡ä»¤æ ¼å¼
                if input_text:
                    formatted_text = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
                else:
                    formatted_text = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
                
                formatted_item = {
                    'text': formatted_text,
                    'instruction': instruction,
                    'input': input_text,
                    'output': output,
                    'task_type': 'instruction_tuning'
                }
                
                # æ·»åŠ ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæœ‰ï¼‰
                if 'system_prompt' in item:
                    formatted_item['system_prompt'] = str(item['system_prompt']).strip()
                
                formatted_data.append(formatted_item)
        
        return formatted_data
    
    def format_qa_data(self, data: List[Dict], qa_type='extractive'):
        """æ ¼å¼åŒ–é—®ç­”æ•°æ®
        
        Args:
            data: åŸå§‹æ•°æ®
            qa_type: é—®ç­”ç±»å‹ ('extractive', 'generative', 'open_domain')
        """
        formatted_data = []
        
        for item in data:
            if 'question' in item and 'answer' in item:
                question = str(item['question']).strip()
                answer = str(item['answer']).strip()
                context = str(item.get('context', '')).strip()
                
                formatted_item = {
                    'question': question,
                    'answer': answer,
                    'context': context,
                    'qa_type': qa_type,
                    'task_type': 'qa'
                }
                
                # æ ¹æ®é—®ç­”ç±»å‹æ„å»ºä¸åŒæ ¼å¼
                if qa_type == 'extractive' and context:
                    # æŠ½å–å¼é—®ç­”ï¼šç­”æ¡ˆåº”è¯¥åœ¨ä¸Šä¸‹æ–‡ä¸­
                    formatted_text = f"Context: {context}\n\nQuestion: {question}\nAnswer: {answer}"
                    
                    # å°è¯•æ‰¾åˆ°ç­”æ¡ˆåœ¨ä¸Šä¸‹æ–‡ä¸­çš„ä½ç½®
                    answer_start = context.find(answer)
                    if answer_start != -1:
                        formatted_item['answer_start'] = answer_start
                        formatted_item['answer_end'] = answer_start + len(answer)
                
                elif qa_type == 'generative':
                    # ç”Ÿæˆå¼é—®ç­”ï¼šåŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
                    if context:
                        formatted_text = f"Based on the following context, answer the question.\n\nContext: {context}\n\nQuestion: {question}\nAnswer: {answer}"
                    else:
                        formatted_text = f"Question: {question}\nAnswer: {answer}"
                
                else:  # open_domain
                    # å¼€æ”¾åŸŸé—®ç­”ï¼šä¸ä¾èµ–ç‰¹å®šä¸Šä¸‹æ–‡
                    formatted_text = f"Question: {question}\nAnswer: {answer}"
                
                formatted_item['text'] = formatted_text
                formatted_data.append(formatted_item)
        
        return formatted_data
    
    def format_dialogue_data(self, data: List[Dict], dialogue_format='alternating'):
        """æ ¼å¼åŒ–å¯¹è¯æ•°æ®
        
        Args:
            dialogue_format: å¯¹è¯æ ¼å¼ ('alternating', 'role_based', 'context_response')
        """
        formatted_data = []
        
        for item in data:
            if 'conversation' in item:
                conversation = item['conversation']
                
                if isinstance(conversation, list):
                    # åˆ—è¡¨æ ¼å¼ï¼š[{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
                    if dialogue_format == 'alternating':
                        # äº¤æ›¿æ ¼å¼ï¼šUser: ... Assistant: ...
                        formatted_conversation = ""
                        for turn in conversation:
                            role = turn.get('role', 'unknown')
                            content = turn.get('content', '')
                            formatted_conversation += f"{role.title()}: {content}\n"
                    
                    elif dialogue_format == 'role_based':
                        # åŸºäºè§’è‰²çš„æ ¼å¼
                        formatted_conversation = ""
                        for turn in conversation:
                            role = turn.get('role', 'unknown')
                            content = turn.get('content', '')
                            formatted_conversation += f"[{role.upper()}] {content}\n"
                    
                    elif dialogue_format == 'context_response':
                        # ä¸Šä¸‹æ–‡-å›å¤æ ¼å¼ï¼ˆç”¨äºè®­ç»ƒï¼‰
                        if len(conversation) >= 2:
                            context_turns = conversation[:-1]
                            response_turn = conversation[-1]
                            
                            context = ""
                            for turn in context_turns:
                                role = turn.get('role', 'unknown')
                                content = turn.get('content', '')
                                context += f"{role}: {content}\n"
                            
                            response = response_turn.get('content', '')
                            formatted_conversation = f"Context:\n{context.strip()}\n\nResponse: {response}"
                        else:
                            continue
                
                elif isinstance(conversation, str):
                    # å­—ç¬¦ä¸²æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨
                    formatted_conversation = conversation
                else:
                    continue
                
                formatted_item = {
                    'conversation': formatted_conversation.strip(),
                    'text': formatted_conversation.strip(),
                    'dialogue_format': dialogue_format,
                    'task_type': 'dialogue'
                }
                    # æ·»åŠ è¯´è¯è€…ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if 'speaker_info' in item:
                    formatted_item['speaker_info'] = item['speaker_info']
                
                formatted_data.append(formatted_item)
        
        return formatted_data
    
    def format_ner_data(self, data: List[Dict], ner_format='bio'):
        """æ ¼å¼åŒ–å‘½åå®ä½“è¯†åˆ«æ•°æ®
        
        Args:
            ner_format: NERæ ‡æ³¨æ ¼å¼ ('bio', 'bilou', 'json')
        """
        formatted_data = []
        
        for item in data:
            if 'text' in item and 'entities' in item:
                text = str(item['text']).strip()
                entities = item['entities']
                
                formatted_item = {
                    'text': text,
                    'entities': entities,
                    'ner_format': ner_format,
                    'task_type': 'ner'
                }
                
                if ner_format == 'bio':
                    # BIOæ ¼å¼ï¼šB-å®ä½“ç±»å‹, I-å®ä½“ç±»å‹, O
                    tokens = text.split()
                    bio_tags = ['O'] * len(tokens)
                    
                    for entity in entities:
                        entity_text = entity.get('text', '')
                        entity_type = entity.get('type', 'ENTITY')
                        start = entity.get('start', 0)
                        end = entity.get('end', 0)
                        
                        # ç®€åŒ–å¤„ç†ï¼šåŸºäºæ–‡æœ¬åŒ¹é…
                        entity_tokens = entity_text.split()
                        for i, token in enumerate(tokens):
                            if token in entity_tokens:
                                if i == 0 or bio_tags[i-1] == 'O':
                                    bio_tags[i] = f'B-{entity_type}'
                                else:
                                    bio_tags[i] = f'I-{entity_type}'
                    
                    formatted_item['bio_tags'] = bio_tags
                    formatted_item['tokens'] = tokens
                
                elif ner_format == 'json':
                    # JSONæ ¼å¼ï¼šä¿æŒåŸæœ‰çš„å®ä½“æ ¼å¼
                    formatted_item['entities_json'] = entities
                
                formatted_data.append(formatted_item)
        
        return formatted_data
    
    def auto_format(self, data: List[Dict], task_type: str, **kwargs):
        """è‡ªåŠ¨æ ¼å¼åŒ–æ•°æ®
        
        æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ ¼å¼åŒ–æ–¹æ³•ã€‚
        """
        if task_type == 'classification':
            return self.format_classification_data(data, **kwargs)
        elif task_type == 'multi_label_classification':
            return self.format_classification_data(data, multi_label=True, **kwargs)
        elif task_type == 'generation':
            return self.format_generation_data(data, **kwargs)
        elif task_type == 'instruction_tuning':
            return self.format_instruction_data(data)
        elif task_type == 'qa':
            return self.format_qa_data(data, **kwargs)
        elif task_type == 'dialogue':
            return self.format_dialogue_data(data, **kwargs)
        elif task_type == 'ner':
            return self.format_ner_data(data, **kwargs)
        else:
            raise ValueError(f"Unsupported task type: {task_type}")
    
    def validate_format(self, data: List[Dict], task_type: str):
        """éªŒè¯æ•°æ®æ ¼å¼
        
        æ£€æŸ¥æ ¼å¼åŒ–åçš„æ•°æ®æ˜¯å¦ç¬¦åˆä»»åŠ¡è¦æ±‚ã€‚
        """
        if task_type not in self.task_formats:
            return False, f"Unknown task type: {task_type}"
        
        task_format = self.task_formats[task_type]
        issues = []
        warnings = []
        
        for i, item in enumerate(data):
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            for field in task_format.required_fields:
                if field not in item:
                    issues.append(f"Item {i}: Missing required field '{field}'")
                elif not item[field] or str(item[field]).strip() == '':
                    issues.append(f"Item {i}: Empty required field '{field}'")
            
            # æ£€æŸ¥æ•°æ®ç±»å‹å’Œæ ¼å¼
            if task_type == 'classification' and 'label' in item:
                if not isinstance(item['label'], str):
                    warnings.append(f"Item {i}: Label should be string, got {type(item['label'])}")
            
            elif task_type == 'multi_label_classification' and 'labels' in item:
                if not isinstance(item['labels'], list):
                    issues.append(f"Item {i}: Labels should be list, got {type(item['labels'])}")
            
            elif task_type == 'qa' and 'answer' in item and 'context' in item:
                # å¯¹äºæŠ½å–å¼é—®ç­”ï¼Œæ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
                if item.get('qa_type') == 'extractive' and item['context']:
                    if item['answer'] not in item['context']:
                        warnings.append(f"Item {i}: Answer not found in context (extractive QA)")
        
        is_valid = len(issues) == 0
        return is_valid, {'issues': issues, 'warnings': warnings}
    
    def export_formatted_data(self, data: List[Dict], output_path: str, format_type='jsonl'):
        """å¯¼å‡ºæ ¼å¼åŒ–æ•°æ®
        
        æ”¯æŒå¤šç§å¯¼å‡ºæ ¼å¼ï¼Œæ–¹ä¾¿ä¸åŒæ¡†æ¶ä½¿ç”¨ã€‚
        """
        if format_type == 'jsonl':
            with open(output_path, 'w', encoding='utf-8') as f:
                for item in data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        elif format_type == 'json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        
        elif format_type == 'csv':
            df = pd.DataFrame(data)
            df.to_csv(output_path, index=False, encoding='utf-8')
        
        elif format_type == 'huggingface':
            # Hugging Face datasetsæ ¼å¼
            from datasets import Dataset
            dataset = Dataset.from_list(data)
            dataset.save_to_disk(output_path)
        
        print(f"æ•°æ®å·²å¯¼å‡ºåˆ°: {output_path} (æ ¼å¼: {format_type})")
    
    def convert_between_formats(self, data: List[Dict], source_format: str, target_format: str):
        """æ ¼å¼è½¬æ¢
        
        åœ¨ä¸åŒä»»åŠ¡æ ¼å¼ä¹‹é—´è¿›è¡Œè½¬æ¢ã€‚
        """
        if source_format == target_format:
            return data
        
        converted_data = []
        
        # ç¤ºä¾‹ï¼šä»åˆ†ç±»æ ¼å¼è½¬æ¢ä¸ºæŒ‡ä»¤æ ¼å¼
        if source_format == 'classification' and target_format == 'instruction_tuning':
            for item in data:
                if 'text' in item and 'label' in item:
                    converted_item = {
                        'instruction': 'è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„ç±»åˆ«',
                        'input': item['text'],
                        'output': f'è¿™ä¸ªæ–‡æœ¬å±äºï¼š{item["label"]}',
                        'task_type': 'instruction_tuning'
                    }
                    converted_data.append(converted_item)
        
        # ç¤ºä¾‹ï¼šä»é—®ç­”æ ¼å¼è½¬æ¢ä¸ºæŒ‡ä»¤æ ¼å¼
        elif source_format == 'qa' and target_format == 'instruction_tuning':
            for item in data:
                if 'question' in item and 'answer' in item:
                    instruction = 'è¯·å›ç­”ä»¥ä¸‹é—®é¢˜'
                    if 'context' in item and item['context']:
                        instruction += 'ï¼ŒåŸºäºç»™å®šçš„ä¸Šä¸‹æ–‡'
                        input_text = f"ä¸Šä¸‹æ–‡ï¼š{item['context']}\n\né—®é¢˜ï¼š{item['question']}"
                    else:
                        input_text = item['question']
                    
                    converted_item = {
                        'instruction': instruction,
                        'input': input_text,
                        'output': item['answer'],
                        'task_type': 'instruction_tuning'
                    }
                    converted_data.append(converted_item)
        
        else:
            raise ValueError(f"Conversion from {source_format} to {target_format} not supported")
        
        return converted_data

# ä½¿ç”¨ç¤ºä¾‹ï¼šå¤šç§ä»»åŠ¡çš„æ•°æ®æ ¼å¼åŒ–
print("=== æ•°æ®æ ¼å¼åŒ–ç¤ºä¾‹ ===")

formatter = DataFormatter()

# ç¤ºä¾‹1ï¼šæƒ…æ„Ÿåˆ†ææ•°æ®æ ¼å¼åŒ–
print("1. æƒ…æ„Ÿåˆ†ææ•°æ®æ ¼å¼åŒ–")
sentiment_data = [
    {'review': 'è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œæˆ‘å¾ˆæ»¡æ„', 'sentiment': 'positive'},
    {'review': 'æœåŠ¡æ€åº¦ä¸é”™ï¼Œå€¼å¾—æ¨è', 'sentiment': 'positive'},
    {'review': 'ä»·æ ¼å¤ªè´µäº†ï¼Œæ€§ä»·æ¯”ä¸é«˜', 'sentiment': 'negative'}
]

formatted_sentiment = formatter.format_classification_data(
    sentiment_data, 
    text_field='review', 
    label_field='sentiment'
)

print("æ ¼å¼åŒ–åçš„æƒ…æ„Ÿåˆ†ææ•°æ®:")
for item in formatted_sentiment[:2]:
    print(f"  æ–‡æœ¬: {item['text']}")
    print(f"  æ ‡ç­¾: {item['label']}")
    print()

# ç¤ºä¾‹2ï¼šæŒ‡ä»¤å¾®è°ƒæ•°æ®æ ¼å¼åŒ–
print("2. æŒ‡ä»¤å¾®è°ƒæ•°æ®æ ¼å¼åŒ–")
instruction_data = [
    {
        'instruction': 'è¯·åˆ†æè¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘',
        'input': 'è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œæˆ‘å¾ˆæ»¡æ„',
        'output': 'è¿™æ®µæ–‡æœ¬è¡¨è¾¾äº†ç§¯ææ­£é¢çš„æƒ…æ„Ÿã€‚ç”¨æˆ·å¯¹äº§å“è¡¨ç¤ºæ»¡æ„ï¼Œæ•´ä½“æƒ…æ„Ÿå€¾å‘ä¸ºæ­£é¢ã€‚'
    },
    {
        'instruction': 'ç¿»è¯‘ä»¥ä¸‹è‹±æ–‡å¥å­',
        'input': 'Hello, how are you?',
        'output': 'ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ'
    }
]

formatted_instruction = formatter.format_instruction_data(instruction_data)
print("æ ¼å¼åŒ–åçš„æŒ‡ä»¤æ•°æ®:")
print(formatted_instruction[0]['text'])
print()

# ç¤ºä¾‹3ï¼šé—®ç­”æ•°æ®æ ¼å¼åŒ–
print("3. é—®ç­”æ•°æ®æ ¼å¼åŒ–")
qa_data = [
    {
        'question': 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ',
        'answer': 'äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚',
        'context': 'äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿï¼Œå¦‚å­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥å’Œå†³ç­–ã€‚'
    }
]

formatted_qa = formatter.format_qa_data(qa_data, qa_type='generative')
print("æ ¼å¼åŒ–åçš„é—®ç­”æ•°æ®:")
print(formatted_qa[0]['text'])
print()

# ç¤ºä¾‹4ï¼šå¯¹è¯æ•°æ®æ ¼å¼åŒ–
print("4. å¯¹è¯æ•°æ®æ ¼å¼åŒ–")
dialogue_data = [
    {
        'conversation': [
            {'role': 'user', 'content': 'ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹è¿™ä¸ªäº§å“'},
            {'role': 'assistant', 'content': 'æ‚¨å¥½ï¼å¾ˆé«˜å…´ä¸ºæ‚¨ä»‹ç»æˆ‘ä»¬çš„äº§å“ã€‚è¯·é—®æ‚¨æƒ³äº†è§£å“ªæ–¹é¢çš„ä¿¡æ¯ï¼Ÿ'},
            {'role': 'user', 'content': 'ä¸»è¦æƒ³çŸ¥é“ä»·æ ¼å’ŒåŠŸèƒ½'},
            {'role': 'assistant', 'content': 'å¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨è¯¦ç»†ä»‹ç»ä»·æ ¼å’Œä¸»è¦åŠŸèƒ½...'}
        ]
    }
]

formatted_dialogue = formatter.format_dialogue_data(dialogue_data, dialogue_format='alternating')
print("æ ¼å¼åŒ–åçš„å¯¹è¯æ•°æ®:")
print(formatted_dialogue[0]['text'])
print()

# æ ¼å¼éªŒè¯
print("5. æ ¼å¼éªŒè¯")
is_valid, validation_result = formatter.validate_format(formatted_sentiment, 'classification')
print(f"æƒ…æ„Ÿåˆ†ææ•°æ®æ ¼å¼éªŒè¯: {'é€šè¿‡' if is_valid else 'å¤±è´¥'}")
if validation_result['issues']:
    print("å‘ç°é—®é¢˜:")
    for issue in validation_result['issues']:
        print(f"  - {issue}")

# æ ¼å¼è½¬æ¢ç¤ºä¾‹
print("\n6. æ ¼å¼è½¬æ¢ç¤ºä¾‹")
converted_data = formatter.convert_between_formats(
    formatted_sentiment, 
    'classification', 
    'instruction_tuning'
)
print("åˆ†ç±»æ ¼å¼è½¬æ¢ä¸ºæŒ‡ä»¤æ ¼å¼:")
print(converted_data[0]['text'])
```

### 3.3 æ•°æ®å¢å¼ºï¼šè§£å†³å°æ ·æœ¬é—®é¢˜

å½“è®­ç»ƒæ•°æ®ä¸è¶³æ—¶ï¼Œæ•°æ®å¢å¼ºæŠ€æœ¯å¯ä»¥é€šè¿‡ç”Ÿæˆæ–°çš„è®­ç»ƒæ ·æœ¬æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚æ•°æ®å¢å¼ºçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨ä¿æŒè¯­ä¹‰ä¸å˜çš„å‰æä¸‹å¢åŠ æ•°æ®çš„å¤šæ ·æ€§ï¼Œè®©æ¨¡å‹è§åˆ°æ›´å¤šçš„è¡¨è¾¾æ–¹å¼ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

**æ•°æ®å¢å¼ºçš„é‡è¦æ€§**

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé«˜è´¨é‡çš„æ ‡æ³¨æ•°æ®å¾€å¾€ç¨€ç¼ºä¸”æ˜‚è´µã€‚ç‰¹åˆ«æ˜¯åœ¨ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ã€é‡‘èï¼‰ï¼Œè·å–è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®å¯èƒ½éœ€è¦æ•°æœˆæ—¶é—´å’Œå¤§é‡æˆæœ¬ã€‚æ•°æ®å¢å¼ºæŠ€æœ¯å¯ä»¥åœ¨æœ‰é™çš„åŸå§‹æ•°æ®åŸºç¡€ä¸Šï¼Œç”Ÿæˆæ›´å¤šçš„è®­ç»ƒæ ·æœ¬ï¼Œæœ‰æ•ˆç¼“è§£æ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚

**æ•°æ®å¢å¼ºçš„æŒ‘æˆ˜**

ç„¶è€Œï¼Œæ–‡æœ¬æ•°æ®å¢å¼ºæ¯”å›¾åƒæ•°æ®å¢å¼ºæ›´åŠ å›°éš¾ï¼Œå› ä¸ºï¼š
1. **è¯­ä¹‰ä¿æŒ**ï¼šæ–‡æœ¬çš„å¾®å°æ”¹åŠ¨å¯èƒ½å¯¼è‡´è¯­ä¹‰å‘ç”Ÿå˜åŒ–
2. **è¯­æ³•æ­£ç¡®æ€§**ï¼šå¢å¼ºåçš„æ–‡æœ¬å¿…é¡»ä¿æŒè¯­æ³•æ­£ç¡®
3. **ä¸Šä¸‹æ–‡ä¸€è‡´æ€§**ï¼šè¯æ±‡æ›¿æ¢å¿…é¡»è€ƒè™‘ä¸Šä¸‹æ–‡ç¯å¢ƒ
4. **æ ‡ç­¾ä¸€è‡´æ€§**ï¼šå¢å¼ºåçš„æ–‡æœ¬æ ‡ç­¾åº”è¯¥ä¿æŒä¸å˜

**æ•°æ®å¢å¼ºæŠ€æœ¯è¯¦è§£**

**1. åŒä¹‰è¯æ›¿æ¢ï¼ˆSynonym Replacementï¼‰**

è¿™æ˜¯æœ€å¸¸ç”¨çš„æ–‡æœ¬å¢å¼ºæŠ€æœ¯ä¹‹ä¸€ã€‚é€šè¿‡å°†æ–‡æœ¬ä¸­çš„è¯æ±‡æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œå¯ä»¥ç”Ÿæˆè¯­ä¹‰ç›¸ä¼¼ä½†è¡¨è¾¾ä¸åŒçš„æ–‡æœ¬ã€‚

**å®é™…æ¡ˆä¾‹ï¼šç”µå•†è¯„è®ºå¢å¼º**
åŸæ–‡ï¼š"è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½"
å¢å¼ºåï¼š"è¿™ä¸ªå•†å“å“è´¨å¾ˆæ£’"ã€"è¿™ä¸ªç‰©å“è´¨é‡å¾ˆä¼˜ç§€"

éœ€è¦æ³¨æ„çš„é—®é¢˜ï¼š
- åŒä¹‰è¯çš„é€‰æ‹©è¦è€ƒè™‘ä¸Šä¸‹æ–‡
- é¿å…æ›¿æ¢å…³é”®è¯ï¼ˆå¦‚å“ç‰Œåã€ä¸“ä¸šæœ¯è¯­ï¼‰
- ä¿æŒè¯­è¨€é£æ ¼çš„ä¸€è‡´æ€§

**2. éšæœºæ’å…¥ï¼ˆRandom Insertionï¼‰**

åœ¨æ–‡æœ¬ä¸­éšæœºä½ç½®æ’å…¥åŒä¹‰è¯ï¼Œå¢åŠ æ–‡æœ¬çš„ä¸°å¯Œæ€§ã€‚

**3. éšæœºäº¤æ¢ï¼ˆRandom Swapï¼‰**

éšæœºäº¤æ¢æ–‡æœ¬ä¸­ä¸¤ä¸ªè¯çš„ä½ç½®ï¼Œåœ¨ä¸æ”¹å˜æ•´ä½“è¯­ä¹‰çš„å‰æä¸‹å¢åŠ è¡¨è¾¾çš„å¤šæ ·æ€§ã€‚

**4. éšæœºåˆ é™¤ï¼ˆRandom Deletionï¼‰**

éšæœºåˆ é™¤æ–‡æœ¬ä¸­çš„éƒ¨åˆ†è¯æ±‡ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯ä¸­çš„ä¸å®Œæ•´è¡¨è¾¾ã€‚

**5. å›è¯‘æŠ€æœ¯ï¼ˆBack Translationï¼‰**

å°†æ–‡æœ¬ç¿»è¯‘æˆå…¶ä»–è¯­è¨€å†ç¿»è¯‘å›æ¥ï¼Œå¯ä»¥äº§ç”Ÿè¯­ä¹‰ç›¸ä¼¼ä½†è¡¨è¾¾ä¸åŒçš„æ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚åˆç”Ÿæˆè‡ªç„¶ã€æµç•…çš„å¢å¼ºæ–‡æœ¬ã€‚

**å®é™…æ¡ˆä¾‹ï¼šå®¢æœå¯¹è¯å¢å¼º**
åŸæ–‡ï¼š"æˆ‘æƒ³é€€è´§"
è‹±æ–‡ç¿»è¯‘ï¼š"I want to return the product"
å›è¯‘ç»“æœï¼š"æˆ‘æƒ³è¦é€€æ¢å•†å“"

**6. ä¸Šä¸‹æ–‡æ‰©å……ï¼ˆContext Augmentationï¼‰**

ä¸ºåŸæœ‰æ–‡æœ¬æ·»åŠ ç›¸å…³çš„èƒŒæ™¯ä¿¡æ¯æˆ–ä¸Šä¸‹æ–‡ï¼Œå¢åŠ æ•°æ®çš„ä¸°å¯Œæ€§ã€‚

```python
import random
import synonyms
from googletrans import Translator
import nltk
from nltk.corpus import wordnet
import jieba.posseg as pseg
import numpy as np
from collections import defaultdict

class DataAugmenter:
    """æ•°æ®å¢å¼ºå™¨
    
    æä¾›å¤šç§æ–‡æœ¬æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¸®åŠ©è§£å†³å°æ ·æœ¬é—®é¢˜ã€‚
    æ”¯æŒä¸­è‹±æ–‡æ–‡æœ¬ï¼Œæä¾›çµæ´»çš„å¢å¼ºç­–ç•¥é…ç½®ã€‚
    """
    
    def __init__(self, language='zh'):
        self.language = language
        self.translator = Translator()
        
        # ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
        try:
            nltk.data.find('corpora/wordnet')
        except LookupError:
            nltk.download('wordnet')
        
        # åœç”¨è¯åˆ—è¡¨ï¼ˆä¸è¿›è¡Œæ›¿æ¢çš„è¯æ±‡ï¼‰
        self.stop_words = {
            'zh': ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'],
            'en': ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should']
        }
    
    def synonym_replacement(self, text, n=1, preserve_pos=True):
        """åŒä¹‰è¯æ›¿æ¢
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            n: æ›¿æ¢çš„è¯æ±‡æ•°é‡
            preserve_pos: æ˜¯å¦ä¿æŒè¯æ€§ä¸€è‡´
        """
        if self.language == 'zh':
            return self._chinese_synonym_replacement(text, n, preserve_pos)
        else:
            return self._english_synonym_replacement(text, n, preserve_pos)
    
    def _chinese_synonym_replacement(self, text, n, preserve_pos):
        """ä¸­æ–‡åŒä¹‰è¯æ›¿æ¢"""
        # ä½¿ç”¨jiebaè¿›è¡Œè¯æ€§æ ‡æ³¨
        if preserve_pos:
            words_pos = list(pseg.cut(text))
            words = [word for word, pos in words_pos]
            pos_tags = [pos for word, pos in words_pos]
        else:
            words = list(jieba.cut(text))
            pos_tags = None
        
        new_words = words.copy()
        
        # é€‰æ‹©å¯æ›¿æ¢çš„è¯æ±‡ï¼ˆæ’é™¤åœç”¨è¯å’Œå•å­—ç¬¦è¯ï¼‰
        replaceable_words = []
        for i, word in enumerate(words):
            if (len(word) > 1 and 
                word not in self.stop_words['zh'] and
                not word.isdigit() and
                not self._is_punctuation(word)):
                replaceable_words.append((i, word))
        
        # éšæœºé€‰æ‹©è¦æ›¿æ¢çš„è¯æ±‡
        random.shuffle(replaceable_words)
        num_replaced = 0
        
        for idx, word in replaceable_words:
            if num_replaced >= n:
                break
            
            # è·å–åŒä¹‰è¯
            synonyms_list = synonyms.nearby(word)[0]
            if len(synonyms_list) >= 1:
                # è¿‡æ»¤åŒä¹‰è¯ï¼ˆä¿æŒè¯æ€§ä¸€è‡´ï¼‰
                valid_synonyms = []
                for synonym in synonyms_list:
                    if synonym != word and len(synonym) > 0:
                        if preserve_pos and pos_tags:
                            # ç®€å•çš„è¯æ€§æ£€æŸ¥ï¼ˆå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
                            if self._check_pos_compatibility(pos_tags[idx], synonym):
                                valid_synonyms.append(synonym)
                        else:
                            valid_synonyms.append(synonym)
                
                if valid_synonyms:
                    synonym = random.choice(valid_synonyms)
                    new_words[idx] = synonym
                    num_replaced += 1
        
        return ''.join(new_words)
    
    def _english_synonym_replacement(self, text, n, preserve_pos):
        """è‹±æ–‡åŒä¹‰è¯æ›¿æ¢"""
        words = text.split()
        new_words = words.copy()
        
        # é€‰æ‹©å¯æ›¿æ¢çš„è¯æ±‡
        replaceable_words = []
        for i, word in enumerate(words):
            if (len(word) > 2 and 
                word.lower() not in self.stop_words['en'] and
                not word.isdigit() and
                word.isalpha()):
                replaceable_words.append((i, word))
        
        random.shuffle(replaceable_words)
        num_replaced = 0
        
        for idx, word in replaceable_words:
            if num_replaced >= n:
                break
            
            synonyms_list = []
            for syn in wordnet.synsets(word):
                for lemma in syn.lemmas():
                    synonym = lemma.name().replace('_', ' ')
                    if synonym != word.lower() and synonym.isalpha():
                        synonyms_list.append(synonym)
            
            if synonyms_list:
                synonym = random.choice(synonyms_list)
                # ä¿æŒåŸè¯çš„å¤§å°å†™æ ¼å¼
                if word.isupper():
                    synonym = synonym.upper()
                elif word.istitle():
                    synonym = synonym.title()
                
                new_words[idx] = synonym
                num_replaced += 1
        
        return ' '.join(new_words)
    
    def random_insertion(self, text, n=1):
        """éšæœºæ’å…¥
        
        åœ¨æ–‡æœ¬ä¸­éšæœºä½ç½®æ’å…¥åŒä¹‰è¯ï¼Œå¢åŠ æ–‡æœ¬çš„ä¸°å¯Œæ€§ã€‚
        """
        words = text.split() if self.language == 'en' else list(jieba.cut(text))
        
        for _ in range(n):
            # é€‰æ‹©ä¸€ä¸ªè¯æ¥ç”ŸæˆåŒä¹‰è¯
            if len(words) == 0:
                break
                
            random_word = random.choice(words)
            synonym = self._get_random_synonym(random_word)
            
            if synonym and synonym != random_word:
                # éšæœºé€‰æ‹©æ’å…¥ä½ç½®
                random_idx = random.randint(0, len(words))
                words.insert(random_idx, synonym)
        
        return ' '.join(words) if self.language == 'en' else ''.join(words)
    
    def random_swap(self, text, n=1):
        """éšæœºäº¤æ¢
        
        éšæœºäº¤æ¢æ–‡æœ¬ä¸­ä¸¤ä¸ªè¯çš„ä½ç½®ã€‚
        """
        words = text.split() if self.language == 'en' else list(jieba.cut(text))
        
        for _ in range(n):
            if len(words) >= 2:
                # éšæœºé€‰æ‹©ä¸¤ä¸ªä¸åŒçš„ä½ç½®
                idx1, idx2 = random.sample(range(len(words)), 2)
                words[idx1], words[idx2] = words[idx2], words[idx1]
        
        return ' '.join(words) if self.language == 'en' else ''.join(words)
    
    def random_deletion(self, text, p=0.1):
        """éšæœºåˆ é™¤
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            p: åˆ é™¤æ¦‚ç‡
        """
        words = text.split() if self.language == 'en' else list(jieba.cut(text))
        
        if len(words) == 1:
            return text
        
        new_words = []
        for word in words:
            # ä¿æŠ¤é‡è¦è¯æ±‡ä¸è¢«åˆ é™¤
            if (word not in self.stop_words.get(self.language, []) and
                not self._is_important_word(word) and
                random.uniform(0, 1) > p):
                new_words.append(word)
            elif word in self.stop_words.get(self.language, []):
                # åœç”¨è¯æœ‰æ›´é«˜çš„ä¿ç•™æ¦‚ç‡
                if random.uniform(0, 1) > p * 0.5:
                    new_words.append(word)
            else:
                new_words.append(word)
        
        # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€ä¸ªè¯
        if len(new_words) == 0:
            new_words = [random.choice(words)]
        
        return ' '.join(new_words) if self.language == 'en' else ''.join(new_words)
    
    def back_translation(self, text, intermediate_languages=['en', 'ja', 'ko']):
        """å›è¯‘å¢å¼º
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            intermediate_languages: ä¸­é—´è¯­è¨€åˆ—è¡¨
        """
        augmented_texts = []
        
        for intermediate_lang in intermediate_languages:
            try:
                # ç¿»è¯‘åˆ°ä¸­é—´è¯­è¨€
                intermediate = self.translator.translate(
                    text, 
                    dest=intermediate_lang
                ).text
                
                # ç¿»è¯‘å›åŸè¯­è¨€
                back_translated = self.translator.translate(
                    intermediate, 
                    dest=self.language
                ).text
                
                # æ£€æŸ¥å›è¯‘è´¨é‡
                if (back_translated != text and 
                    len(back_translated.strip()) > 0 and
                    self._is_valid_translation(text, back_translated)):
                    augmented_texts.append(back_translated)
                
            except Exception as e:
                print(f"å›è¯‘å¤±è´¥ ({intermediate_lang}): {e}")
                continue
        
        return augmented_texts
    
    def contextual_augmentation(self, text, context_templates=None):
        """ä¸Šä¸‹æ–‡å¢å¼º
        
        ä¸ºæ–‡æœ¬æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¢åŠ æ•°æ®çš„ä¸°å¯Œæ€§ã€‚
        """
        if context_templates is None:
            if self.language == 'zh':
                context_templates = [
                    "æ ¹æ®æˆ‘çš„ç»éªŒï¼Œ{text}",
                    "æˆ‘è§‰å¾—{text}",
                    "æ€»çš„æ¥è¯´ï¼Œ{text}",
                    "ä»æˆ‘çš„è§’åº¦çœ‹ï¼Œ{text}",
                    "ä¸ªäººè®¤ä¸º{text}"
                ]
            else:
                context_templates = [
                    "In my opinion, {text}",
                    "I believe that {text}",
                    "From my experience, {text}",
                    "Generally speaking, {text}",
                    "It seems to me that {text}"
                ]
        
        augmented_texts = []
        for template in context_templates:
            augmented_text = template.format(text=text)
            augmented_texts.append(augmented_text)
        
        return augmented_texts
    
    def paraphrase_generation(self, text, methods=['synonym', 'swap', 'deletion'], num_variants=3):
        """ç”Ÿæˆé‡Šä¹‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            methods: ä½¿ç”¨çš„å¢å¼ºæ–¹æ³•åˆ—è¡¨
            num_variants: æ¯ç§æ–¹æ³•ç”Ÿæˆçš„å˜ä½“æ•°é‡
        """
        paraphrases = []
        
        for method in methods:
            for _ in range(num_variants):
                try:
                    if method == 'synonym':
                        variant = self.synonym_replacement(text, n=random.randint(1, 3))
                    elif method == 'swap':
                        variant = self.random_swap(text, n=random.randint(1, 2))
                    elif method == 'deletion':
                        variant = self.random_deletion(text, p=random.uniform(0.05, 0.15))
                    elif method == 'insertion':
                        variant = self.random_insertion(text, n=random.randint(1, 2))
                    elif method == 'back_translation':
                        variants = self.back_translation(text)
                        paraphrases.extend(variants[:num_variants])
                        continue
                    elif method == 'contextual':
                        variants = self.contextual_augmentation(text)
                        paraphrases.extend(variants[:num_variants])
                        continue
                    else:
                        continue
                    
                    # è´¨é‡æ£€æŸ¥
                    if (variant != text and 
                        len(variant.strip()) > 0 and
                        self._is_valid_paraphrase(text, variant)):
                        paraphrases.append(variant)
                        
                except Exception as e:
                    print(f"ç”Ÿæˆé‡Šä¹‰å¤±è´¥ ({method}): {e}")
                    continue
        
        # å»é‡
        unique_paraphrases = []
        for p in paraphrases:
            if p not in unique_paraphrases and p != text:
                unique_paraphrases.append(p)
        
        return unique_paraphrases
    
    def augment_dataset(self, data, augment_ratio=0.5, methods=['synonym', 'swap'], 
                       preserve_balance=True, min_samples_per_class=None):
        """å¢å¼ºæ•´ä¸ªæ•°æ®é›†
        
        Args:
            data: åŸå§‹æ•°æ®åˆ—è¡¨
            augment_ratio: å¢å¼ºæ¯”ä¾‹
            methods: ä½¿ç”¨çš„å¢å¼ºæ–¹æ³•
            preserve_balance: æ˜¯å¦ä¿æŒç±»åˆ«å¹³è¡¡
            min_samples_per_class: æ¯ä¸ªç±»åˆ«çš„æœ€å°æ ·æœ¬æ•°
        """
        augmented_data = data.copy()
        
        if preserve_balance and 'label' in data[0]:
            # æŒ‰ç±»åˆ«åˆ†ç»„
            class_data = defaultdict(list)
            for item in data:
                class_data[item['label']].append(item)
            
            # ä¸ºæ¯ä¸ªç±»åˆ«å•ç‹¬å¢å¼º
            for label, items in class_data.items():
                target_count = len(items)
                if min_samples_per_class:
                    target_count = max(target_count, min_samples_per_class)
                
                num_to_augment = int(target_count * augment_ratio)
                
                for _ in range(num_to_augment):
                    # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¢å¼º
                    original_item = random.choice(items)
                    
                    if 'text' in original_item:
                        # ç”Ÿæˆå¢å¼ºæ–‡æœ¬
                        augmented_texts = self.paraphrase_generation(
                            original_item['text'], 
                            methods=methods,
                            num_variants=1
                        )
                        
                        for aug_text in augmented_texts[:1]:
                            augmented_item = original_item.copy()
                            augmented_item['text'] = aug_text
                            augmented_item['is_augmented'] = True
                            augmented_item['augmentation_method'] = random.choice(methods)
                            augmented_data.append(augmented_item)
        else:
            # æ•´ä½“å¢å¼º
            num_to_augment = int(len(data) * augment_ratio)
            
            for _ in range(num_to_augment):
                original_item = random.choice(data)
                
                if 'text' in original_item:
                    augmented_texts = self.paraphrase_generation(
                        original_item['text'], 
                        methods=methods,
                        num_variants=1
                    )
                    
                    for aug_text in augmented_texts[:1]:
                        augmented_item = original_item.copy()
                        augmented_item['text'] = aug_text
                        augmented_item['is_augmented'] = True
                        augmented_item['augmentation_method'] = random.choice(methods)
                        augmented_data.append(augmented_item)
        
        return augmented_data
    
    def _get_random_synonym(self, word):
        """è·å–éšæœºåŒä¹‰è¯"""
        if self.language == 'zh':
            synonyms_list = synonyms.nearby(word)[0]
            return random.choice(synonyms_list) if synonyms_list else None
        else:
            synonyms_list = []
            for syn in wordnet.synsets(word):
                for lemma in syn.lemmas():
                    synonyms_list.append(lemma.name().replace('_', ' '))
            return random.choice(synonyms_list) if synonyms_list else None
    
    def _is_punctuation(self, word):
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡ç‚¹ç¬¦å·"""
        punctuation = 'ï¼ï¼Ÿã€‚ï¼Œã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹'
        return all(char in punctuation for char in word)
    
    def _is_important_word(self, word):
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡è¦è¯æ±‡ï¼ˆå¦‚ä¸“æœ‰åè¯ã€æ•°å­—ç­‰ï¼‰"""
        # ç®€å•çš„å¯å‘å¼è§„åˆ™
        if word.isdigit():
            return True
        if len(word) > 6:  # é•¿è¯æ±‡å¯èƒ½æ˜¯ä¸“æœ‰åè¯
            return True
        if word.isupper():  # å…¨å¤§å†™å¯èƒ½æ˜¯ç¼©å†™
            return True
        return False
    
    def _check_pos_compatibility(self, original_pos, synonym):
        """æ£€æŸ¥è¯æ€§å…¼å®¹æ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„è¯æ€§æ£€æŸ¥é€»è¾‘
        return True
    
    def _is_valid_translation(self, original, translated):
        """æ£€æŸ¥å›è¯‘è´¨é‡"""
        # ç®€å•çš„è´¨é‡æ£€æŸ¥
        if len(translated) < len(original) * 0.5:
            return False
        if len(translated) > len(original) * 2:
            return False
        return True
    
    def _is_valid_paraphrase(self, original, paraphrase):
        """æ£€æŸ¥é‡Šä¹‰è´¨é‡"""
        # ç®€å•çš„è´¨é‡æ£€æŸ¥
        if len(paraphrase) < 3:
            return False
        if paraphrase == original:
            return False
        # å¯ä»¥æ·»åŠ æ›´å¤šè´¨é‡æ£€æŸ¥é€»è¾‘
        return True
    
    def generate_augmentation_report(self, original_data, augmented_data):
        """ç”Ÿæˆå¢å¼ºæŠ¥å‘Š"""
        original_count = len(original_data)
        augmented_count = len([item for item in augmented_data if item.get('is_augmented')])
        total_count = len(augmented_data)
        
        # ç»Ÿè®¡å¢å¼ºæ–¹æ³•ä½¿ç”¨æƒ…å†µ
        method_counts = defaultdict(int)
        for item in augmented_data:
            if item.get('is_augmented'):
                method = item.get('augmentation_method', 'unknown')
                method_counts[method] += 1
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼‰
        class_distribution = {}
        if 'label' in original_data[0]:
            original_classes = defaultdict(int)
            augmented_classes = defaultdict(int)
            
            for item in original_data:
                original_classes[item['label']] += 1
            
            for item in augmented_data:
                augmented_classes[item['label']] += 1
            
            class_distribution = {
                'original': dict(original_classes),
                'augmented': dict(augmented_classes)
            }
        
        report = {
            'data_statistics': {
                'original_count': original_count,
                'augmented_count': augmented_count,
                'total_count': total_count,
                'augmentation_ratio': f"{augmented_count / original_count:.2f}"
            },
            'method_usage': dict(method_counts),
            'class_distribution': class_distribution
        }
        
        return report

# ä½¿ç”¨ç¤ºä¾‹ï¼šç”µå•†è¯„è®ºæ•°æ®å¢å¼º
print("=== æ•°æ®å¢å¼ºç¤ºä¾‹ ===")

# åŸå§‹æ•°æ®é›†ï¼ˆå°æ ·æœ¬ï¼‰
sample_data = [
    {'text': 'è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œæˆ‘å¾ˆæ»¡æ„', 'label': 'positive'},
    {'text': 'æœåŠ¡æ€åº¦ä¸é”™ï¼Œå€¼å¾—æ¨è', 'label': 'positive'},
    {'text': 'å¿«é€’é€Ÿåº¦å¾ˆå¿«ï¼ŒåŒ…è£…ä¹Ÿå¾ˆå¥½', 'label': 'positive'},
    {'text': 'ä»·æ ¼å¤ªè´µäº†ï¼Œæ€§ä»·æ¯”ä¸é«˜', 'label': 'negative'},
    {'text': 'è´¨é‡å¾ˆå·®ï¼Œå®Œå…¨ä¸å€¼è¿™ä¸ªä»·', 'label': 'negative'},
    {'text': 'è¿˜å¯ä»¥å§ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„', 'label': 'neutral'}
]

augmenter = DataAugmenter(language='zh')

# å•ä¸ªæ–‡æœ¬å¢å¼ºæ¼”ç¤º
print("1. å•ä¸ªæ–‡æœ¬å¢å¼ºæ¼”ç¤º")
original_text = "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œæˆ‘å¾ˆæ»¡æ„"
print(f"åŸæ–‡: {original_text}")

# åŒä¹‰è¯æ›¿æ¢
synonym_variants = [augmenter.synonym_replacement(original_text, n=2) for _ in range(3)]
print("åŒä¹‰è¯æ›¿æ¢ç»“æœ:")
for i, variant in enumerate(synonym_variants):
    print(f"  {i+1}. {variant}")

# éšæœºäº¤æ¢
swap_variants = [augmenter.random_swap(original_text, n=1) for _ in range(2)]
print("éšæœºäº¤æ¢ç»“æœ:")
for i, variant in enumerate(swap_variants):
    print(f"  {i+1}. {variant}")

# ç»¼åˆå¢å¼º
print("\n2. ç»¼åˆå¢å¼ºæ¼”ç¤º")
paraphrases = augmenter.paraphrase_generation(
    original_text, 
    methods=['synonym', 'swap', 'deletion'],
    num_variants=2
)
print("ç»¼åˆå¢å¼ºç»“æœ:")
for i, paraphrase in enumerate(paraphrases):
    print(f"  {i+1}. {paraphrase}")

# æ•°æ®é›†å¢å¼º
print("\n3. æ•°æ®é›†å¢å¼º")
print(f"åŸå§‹æ•°æ®é›†å¤§å°: {len(sample_data)}")

augmented_dataset = augmenter.augment_dataset(
    sample_data, 
    augment_ratio=1.0,  # å¢å¼º100%
    methods=['synonym', 'swap'],
    preserve_balance=True
)

print(f"å¢å¼ºåæ•°æ®é›†å¤§å°: {len(augmented_dataset)}")

# æ˜¾ç¤ºå¢å¼ºåçš„æ•°æ®
print("\nå¢å¼ºåçš„æ•°æ®æ ·ä¾‹:")
for item in augmented_dataset:
    if item.get('is_augmented'):
        print(f"åŸæ–‡: {[x['text'] for x in sample_data if x['label'] == item['label']][0]}")
        print(f"å¢å¼º: {item['text']} (æ–¹æ³•: {item.get('augmentation_method')})")
        print(f"æ ‡ç­¾: {item['label']}")
        print("---")

# ç”Ÿæˆå¢å¼ºæŠ¥å‘Š
report = augmenter.generate_augmentation_report(sample_data, augmented_dataset)
print("\n4. å¢å¼ºæŠ¥å‘Š")
print(f"æ•°æ®ç»Ÿè®¡: {report['data_statistics']}")
print(f"æ–¹æ³•ä½¿ç”¨: {report['method_usage']}")
print(f"ç±»åˆ«åˆ†å¸ƒ: {report['class_distribution']}")
```

é€šè¿‡ç³»ç»Ÿçš„æ•°æ®é¢„å¤„ç†æµç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†åŸå§‹çš„ã€æ‚ä¹±çš„æ•°æ®è½¬æ¢ä¸ºé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ã€‚è¿™ä¸ªè¿‡ç¨‹è™½ç„¶ç¹çï¼Œä½†å¯¹äºæ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½è‡³å…³é‡è¦ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå»ºè®®æ ¹æ®å…·ä½“ä»»åŠ¡çš„ç‰¹ç‚¹å’Œæ•°æ®çš„ç‰¹æ€§ï¼Œçµæ´»è°ƒæ•´é¢„å¤„ç†ç­–ç•¥ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯ä¸åŒå¤„ç†æ–¹æ³•çš„æ•ˆæœã€‚

**æ•°æ®é¢„å¤„ç†çš„æœ€ä½³å®è·µå»ºè®®ï¼š**

1. **å»ºç«‹æ•°æ®è´¨é‡æ ‡å‡†**ï¼šåœ¨é¡¹ç›®å¼€å§‹æ—¶å°±æ˜ç¡®æ•°æ®è´¨é‡è¦æ±‚ï¼Œå»ºç«‹æ£€æŸ¥æ¸…å•
2. **ä¿ç•™åŸå§‹æ•°æ®**ï¼šåœ¨æ¯ä¸ªå¤„ç†æ­¥éª¤éƒ½ä¿ç•™åŸå§‹æ•°æ®çš„å¤‡ä»½ï¼Œä¾¿äºå›æº¯å’Œè°ƒè¯•
3. **æ¸è¿›å¼å¤„ç†**ï¼šä¸è¦ä¸€æ¬¡æ€§åº”ç”¨æ‰€æœ‰å¤„ç†æ­¥éª¤ï¼Œé€æ­¥éªŒè¯æ¯ä¸ªæ­¥éª¤çš„æ•ˆæœ
4. **è‡ªåŠ¨åŒ–æµç¨‹**ï¼šå°†å¸¸ç”¨çš„é¢„å¤„ç†æ­¥éª¤è‡ªåŠ¨åŒ–ï¼Œæé«˜æ•ˆç‡å’Œä¸€è‡´æ€§
5. **è´¨é‡ç›‘æ§**ï¼šå»ºç«‹æ•°æ®è´¨é‡ç›‘æ§æœºåˆ¶ï¼ŒåŠæ—¶å‘ç°å’Œå¤„ç†é—®é¢˜æ•°æ®
6. **æ–‡æ¡£è®°å½•**ï¼šè¯¦ç»†è®°å½•æ¯ä¸ªå¤„ç†æ­¥éª¤å’Œå‚æ•°è®¾ç½®ï¼Œç¡®ä¿å®éªŒçš„å¯é‡ç°æ€§

æ•°æ®é¢„å¤„ç†æ˜¯ä¸€ä¸ªéœ€è¦ä¸æ–­ä¼˜åŒ–çš„è¿‡ç¨‹ï¼Œéšç€å¯¹ä»»åŠ¡ç†è§£çš„æ·±å…¥å’Œæ•°æ®ç‰¹ç‚¹çš„å‘ç°ï¼Œé¢„å¤„ç†ç­–ç•¥ä¹Ÿéœ€è¦ç›¸åº”è°ƒæ•´ã€‚åªæœ‰é€šè¿‡ç²¾å¿ƒçš„æ•°æ®é¢„å¤„ç†ï¼Œæ‰èƒ½ä¸ºæ¨¡å‹è®­ç»ƒå¥ å®šåšå®çš„åŸºç¡€ã€‚
                 
                