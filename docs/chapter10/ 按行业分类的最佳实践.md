
## 4. 按行业分类的最佳实践

### 4.1 🏥 医疗：构建基于Llama3.1-70B的医疗诊断系统

    
在大模型技术迅猛发展的背景下，通用人工智能正加速向垂直领域渗透，行业智能化升级的需求日益迫切。面对医疗等专业场景对模型精准性、专业性与可解释性的高要求，通用大模型的泛化能力已难以充分满足实际应用需求——产业界正呼唤更加专业化、场景化、高可靠的AI解决方案。

Llama 3.1-70B是Meta推出的高性能开源大语言模型，拥有700亿参数，具备强大的语言理解与生成能力，适用于医疗等高专业性领域。通过结合高质量行业数据集（如：`medical_o1_sft_Chinese_alpaca`）进行指令微调，可构建具备深度领域认知的专用模型，精准掌握医学推理逻辑与术语规范。

在微调过程中，Llama 3.1-70B融合了大量医学对话、临床推理案例与专业文本数据，显著提升了其在长上下文中的医学信息记忆能力、诊断逻辑一致性与术语使用准确性。它不仅可作为医生的智能辅助工具，提升诊疗效率，也可用于医学教育中的虚拟病例交互、患者健康咨询机器人等场景。



#### 4.1.1 前提条件

* 用户已经获取LLaMA-Factory Online平台账户和密码，如果需要帮助或尚未注册，可参考[注册账户](/docs/documents/quickstart/loginAccount)完成注册。
* 当前账号的余额充裕，可满足模型微调服务的需要。点击可了解[最新的活动](/docs/documents/quickstart/freetrialguide)及[费用信息](/docs/documents/recharge/charging)，或前往<Anchor label="充值" href="/cost/recharge" />，如需了解更多请[联系我们](/docs/documents/recharge/refill#发票)。

#### 4.1.2 操作步骤

**配置概览**

| 配置参数 |  配置项  | 是否预置         |  说明|
| --- |:----------|:----------------------|----| 
| 模型 | Llama 3.1-70B-Instruct     | 是| 经过指令微调，参数量约700亿 (70B)，专为高精度指令理解与复杂任务执行而优化。 |
| 数据集| medical_o1_sft_Chinese_alpaca |  是     |医疗诊断数据集。|
|GPU| H800*8（推荐）  | -                       |- |
|微调方法|lora|-|显著降低计算与存储成本，兼具高性能与部署灵活性。|



**操作详情**

1. 使用已注册的LLaMA-Factory Online账号登录平台，选择[实例空间]菜单项，进入实例空间页面，如下图所示。

     ![开始微调](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/startWT.png)

2. 单击上图“开始微调”按钮，进入[配置资源]页面，选择GPU资源，卡数填写`8`，点击“启动”，如下图所示。

![SwanLab对比图](http://s1.llamafactory.online/lmlab/docs/v1.0/paper/3.png) 

3. 待实例启动后，点击[LLaMA-Factory快速微调模型]页签，进入LLaMA Factory Online在线WebUI微调配置页面，语言选择`zh`，如下图高亮①所示；模型名称选择`Llama-3.1-70B-Instruct`，如下图高亮②所示；系统默认填充模型路径`/shared-only/models/meta-llama/Llama-3.1-70B-Instruct`。

4. 微调方法选择`lora`，如下图高亮④所示；选择“train”标签，训练方式保持`Supervised Fine-Tuning`，如下图高亮⑤所示；数据路径保持`/workspace/llamafactory/data`，如下图高亮⑥所示；数据集选择平台已预置的`medical_o1_sft_Chinese_alpaca`，如下图高亮⑦所示。

      ![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/02.png)

    点击“预览数据集”，可以预览数据样例。

      ![预览数据集](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/3.png)


5. 其余参数可根据实际需求调整，具体说明可参考[参数说明](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html)。  
     本实践中的其他参数设置修改如下，其他参数保持默认。
    
    ![参数设置2](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/005.png)


6. 参数配置完成后，点击“开始”按钮启动微调任务。页面底部将实时显示微调过程中的日志信息，例如下图高亮①所示；同时展示当前微调进度及Loss变化曲线，如下图高亮②所示。微调完成后，下图高亮③处会提示：“训练完毕”。
      
 ![完成训练](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/9.png)



7. 切换至“chat”界面，选择上一步骤已经训练完成的检查点路径，如下图高亮①所示；单击“加载模型”按钮（高亮②），微调的模型加载后，用户输入问题（高亮③）后，点击“提交”（高亮④），观察模型回答，如下图高亮⑤所示。

      ![微调模型对话](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/chat1.png)




微调后的模型按用户要求逐步分析题目信息和选项，最终给出正确选项。其回答条理清晰，简单明了。



8. 返回LLaMA-Factory Online平台的“实例空间”页面，点击页面右上角 <img src="http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/resetGPU.png" alt="resetGPU" width="20" height="20"/> 按钮重新设置GPU数量为4，如下图所示。然后点击页面右侧“JupyterLab处理专属数据”，进入jupyterlab工作空间。
 
![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/z1.png)

9. 在`/workspace`路径下新建一个文件夹，命名：`medi_eval`，用作存放评估相关的脚本和输出文件。

![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/7.png)

下载[LLaMAFactory项目压缩包](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/LLaMAFactory-main.zip)，拖到`medi_eval`文件夹下，目录为`/workspace/medi_eval`。  
点击jupyterlab页面的`Terminal`打开终端，使用下面的命令解压下载的压缩包。

```python
unzip /workspace/medi_eval/LLaMA-Factory-main.zip
```
解压后如下图所示：高亮①处为上传的压缩包，高亮②处为解压的指令，高亮③处为解压后的文件夹。

![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/6.png)

10. **生成评估案例及样本。**
  
打开jupyter notebook内的终端（Terminal），默认启动的环境名为lf。使用lf环境，运行下列vllm_infer.py的命令，开始进行模型评估样本的生成。

```python
python /workspace/medi_eval/LLaMA-Factory-main/scripts/vllm_infer.py \
--model_name_or_path /shared-only/models/meta-llama/Llama-3.1-70B-Instruct \
--adapter_name_or_path /workspace/user-data/models/output/{your_checkpoint} \
--dataset_dir /workspace/llamafactory/data \
--dataset medical_o1_sft_Chinese_alpaca   \
--template llama3    \
--cutoff_len 2048  \
--max_samples 500  \
--save_name /workspace/{your_save_name}.jsonl \
--temperature 0.1   \
--top_p 0.9 \
--top_k 40 \
--max_new_tokens 512 \
--repetition_penalty 1.1 \
--batch_size 8  \
--seed 42 \
--vllm_config '{"tensor_parallel_size": 4}'  
```

> **⚠️ 警告**  
> 请更改以下参数为自定义值。  
>   
> **adapter_name_or_path：** `{your_checkpoint}` 处填入需加载的 checkpoint 路径（如果对基模型进行评估，请勿传入这一参数）。本实践的 checkpoint 路径为：  
> `/workspace/user-data/models/output/Llama-3.1-70B-Instruct/lora/train_2025-08-27-12-11-52/checkpoint-2000`  
>   
> **save_name：** 微调后的模型生成的评估案例保存路径。本实践的保存路径为：  
> `/workspace/llamafactory/evaluate_case_Llama-3.1-70B-Instruct.jsonl`  
>   
> **vllm_config：** `tensor_parallel_size` 的值需与您启动实例时的 GPU 卡数一致。

出现如下图所示的输出，表示运行成功。

![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/8.png)



11. **准备评估环境。** 创建新环境，为评估模型做准备。

点击jupyterlab页面的`Terminal`打开终端，创建新的python环境。（本实践中新环境命名为“evaluate”，python版本为"3.10"）

```python
conda create -n evaluate python=3.10
```
创建完成后，启动该环境。
```python
conda activate evaluate
```
安装指令评估所需要的python包。
```python
pip install evaluate rouge-score nltk -i https://pypi.tuna.tsinghua.edu.cn/simple
```
       
12. **开始评估**。   

下载[指标评估文件](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/70b_eval_metrics.py)，拖拽到`/worksapace/medi_eval`目录下。  
      
![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/10.png)

在新建的“evaluate”环境中，运行以下指令进行评估。

```python
python medi_eval/70b_eval_metrics.py \
--pred_file {generated_predictions}.jsonl \
--output_metrics {metrics_save_name}.json
```
            
> **⚠️ 警告**  
> 修改以下参数值：  
>   
> - **pred_file**：评估案例保存的文件路径。本实践路径为：  
>   `/workspace/llamafactory/evaluate_case_Llama-3.1-70B-Instruct.jsonl`  
>   
> - **output_metrics**：本次指令评估的结果保存路径。本实践路径为：  
>   `/workspace/medi_eval/evaluate_result_Llama-3.1-70B-Instruct.json`


      

13. **评估结果如下所示。**
<Tabs>
<TabItem value="微调后模型" label="微调后模型评估">

![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/result1.png)

```python
{
      BLEU-4: 0.0469
      ROUGE-1: 0.7839
      ROUGE-2: 0.5865
      ROUGE-L: 0.7696
}
```
结果解读：模型生成结果与参考答案在整体内容和结构上较为接近（ROUGE分数高），但在字面和短语精确匹配上还有提升空间（BLEU分数较低）。模型适合生成内容覆盖面广、结构合理的文本，但字面一致性有待加强。
     


微调后模型在医疗诊断任务上的生成质量远高于原生模型，内容覆盖、结构合理性和语义连贯性均有明显提升，更适合实际医疗场景应用。

#### 4.1.3 总结
用户可通过LLaMA-Factory Online平台预置的模型及数据集完成快速微调与效果验证。从上述实践案例可以看出，基于`Llama3.1-70B-Instruct`模型，采用LoRA方法在`medical_o1_sft_Chinese_alpaca`医学诊断数据集上进行指令微调后，模型医学诊断和临床决策领域的专业能力有显著提升。

本实践为构建高质量的医学诊断和临床决策系统提供了可复用的技术路径，适用于智能辅助工具、医学教育中的虚拟病例交互、患者健康咨询机器人等场景。未来可以使用实际业务数据集，对模型进行微调，得到能够解决医疗领域实际问题的本地化大模型。






### 4.2 ⚖️ 法律：构建LightLLM+LlamaIndex法律智能体


近年来，大语言模型（LLM）能力显著，但其知识的静态滞后性导致其在处理新信息或专业问题时易产生“幻觉”。RAG（retrieval-augmented generation，检索增强生成）框架通过“先检索后生成”的方式，将外部知识库的准确性与LLM的生成能力相结合，有效解决了这一问题，成为构建企业知识库、法律医疗等专业应用的核心技术。在此背景下，掌握以LightLLM为代表的高效推理框架，不仅能帮助学习者深入理解RAG的协同机制，更能培养在资源受限环境下优化与部署AI系统的关键能力，为投身AI应用开发奠定坚实基础。

#### 4.2.1 前提条件
* 用户已经获取LLaMA-Factory Online平台账户和密码，如果需要帮助或尚未注册，可参考[注册账户](/docs/documents/quickstart/loginAccount)完成注册。
* 当前账号的余额充裕，可满足实例运行的需要。点击可了解[最新的活动](/docs/documents/quickstart/freetrialguide)及[费用信息](/docs/documents/recharge/charging)，或前往<Anchor label="充值" href="/cost/recharge" />，如需了解更多请[联系我们](/docs/documents/recharge/refill#发票)。




#### 4.2.2 操作步骤
**配置概览**

| 配置参数 |  配置项    |  说明|
| --- |:----------|----| 
| 模型 |Qwen3-8B |本实践通过本地启动大模型服务 |
|GPU| H800*1  | -                    | |

**操作详情**


1. 使用已注册的LLaMA-Factory Online账号登录平台，选择[实例空间]菜单项，进入实例空间页面，例如下图所示。

![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/startWT.png)

2. 单击上图“开始微调”按钮，进入[配置资源]页面，选择GPU资源，卡数填写`1`，其他参数保持为默认值，单击“启动”按钮，启动实例。

![SwanLab对比图](http://s1.llamafactory.online/lmlab/docs/v1.0/paper/1.png) 

3. 实例启动后，可启动VSCode或者JupyterLab专属数据处理，本次实践我们使用VSCode专属数据处理，单击图标链接进入数据处理页面，点击[Terminal/New Terminal]进入终端页面。在终端运行如下所示的命令安装服务运行所需环境。
 
    1. 运行如下命令创建一个名称为“py39”的Python环境。
     ```shell
     conda create -n lightllm python=3.10
     ```
     2. 运行如下所示的命令激活自定义的环境。
     ```shell
     conda activate lightllm
     ```
     3. 运行如下所示的命令，在已经激活的Python环境中安装`ipykernel`包。
      ```shell
      pip install ipykernel -i https://pypi.tuna.tsinghua.edu.cn/simple
      ```
     4. 运行如下所示的命令，将当前Python环境注册为Jupyter内核。

      ```bash
      kernel_install --name lightllm  --display-name "Python 3.10 (lightllm)"
      ```
    > **提示**：您需在对应的 Conda 环境中运行上述注册命令，否则内核（kernel）将无法正确注册。
4. Python环境安装完成后，在终端运行如下所示的命令下载并安装LightLLM框架及其依赖项，例如下图所示。

     1. 运行如下命令，从GitHub仓库克隆LightLLM项目的源代码到本地。
     ```shell
     git clone https://github.com/ModelTC/lightllm.git
     ```
     2. 运行如下所示的命令，进入项目目录。
     ```shell
     cd lightllm
     ```
     3. 运行如下所示的命令，安装项目运行所需的所有Python依赖包。
      ```shell
      pip install -r requirements.txt
      ```
     4. 运行如下所示的命令，以开发模式安装LightLLM包。
      ```shell
      python setup.py install
      ```
     5. 运行如下命令，安装LlamaIndex库，用于构建基于私有数据的检索增强生成（RAG）应用。
     ```shell
     pip install llama-index
     ```
    6. 运行如下所示的命令，安装Streamlit库，用于快速创建交互式数据可视化网页应用。
     ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/2.png)

     ```shell
     pip install streamlit
     ```
   
   
   7. 运行如下命令，安装LightLLM运行所需的计算机视觉处理库和WebSocket通信支持依赖包。
     ```shell
     pip install torchvision
     pip install websockets
     pip install partial-json-parser
     pip install interegular
     ```
    8. 运行如下命令，安装LlamaIndex框架对HuggingFace本地嵌入模型的支持包，使RAG应用能够使用BAAI/bge等本地嵌入模型进行文本向量化。
    ```shell
    pip install llama-index-embeddings-huggingface
    ```
    
       
    您可运行`pip list | grep -E "torchvision|websockets|partial-json-parser|interegular|streamlit|llama-index|llama-index-embeddings-huggingface"`命令查看依赖包安装情况，例如下图所示。
     
     ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/8.png)
    

5. 依赖包安装完成后，运行如下所示命令，启动LightLLM框架的API服务器提供标准的OpenAI兼容接口，支持文本生成、对话等推理功能，模型加载成功后，页面显示例如下图所示，默认端口为8000。
   
   ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/lanunchser.png)

      ```shell
      python -m lightllm.server.api_server --model_dir /shared-only/models/Qwen/Qwen3-8B
      ```

  
    
6. 新建Terminal窗口，执行如下代码测试模型推理功能是否正常，服务正常启动页面例如下图所示。

    ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/result.png)
        ```json
        curl -X POST "http://localhost:8000/v1/chat/completions" \
          -H "Content-Type: application/json" \
          -d '{
            "model": "Qwen3-8B",
            "messages": [{"role": "user", "content": "Hello"}],
            "max_tokens": 1000
          }'
        ```

7. 点击链接[下载app.py](http://llamafactory-online-assets.oss-cn-beijing.aliyuncs.com/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/app.py)，下载后将文件拖拽至`/workspace/lightllm`目录下，然后运行如下所示命令加载RAG服务，加载成功后页面例如下图所示。

    ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/3.png)

      ```shell
      streamlit run app.py \
          --server.address 0.0.0.0 \
          --server.port 6666 \
          --server.enableCORS false \
          --server.enableXsrfProtection false
      ```

8. 点击服务链接，进入本地RAG文档搜索助手页面，例如下图所示。

    ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/44.png)

#### 4.2.3 服务应用
服务部署完成后，用户可自定义构建知识库，并基于库内信息查询相关问题，快速获取解决方案。


单击上方高亮区域的“Browse files”按钮，选择PDF或TXT格式的知识库文件上传，或直接将文件拖拽至该区域，本实践上传[示例1](http://llamafactory-online-assets.oss-cn-beijing.aliyuncs.com/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/%E5%8A%B3%E5%8A%A8%E6%B3%95%E6%9D%A1%E4%BE%8B1.txt)，[示例2](http://llamafactory-online-assets.oss-cn-beijing.aliyuncs.com/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/%E5%8A%B3%E5%8A%A8%E6%B3%95%E6%9D%A1%E4%BE%8B2.txt)。

待页面出现“✅文档已加载并索引！”即可输入您的问题，例如：“ 我有考勤记录截图和与领导沟通加班的微信记录。劳动合同写的是月薪8000元，但公司一直按基本工资3000元为基数算加班费。我主张最近一年的。”。观察应用回复，例如下图所示，回答详情可展开“Details”查看详情。

![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/5.png)




<details>
<summary>回答</summary>

&lt;think&gt;

好的，用户上个月离职了，但公司还欠他很多加班费没给，他不知道该怎么办。首先，我需要根据提供的上下文信息来给出建议。用户之前已经有一些相关的提问和回答，比如关于加班费计算基数、仲裁申请书的撰写以及电子签字的补强方法。现在的情况是用户已经离职，但仍有未支付的加班费。

首先，用户需要确认自己是否已经收集了足够的证据。根据之前的对话，用户有加班考勤记录、工资条和劳动合同。这些证据是关键，因为仲裁需要这些来支持主张。用户可能还需要检查是否有其他证据，比如微信聊天记录、邮件或者工作群里的消息，这些可以证明加班的事实和公司的安排。

接下来，用户需要考虑是否已经超过了仲裁时效。根据中国的劳动法，劳动争议的仲裁时效是一年，从知道或应当知道权利被侵害之日起计算。用户上个月离职，如果加班费是在离职前发生的，那么时效可能还未超过。但需要确认具体的时间点，比如最后加班的时间和离职时间之间的关系。

然后，用户应该准备仲裁申请。根据之前的建议，需要整理好证据，包括考勒记录、工资条、劳动合同等，并计算加班费的差额。用户可能需要计算按照劳动合同约定的工资基数计算的加班费，与公司实际支付的差额，并准备具体的金额。同时，用户需要撰写仲裁申请书，明确请求支付的金额和理由。

另外，用户提到之前有电子审批单的问题，但现在已经离职，可能需要确保电子证据的合法性。比如，如果电子签字被质疑，用户需要准备系统登录方式或关联的微信记录来补强证据。不过，现在可能已经没有机会在庭上展示，所以需要提前准备好这些材料，以防仲裁员提出质疑。

用户可能还需要考虑是否需要律师的帮助，尤其是如果公司有较强的法律团队，或者案件复杂的情况下。但根据之前的建议，用户可以自行准备材料，因为证据充分的话，法律优势明显。

最后，用户需要按照仲裁程序提交申请，确保在规定的时效内，并且正确填写所有必要的文件。如果仲裁结果不满意，还可以考虑诉讼，但诉讼的时效是两年，需要留意时间节点。

总结下来，用户需要做的步骤包括:确认证据完整性，计算加班费差额，准备仲裁申请材料，确保时效性，并可能寻求法律援助。同时，注意不要遗漏任何可能的证据，如电子记录的合法性，以及离职后是否还有追诉权。

&lt;think&gt;

你的考勤记录截图和微信沟通记录已能形成完整证据链，可明确主张以实际月工资 8000 元为基数计算加班费。具体操作建议如下：

你已离职但仍有加班费未支付，可按以下步骤维权:

1. **确认证据完整性**：整理所有加班记录(考勤截图、工资条、劳动合同、微信/邮件沟通记录等)，确保能证明加班事实及公司未足额支付。

2. **计算加班费差额**：以劳动合同约定的工资为基数(如月薪7000元)，按法定标准(工作日1.5倍、周末2倍、节假日3倍)计算应得加班费，核对已支付金额，明确差额。

3. **申请劳动仲裁**：在离职后1年内向公司所在地劳动仲裁委员会提交申请，明确请求支付欠付的加班费差额。需准备证据清单及仲裁申请书，说明事实与理由。

4. **应对电子证据质疑**：若存在电子审批单等电子证据，可提前准备系统操作记录或关联聊天记录(如领导微信确认加班内容)，证明其真实性。

5. **时效与后续程序**：劳动仲裁时效为1年，离职后仍可主张。若仲裁结果不满意，可在15日内向法院提起诉讼。

建议尽快行动，避免超过时效，同时保持证据链完整以提高胜诉率。
</details>





多维度对比两类服务的回复，对比详情如下所示。

| 对比维度 | RAG服务回复 |原生模型回复 |
|---------|------------------|--------------------|
| **1. 计算基数主张** | ✅ 明确指出应以实际发放工资8000元为基数，并提出反驳策略：“工资条未列明加班费项目”、“银行流水为证” | ⚠️ 仅说明“可能存在违法风险”，未给出具体举证方式或反制话术 |
| **2. 证据链构建** | ✅ 强调“考勤截图+微信记录”需相互印证，并建议“当庭登录系统验证真实性”——这是关键实操技巧 | ❌ 仅说“需能证明具体时间”，缺乏操作指引 |
| **3. 加班事实认定** | ✅ 点出核心：“加班系公司安排而非自愿”——这是仲裁胜负关键点之一 | ❌ 未涉及此关键法律要件 |
| **4. 差额计算方法** | ✅ 明确列出倍数标准（工作日1.5倍、周末2倍、法定节假日3倍），要求精确核算差额金额 | ❌ 只提法律规定，无具体计算指导 |
| **5. 应对公司抗辩** | ✅ 预判对方可能说“3000是基本工资”，并提供标准反驳话术：“劳动合同未约定工资构成”、“公司未提供薪酬制度证明”，应视为未足额支付 | ❌ 虽提到地方差异，但未教用户如何应对企业常见推诿 |
| **6. 仲裁请求撰写** | ✅ 明确写出应提交的仲裁请求模板，包括诉求金额、法律依据（《劳动法》第44条、《工资支付暂行规定》第13条） | ❌ 仅有知识介绍，无行动指令 |
| **7. 用户角色定位** | ✅ 把用户当作即将出庭的申请人，给予战斗性、可执行的诉讼策略指导 | ❌ 把用户当作学习者，进行普法教育式的内容输出 |

RAG服务回复更具实战性、对抗性和可操作性，适合作为智能法律助手的输出标准，把用户当作即将出庭的申请人，给予战斗性、可执行的诉讼策略指导。




#### 4.2.4 总结
通过“轻量级RAG验证 → 功能迭代优化 → 企业级部署落地”的渐进式路径，我们基于LightLLM高效推理引擎，构建了一套可落地的智能知识服务系统。该方案有效缓解了大模型在专业领域存在的知识滞后与幻觉问题，可在法律场景中实现从法条检索、判例匹配到案情分析的全流程应用，显著提升响应准确率与业务可用性。

实践验证了该架构在高精度、强合规性场景下的可行性，具备向金融、医疗、政务等知识密集型行业快速复制的能力。后续将持续深化多模态内容理解、复杂任务编排与安全可控机制，推动RAG技术与企业业务流程深度融合，打造稳定、可解释、可扩展的行业AI解决方案，真正实现人工智能在核心业务场景中的价值闭环。

### 4.3 💰 金融：构建基于Llama-3.2-11B-Vision-Instruct金融分析


在金融领域中，图表承担了展示信息的重要责任，而读取图表中所包含的信息对每位金融从业者更是必不可少的机能。如今，金融分析愈发成熟，我们每天需要处理的图标也随之越来越复杂，此时AI的引入能够帮助我们更有效更快速地抓取关键信息在金融领域中，图表承担了展示信息的重要责任，而读取图表中所包含的信息对每位金融从业者更是必不可少的机能。如今，金融分析愈发成熟，我们每天需要处理的图表也随之越来越复杂，此时AI的引入能够帮助我们更有效更快速地抓取关键信息。

金融特化版Llama-3.2-11B-Vision-Instruct是针对金融分析与决策优化的多模态模型，能够同时处理文本、数据和视觉输入（如财报、市场指标、K线图等），输出包括财务分析、风险提示、趋势解读等结构化洞察。其应用涵盖自动化投研（快速提取关键财务信号）、合规审查（识别文档风险点）及可视化报告生成（结合图表与文本分析），通过金融领域语料和视觉数据的针对性微调，提升了处理复杂金融信息的准确性与实用性，适用于投行、资管和企业财务等场景。

### 4.3.1 前提条件
* 用户已经获取LLaMA-Factory Online平台账户和密码，如果需要帮助或尚未注册，可参考[注册账户](/docs/documents/quickstart/loginAccount)完成注册。
* 当前账号的余额充裕，可满足模型微调服务的需要。点击可了解[最新的活动](/docs/documents/quickstart/freetrialguide)及[费用信息](/docs/documents/recharge/charging)，或前往<Anchor label="充值" href="/cost/recharge" />，如需了解更多请[联系我们](/docs/documents/recharge/refill#发票)。



### 4.3.2 操作步骤
**配置概览**

| 配置参数 |  配置项  | 是否预置         |  说明|
| --- |:----------|:----------------------|----| 
| 模型 | Llama3.2-11B-Vision-Instruct     | 是| 经过指令微调，参数量约110亿 (11B)，专为处理图像和文本数据的任务而设计。 |
| 数据集| fin_mme |  是     |金融多模态推理评估基准数据集。|
|GPU| H800*4（推荐）  | -                       |H800**2（最少）。 |
|微调方法|lora|-|显著降低计算与存储成本，兼具高性能与部署灵活性。|



**操作详情**


1. 使用已注册的LLaMA Factory Online账号登录平台，选择[实例空间]菜单项，进入实例空间页面，例如下图所示。

     ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/startWT.png)

2. 单击上图“开始微调”按钮，进入[配置资源]页面，选择GPU资源，卡数填写`4`，其他参数保持为默认值，例如下图所示。

     ![SwanLab对比图](http://s1.llamafactory.online/lmlab/docs/v1.0/paper/2.png) 
     
3. 单击“启动”按钮，待实例启动后，点击[LLaMA-Factory快速微调模型]页签，进入LLaMA-Factory Online在线WebUI微调配置页面，语言选择`zh`，如下图高亮①所示；模型名称选择`Llama-3.2-11B-Vision-Instruct`，如下图高亮②所示；系统默认填充模型路径`/shared-only/models/meta-llama/Llama-3.2-11B-Vision-Instruct`。

4. 微调方法选择`lora`，如下图高亮④所示；选择“train”功能性，训练方式保持`Supervised Fine-Tuning`，如下图高亮⑤所示；数据路径保持`/workspace/llamafactory/data`，如下图高亮⑥所示；数据集选择平台已预置的`fin_mme`，如下图高亮⑦所示。

    ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/parameter.png)

点击“设置/常规”，即可获取API Key，示例如下图所示。
[SwanLab](https://www.swanlab.cn/)是一款开源且轻量级的AI模型训练可视化追踪工具。在本次微调任务中，我们将使用SwanLab记录整个微调过程。在开始之前，请确保您已经登录[SwanLab](https://www.swanlab.cn/)平台。登录后，点击“设置/常规”，即可获取API Key，示例如下图所示。



5. （可选）其余参数可根据实际需求调整，具体说明可参考[参数说明](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html)，本实践中的其他参数均保持默认值。


6. 参数配置完成后，点击“开始”按钮启动微调任务。页面底部将实时显示微调过程中的日志信息，例如下图高亮①所示；同时展示当前微调进度及Loss变化曲线。经过多轮微调后，例如下图高亮②所示，从图中可以看出Loss逐渐趋于收敛。微调完成后，系统提示“训练完毕”，例如下图高亮③所示。
      
       ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/loss.png)




7. 切换至“chat”界面，如下图高亮①所示；选择上一步骤已经训练完成的检查点路径，如下图高亮②所示；单击“加载模型”按钮，微调的模型加载后，在系统提示词处填入提示词，如下图高亮③所示；上传图片后输入用户模拟词“Describe the overall change in trend of revenue presented in the chart”，观察模型回答，如下图高亮⑤所示。

      ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/fine.png)



微调后的模型在图表读取上表现出更强的契合度，其回答不仅更贴近系统预设的角色定位，也更符合用户的认知预期。



8. 切换至“Evaluate & Predict”页面，选择微调后模型的检查点路径，例如下图高亮①所示；然后选择平台预置的`fin_mme`数据集，并根据实际需求配置评估参数（本实践的参数设置如下图所示）。


     ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/evaluate.png)

9. 参数配置完成后，点击“开始”按钮即可启动评估，页面底部将实时显示评估过程中的日志信息，评估完成后，记录评估结果，结果如下所示。

          ```shell
         {
            "predict_bleu-4": 27.38479177927928,
            "predict_model_preparation_time": 0.0082,
            "predict_rouge-1": 46.59722225225225,
            "predict_rouge-2": 0.07882882882882883,
            "predict_rouge-l": 46.59722225225225,
            "predict_runtime": 475.4967,
            "predict_samples_per_second": 18.673,
            "predict_steps_per_second": 2.334
          }
          ```
      结果解读：该模型在语义覆盖（`ROUGE-1`）、句法连贯性（`ROUGE-L`）和短语匹配（`ROUGE-2`）方面均有良好表现，相较原生模型评估`BLEU-4=6.66`，有了明显改进，该版本已具备使用基础。
  


微调后模型在相同评估条件下有明显提升，生成质量上，`BLEU-4:27.38`，`ROUGE-1:46.60`，`ROUGE-2`和`ROUGE-L`也分别达到`0.0079`和`46.60`，显示出更优的关键词覆盖能力、短语搭配合理性和句级语义连贯性。表明微调后的模型显著增强了语言生成质量。综上，微调后的模型生成性能上远优于原生模型，具备更好的应用潜力。



## 总结
用户可通过LLaMA-Factory Online平台预置的模型及数据集完成快速微调与效果验证。从上述实践案例可以看出，基于`Llama3.2-11B-Vision-Instruct`模型，采用LoRA方法在`fin_mme`金融数据集上进行指令微调后，模型在处理文本、数据和视觉输入（如财报、市场指标、K线图等），输出包括财务分析、风险提示、趋势解读等结构化洞察等方面均有显著提升。

本实践构建自动化投研、合规审查、可视化报告生成系统提供了可复用的技术路径，适用于投行、资管和企业财务等场景。未来可进一步探索多模态输入输出、强化实时数据联动能力、优化长周期时序分析能力，构建行业细分场景的适配性，增强模型的可解释性和可追溯性。



