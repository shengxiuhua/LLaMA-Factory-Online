
## 4. 按行业分类的最佳实践

### 4.1 🏥 医疗：构建基于Llama3.1-70B的医疗诊断系统

在大模型技术迅猛发展的背景下，通用人工智能正加速向垂直领域渗透，行业智能化升级的需求日益迫切。面对医疗等专业场景对模型精准性、专业性与可解释性的高要求，通用大模型的泛化能力已难以充分满足实际应用需求——产业界正呼唤更加专业化、场景化、高可靠的AI解决方案。

Llama 3.1-70B是Meta推出的高性能开源大语言模型，拥有700亿参数，具备强大的语言理解与生成能力，适用于医疗等高专业性领域。通过结合高质量行业数据集（如：`medical_o1_sft_Chinese_alpaca`）进行指令微调，可构建具备深度领域认知的专用模型，精准掌握医学推理逻辑与术语规范。

在微调过程中，Llama 3.1-70B融合了大量医学对话、临床推理案例与专业文本数据，显著提升了其在长上下文中的医学信息记忆能力、诊断逻辑一致性与术语使用准确性。它不仅可作为医生的智能辅助工具，提升诊疗效率，也可用于医学教育中的虚拟病例交互、患者健康咨询机器人等场景。



#### 4.1.1 前提条件

* 用户已经获取LLaMA-Factory Online平台账户和密码，如果需要帮助或尚未注册，可参考[注册账户](/docs/documents/quickstart/loginAccount)完成注册。
* 当前账号的余额充裕，可满足模型微调服务的需要。点击可了解[最新的活动](/docs/documents/quickstart/freetrialguide)及[费用信息](/docs/documents/recharge/charging)，或前往<Anchor label="充值" href="/cost/recharge" />，如需了解更多请[联系我们](/docs/documents/recharge/refill#发票)。

#### 4.1.2 操作步骤

**配置概览**

| 配置参数 |  配置项  | 是否预置         |  说明|
| --- |:----------|:----------------------|----| 
| 模型 | Llama 3.1-70B-Instruct     | 是| 经过指令微调，参数量约700亿 (70B)，专为高精度指令理解与复杂任务执行而优化。 |
| 数据集| medical_o1_sft_Chinese_alpaca |  是     |医疗诊断数据集。|
|GPU| H800*8（推荐）  | -                       |- |
|微调方法|lora|-|显著降低计算与存储成本，兼具高性能与部署灵活性。|





**操作详情**

1. 使用已注册的LLaMA-Factory Online账号登录平台，选择[实例空间]菜单项，进入实例空间页面，如下图所示。

     ![开始微调](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/startWT.png)

2. 单击上图“开始微调”按钮，进入[配置资源]页面，选择GPU资源，卡数填写`8`，点击“启动”，如下图所示。

      ![SwanLab对比图](http://s1.llamafactory.online/lmlab/docs/v1.0/paper/3.png) 

3. 待实例启动后，点击[LLaMA-Factory快速微调模型]页签，进入LLaMA Factory Online在线WebUI微调配置页面，语言选择`zh`，如下图高亮①所示；模型名称选择`Llama-3.1-70B-Instruct`，如下图高亮②所示；系统默认填充模型路径`/shared-only/models/meta-llama/Llama-3.1-70B-Instruct`。

4. 微调方法选择`lora`，如下图高亮④所示；选择“train”标签，训练方式保持`Supervised Fine-Tuning`，如下图高亮⑤所示；数据路径保持`/workspace/llamafactory/data`，如下图高亮⑥所示；数据集选择平台已预置的`medical_o1_sft_Chinese_alpaca`，如下图高亮⑦所示。

      ![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/02.png)

    点击“预览数据集”，可以预览数据样例。

      ![预览数据集](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/3.png)


5. 其余参数可根据实际需求调整，具体说明可参考[参数说明](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html)。  
     本实践中的其他参数设置修改如下，其他参数保持默认。
    
    ![参数设置2](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/005.png)


6. 参数配置完成后，点击“开始”按钮启动微调任务。页面底部将实时显示微调过程中的日志信息，例如下图高亮①所示；同时展示当前微调进度及Loss变化曲线，如下图高亮②所示。微调完成后，下图高亮③处会提示：“训练完毕”。
      
       ![完成训练](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/9.png)


<Tabs>
<TabItem value="Windows" label="微调后模型对话" >


7. 切换至“chat”界面，选择上一步骤已经训练完成的检查点路径，如下图高亮①所示；单击“加载模型”按钮（高亮②），微调的模型加载后，用户输入问题（高亮③）后，点击“提交”（高亮④），观察模型回答，如下图高亮⑤所示。

      ![微调模型对话](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/chat1.png)



</TabItem>


 <TabItem value="Liux" label="原生模型对话" >

7. 单击下图高亮①所示的“卸载模型”按钮，卸载微调后的模型，清空“检查点路径”中的LoRA配置（高亮②），单击“加载模型”按钮（高亮③），加载原生的`Llama-3.1-70B-Instruct`模型进行对话，其余配置保持不变。用户在下图高亮④处输入同样的问题，点击“提交”（高亮⑤），观察模型回答，如下图高亮⑥所示。

      ![原生模型对话](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/chat2.png)

 </TabItem>

</Tabs>

通过对比微调模型与原生模型的对话结果可以发现，微调后的模型按用户要求逐步分析题目信息和选项，最终给出正确选项。其回答条理清晰，简单明了。



8. 返回LLaMA-Factory Online平台的“实例空间”页面，点击页面右上角 <img src="http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/resetGPU.png" alt="resetGPU" width="20" height="20"/> 按钮重新设置GPU数量为4，如下图所示。然后点击页面右侧“JupyterLab处理专属数据”，进入jupyterlab工作空间。
      :::warning
      目前LFO webUI不支持多卡评估，而70B模型单卡无法进行推断，所以需要在jupyterlab的终端内运行llama-factory项目中的vllm_infer.py脚本，进行分布式评估。
      :::
 
            ![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/z1.png)

9. 在`/workspace`路径下新建一个文件夹，命名：`medi_eval`，用作存放评估相关的脚本和输出文件。

            ![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/7.png)

            下载[LLaMAFactory项目压缩包](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/LLaMAFactory-main.zip)，拖到`medi_eval`文件夹下，目录为`/workspace/medi_eval`。  
            点击jupyterlab页面的`Terminal`打开终端，使用下面的命令解压下载的压缩包。

            ```python
            unzip /workspace/medi_eval/LLaMA-Factory-main.zip
            ```
            解压后如下图所示：高亮①处为上传的压缩包，高亮②处为解压的指令，高亮③处为解压后的文件夹。

            ![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/6.png)

10. **生成评估案例及样本。**
<Tabs>
  <TabItem value="微调后模型" label="微调后模型评估">
  
            打开jupyter notebook内的终端（Terminal），默认启动的环境名为lf。使用lf环境，运行下列vllm_infer.py的命令，开始进行模型评估样本的生成。

            ```python
            python /workspace/medi_eval/LLaMA-Factory-main/scripts/vllm_infer.py \
            --model_name_or_path /shared-only/models/meta-llama/Llama-3.1-70B-Instruct \
            --adapter_name_or_path /workspace/user-data/models/output/{your_checkpoint} \
            --dataset_dir /workspace/llamafactory/data \
            --dataset medical_o1_sft_Chinese_alpaca   \
            --template llama3    \
            --cutoff_len 2048  \
            --max_samples 500  \
            --save_name /workspace/{your_save_name}.jsonl \
            --temperature 0.1   \
            --top_p 0.9 \
            --top_k 40 \
            --max_new_tokens 512 \
            --repetition_penalty 1.1 \
            --batch_size 8  \
            --seed 42 \
            --vllm_config '{"tensor_parallel_size": 4}'  
            ```

            :::warning
            请更改以下参数为自定义值。  
            **adapter_name_or_path：**`{your_checkpoint}`处填入需加载的checkpoint路径（如果对基模型进行评估，请勿传入这一参数）。本实践的checkpoint路径为：`/workspace/user-data/models/output/Llama-3.1-70B-Instruct/lora/train_2025-08-27-12-11-52/checkpoint-2000 \`  

            **save_name：** 微调后的模型生成的评估案例保存路径。 本实践的保存路径为：`/workspace/llamafactory/evaluate_case_Llama-3.1-70B-Instruct.jsonl \`

            **vllm_config：** tensor_parallel_size的值与您启动实例时的GPU卡数一致。
            :::

                  出现如下图所示的输出，表示运行成功。

                  ![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/8.png)

  </TabItem>
  <TabItem value="原生模型评估" label="原生模型评估">
            打开jupyter notebook内的终端（Terminal），默认启动的环境名为lf。使用lf环境，运行下列vllm_infer.py的命令，开始进行模型评估样本的生成。

            ```python
            python /workspace/medi_eval/LLaMA-Factory-main/scripts/vllm_infer.py \
            --model_name_or_path /shared-only/models/meta-llama/Llama-3.1-70B-Instruct \
            --dataset_dir /workspace/llamafactory/data \
            --dataset medical_o1_sft_Chinese_alpaca   \
            --template llama3    \
            --cutoff_len 2048  \
            --max_samples 500  \
            --save_name /workspace/{your_save_name}.jsonl \
            --temperature 0.1   \
            --top_p 0.9 \
            --top_k 40 \
            --max_new_tokens 512 \
            --repetition_penalty 1.1 \
            --batch_size 8  \
            --seed 42 \
            --vllm_config '{"tensor_parallel_size": 4}'  
            ```

            :::warning
            请更改以下参数为自定义值。  
            **save_name：** 微调后的模型生成的评估案例保存路径。 本实践的保存路径为：`/workspace/llamafactory/evaluate_case__base_Llama-3.1-70B-Instruct.jsonl \`

            **vllm_config：** tensor_parallel_size的值与您启动实例时的GPU卡数一致。
            :::

                  出现如下图所示的输出，表示运行成功。

                  ![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/8.png)

  </TabItem>
</Tabs>

11. **准备评估环境。** 创建新环境，为评估模型做准备。

      点击jupyterlab页面的`Terminal`打开终端，创建新的python环境。（本实践中新环境命名为“evaluate”，python版本为"3.10"）

      ```python
      conda create -n evaluate python=3.10
      ```
      创建完成后，启动该环境。
      ```python
      conda activate evaluate
      ```
      安装指令评估所需要的python包。
      ```python
      pip install evaluate rouge-score nltk -i https://pypi.tuna.tsinghua.edu.cn/simple
      ```
       
12. **开始评估**。   

      下载[指标评估文件](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/70b_eval_metrics.py)，拖拽到`/worksapace/medi_eval`目录下。  
      
      ![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/10.png)

      在新建的“evaluate”环境中，运行以下指令进行评估。
            <Tabs>
            <TabItem value="微调后模型" label="微调后模型评估">
            ```python
            python medi_eval/70b_eval_metrics.py \
            --pred_file {generated_predictions}.jsonl \
            --output_metrics {metrics_save_name}.json

            ```
            
            :::warning
            修改以下参数值：  
            pred_file：评估案例保存的文件路径。本实践路径为：`/workspace/llamafactory/evaluate_case_Llama-3.1-70B-Instruct.jsonl \`  
            output_metrics：本次指令评估的结果保存路径。本实践路径为：`/workspace/medi_eval/evaluate_result_Llama-3.1-70B-Instruct.json`
            :::
            </TabItem>
            <TabItem value="原生模型评估" label="原生模型评估">
            ```python
            python medi_eval/70b_eval_metrics.py \
            --pred_file {generated_predictions}.jsonl \
            --output_metrics {metrics_save_name}.json

            ```
            
            :::warning
            修改以下参数值：  
            pred_file：评估案例保存的文件路径。本实践路径为：`/workspace/llamafactory/evaluate_case__base_Llama-3.1-70B-Instruct.jsonl \`  
            output_metrics：本次指令评估的结果保存路径。本实践路径为：`/workspace/medi_eval/evaluate_base_result_Llama-3.1-70B-Instruct.json`
            :::
            </TabItem>
            </Tabs>

      

13. **评估结果如下所示。**
      <Tabs>
      <TabItem value="微调后模型" label="微调后模型评估">

            ![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/result1.png)

            ```python
            {
                  BLEU-4: 0.0469
                  ROUGE-1: 0.7839
                  ROUGE-2: 0.5865
                  ROUGE-L: 0.7696
            }
            ```
            结果解读：模型生成结果与参考答案在整体内容和结构上较为接近（ROUGE分数高），但在字面和短语精确匹配上还有提升空间（BLEU分数较低）。模型适合生成内容覆盖面广、结构合理的文本，但字面一致性有待加强。
     
      </TabItem>
      <TabItem value="原生模型评估" label="原生模型评估">
            ![参数设置1](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/medical/result2.png)

            ```python
            {
                  BLEU-4: 0.0032
                  ROUGE-1: 0.1911
                  ROUGE-2: 0.0641
                  ROUGE-L: 0.1874
            }
            ```
            结果解读：这些分数均处于较低水平，表明当前模型生成内容与参考答案在词汇、短语和句子结构层面匹配度较差，语言流畅性和结构准确性不足，生成质量有待提升。原生模型在医疗诊断任务上的表现较弱，建议通过微调提升模型专业性和生成能力。
      </TabItem>
      </Tabs>


微调后模型在医疗诊断任务上的生成质量远高于原生模型，内容覆盖、结构合理性和语义连贯性均有明显提升，更适合实际医疗场景应用。

## 总结
用户可通过LLaMA-Factory Online平台预置的模型及数据集完成快速微调与效果验证。从上述实践案例可以看出，基于`Llama3.1-70B-Instruct`模型，采用LoRA方法在`medical_o1_sft_Chinese_alpaca`医学诊断数据集上进行指令微调后，模型医学诊断和临床决策领域的专业能力有显著提升。

本实践为构建高质量的医学诊断和临床决策系统提供了可复用的技术路径，适用于智能辅助工具、医学教育中的虚拟病例交互、患者健康咨询机器人等场景。未来可以使用实际业务数据集，对模型进行微调，得到能够解决医疗领域实际问题的本地化大模型。


### 4.2 ⚖️ 法律：构建LightLLM+LlamaIndex法律智能体




:::info
LightLLM 是一款面向高效推理与轻量化部署的开源推理框架，其设计核心聚焦于在有限硬件资源上实现主流大语言模型的快速运行，通过高效内存管理与算子融合技术优化推理速度，具备良好的扩展性以快速集成不同规模模型并支持多种量化策略，适用于教学实验、小型企业部署及个人项目开发等多类场景。
:::

近年来，大语言模型（LLM）能力显著，但其知识的静态滞后性导致其在处理新信息或专业问题时易产生“幻觉”。RAG（retrieval-augmented generation，检索增强生成）框架通过“先检索后生成”的方式，将外部知识库的准确性与LLM的生成能力相结合，有效解决了这一问题，成为构建企业知识库、法律医疗等专业应用的核心技术。在此背景下，掌握以LightLLM为代表的高效推理框架，不仅能帮助学习者深入理解RAG的协同机制，更能培养在资源受限环境下优化与部署AI系统的关键能力，为投身AI应用开发奠定坚实基础。

#### 4.2.1 前提条件
* 用户已经获取LLaMA-Factory Online平台账户和密码，如果需要帮助或尚未注册，可参考[注册账户](/docs/documents/quickstart/loginAccount)完成注册。
* 当前账号的余额充裕，可满足实例运行的需要。点击可了解[最新的活动](/docs/documents/quickstart/freetrialguide)及[费用信息](/docs/documents/recharge/charging)，或前往<Anchor label="充值" href="/cost/recharge" />，如需了解更多请[联系我们](/docs/documents/recharge/refill#发票)。




#### 4.2.2 操作步骤
**配置概览**

| 配置参数 |  配置项    |  说明|
| --- |:----------|----| 
| 模型 |Qwen3-8B |本实践通过本地启动大模型服务 |
|GPU| H800*1  | -                    | |

**操作详情**


1. 使用已注册的LLaMA-Factory Online账号登录平台，选择[实例空间]菜单项，进入实例空间页面，例如下图所示。

     ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/startWT.png)

2. 单击上图“开始微调”按钮，进入[配置资源]页面，选择GPU资源，卡数填写`1`，其他参数保持为默认值，单击“启动”按钮，启动实例。

      ![SwanLab对比图](http://s1.llamafactory.online/lmlab/docs/v1.0/paper/1.png) 

3. 实例启动后，可启动VSCode或者JupyterLab专属数据处理，本次实践我们使用VSCode专属数据处理，单击图标链接进入数据处理页面，点击[Terminal/New Terminal]进入终端页面。在终端运行如下所示的命令安装服务运行所需环境。
 
    1. 运行如下命令创建一个名称为“py39”的Python环境。
     ```shell
     conda create -n lightllm python=3.10
     ```
     2. 运行如下所示的命令激活自定义的环境。
     ```shell
     conda activate lightllm
     ```
     3. 运行如下所示的命令，在已经激活的Python环境中安装`ipykernel`包。
      ```shell
      pip install ipykernel -i https://pypi.tuna.tsinghua.edu.cn/simple
      ```
     4. 运行如下所示的命令，将当前Python环境注册为Jupyter内核。

      ```bash
      kernel_install --name lightllm  --display-name "Python 3.10 (lightllm)"
      ```
     :::tip
     您需在对应的Conda环境中运行上述注册命令，否则内核（kernel）将无法正确注册。
     :::
4. Python环境安装完成后，在终端运行如下所示的命令下载并安装LightLLM框架及其依赖项，例如下图所示。

     1. 运行如下命令，从GitHub仓库克隆LightLLM项目的源代码到本地。
     ```shell
     git clone https://github.com/ModelTC/lightllm.git
     ```
     2. 运行如下所示的命令，进入项目目录。
     ```shell
     cd lightllm
     ```
     3. 运行如下所示的命令，安装项目运行所需的所有Python依赖包。
      ```shell
      pip install -r requirements.txt
      ```
     4. 运行如下所示的命令，以开发模式安装LightLLM包。
      ```shell
      python setup.py install
      ```
     5. 运行如下命令，安装LlamaIndex库，用于构建基于私有数据的检索增强生成（RAG）应用。
     ```shell
     pip install llama-index
     ```
    6. 运行如下所示的命令，安装Streamlit库，用于快速创建交互式数据可视化网页应用。
     <!-- ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/2.png)  -->
     ```shell
     pip install streamlit
     ```
   
   
   7. 运行如下命令，安装LightLLM运行所需的计算机视觉处理库和WebSocket通信支持依赖包。
     ```shell
     pip install torchvision
     pip install websockets
     pip install partial-json-parser
     pip install interegular
     ```
    8. 运行如下命令，安装LlamaIndex框架对HuggingFace本地嵌入模型的支持包，使RAG应用能够使用BAAI/bge等本地嵌入模型进行文本向量化。
    ```shell
    pip install llama-index-embeddings-huggingface
    ```
    
       
    您可运行`pip list | grep -E "torchvision|websockets|partial-json-parser|interegular|streamlit|llama-index|llama-index-embeddings-huggingface"`命令查看依赖包安装情况，例如下图所示。
     
     ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/8.png)
    

5. 依赖包安装完成后，运行如下所示命令，启动LightLLM框架的API服务器提供标准的OpenAI兼容接口，支持文本生成、对话等推理功能，模型加载成功后，页面显示例如下图所示，默认端口为8000。
   
   ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/lanunchser.png)

      ```shell
      python -m lightllm.server.api_server --model_dir /shared-only/models/Qwen/Qwen3-8B
      ```

  
    
6. 新建Terminal窗口，执行如下代码测试模型推理功能是否正常，服务正常启动页面例如下图所示。

    ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/result.png)
        ```json
        curl -X POST "http://localhost:8000/v1/chat/completions" \
          -H "Content-Type: application/json" \
          -d '{
            "model": "Qwen3-8B",
            "messages": [{"role": "user", "content": "Hello"}],
            "max_tokens": 1000
          }'
        ```

7. 点击链接[下载app.py](http://llamafactory-online-assets.oss-cn-beijing.aliyuncs.com/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/app.py)，下载后将文件拖拽至`/workspace/lightllm`目录下，然后运行如下所示命令加载RAG服务，加载成功后页面例如下图所示。

    ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/3.png)

      ```shell
      streamlit run app.py \
          --server.address 0.0.0.0 \
          --server.port 6666 \
          --server.enableCORS false \
          --server.enableXsrfProtection false
      ```

8. 点击服务链接，进入本地RAG文档搜索助手页面，例如下图所示。

    ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/44.png)

#### 4.2.3 服务应用
服务部署完成后，用户可自定义构建知识库，并基于库内信息查询相关问题，快速获取解决方案。
<Tabs>
<TabItem value="Windws" label="RAG服务回复" >

单击上方高亮区域的“Browse files”按钮，选择PDF或TXT格式的知识库文件上传，或直接将文件拖拽至该区域，本实践上传[示例1](http://llamafactory-online-assets.oss-cn-beijing.aliyuncs.com/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/%E5%8A%B3%E5%8A%A8%E6%B3%95%E6%9D%A1%E4%BE%8B1.txt)，[示例2](http://llamafactory-online-assets.oss-cn-beijing.aliyuncs.com/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/%E5%8A%B3%E5%8A%A8%E6%B3%95%E6%9D%A1%E4%BE%8B2.txt)。

待页面出现“✅文档已加载并索引！”即可输入您的问题，例如：“ 我有考勤记录截图和与领导沟通加班的微信记录。劳动合同写的是月薪8000元，但公司一直按基本工资3000元为基数算加班费。我主张最近一年的。”。观察应用回复，例如下图所示，回答详情可展开“Details”查看详情。

     ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/5.png)

import Details from '@theme/Details';


<details>
<summary>回答</summary>

&lt;think&gt;

好的，用户上个月离职了，但公司还欠他很多加班费没给，他不知道该怎么办。首先，我需要根据提供的上下文信息来给出建议。用户之前已经有一些相关的提问和回答，比如关于加班费计算基数、仲裁申请书的撰写以及电子签字的补强方法。现在的情况是用户已经离职，但仍有未支付的加班费。

首先，用户需要确认自己是否已经收集了足够的证据。根据之前的对话，用户有加班考勤记录、工资条和劳动合同。这些证据是关键，因为仲裁需要这些来支持主张。用户可能还需要检查是否有其他证据，比如微信聊天记录、邮件或者工作群里的消息，这些可以证明加班的事实和公司的安排。

接下来，用户需要考虑是否已经超过了仲裁时效。根据中国的劳动法，劳动争议的仲裁时效是一年，从知道或应当知道权利被侵害之日起计算。用户上个月离职，如果加班费是在离职前发生的，那么时效可能还未超过。但需要确认具体的时间点，比如最后加班的时间和离职时间之间的关系。

然后，用户应该准备仲裁申请。根据之前的建议，需要整理好证据，包括考勒记录、工资条、劳动合同等，并计算加班费的差额。用户可能需要计算按照劳动合同约定的工资基数计算的加班费，与公司实际支付的差额，并准备具体的金额。同时，用户需要撰写仲裁申请书，明确请求支付的金额和理由。

另外，用户提到之前有电子审批单的问题，但现在已经离职，可能需要确保电子证据的合法性。比如，如果电子签字被质疑，用户需要准备系统登录方式或关联的微信记录来补强证据。不过，现在可能已经没有机会在庭上展示，所以需要提前准备好这些材料，以防仲裁员提出质疑。

用户可能还需要考虑是否需要律师的帮助，尤其是如果公司有较强的法律团队，或者案件复杂的情况下。但根据之前的建议，用户可以自行准备材料，因为证据充分的话，法律优势明显。

最后，用户需要按照仲裁程序提交申请，确保在规定的时效内，并且正确填写所有必要的文件。如果仲裁结果不满意，还可以考虑诉讼，但诉讼的时效是两年，需要留意时间节点。

总结下来，用户需要做的步骤包括:确认证据完整性，计算加班费差额，准备仲裁申请材料，确保时效性，并可能寻求法律援助。同时，注意不要遗漏任何可能的证据，如电子记录的合法性，以及离职后是否还有追诉权。

&lt;think&gt;

你的考勤记录截图和微信沟通记录已能形成完整证据链，可明确主张以实际月工资 8000 元为基数计算加班费。具体操作建议如下：

你已离职但仍有加班费未支付，可按以下步骤维权:

1. **确认证据完整性**：整理所有加班记录(考勤截图、工资条、劳动合同、微信/邮件沟通记录等)，确保能证明加班事实及公司未足额支付。

2. **计算加班费差额**：以劳动合同约定的工资为基数(如月薪7000元)，按法定标准(工作日1.5倍、周末2倍、节假日3倍)计算应得加班费，核对已支付金额，明确差额。

3. **申请劳动仲裁**：在离职后1年内向公司所在地劳动仲裁委员会提交申请，明确请求支付欠付的加班费差额。需准备证据清单及仲裁申请书，说明事实与理由。

4. **应对电子证据质疑**：若存在电子审批单等电子证据，可提前准备系统操作记录或关联聊天记录(如领导微信确认加班内容)，证明其真实性。

5. **时效与后续程序**：劳动仲裁时效为1年，离职后仍可主张。若仲裁结果不满意，可在15日内向法院提起诉讼。

建议尽快行动，避免超过时效，同时保持证据链完整以提高胜诉率。
</details>




</TabItem>


 <TabItem value="Lix" label="原生模型回复" >

应用步骤6命令，修改content内容为：“我有考勤记录截图和与领导沟通加班的微信记录。劳动合同写的是月薪8000元，但公司一直按基本工资3000元为基数算加班费。我主张最近一年的。”观察模型回答，例如下图所示，回答详情可展开“Details”查看详情。
    ```json
    curl -X POST "http://localhost:8000/v1/chat/completions" \
          -H "Content-Type: application/json" \
          -d '{
            "model": "Qwen3-8B",
            "messages": [{"role": "user", "content": " 我有考勤记录截图和与领导沟通加班的微信记录。劳动合同写的是月薪8000元，但公司一直按基本工资3000元为基数算加班费。我主张最近一年的。"}],
            "max_tokens": 1000
          }'
    ```

     ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/lightLLMagent/66.png)


<details>
<summary>回答</summary>

&lt;think&gt;\n嗯，用户上个月离职了，但公司还欠他很多加班费没给，他不知道该怎么办。首先，我需要了解他可能的情况和需求。他可能已经尝试过和公司沟通，但没有得到解决，所以现在需要法律上的帮助。我应该先确认他的基本情况，比如公司名称、离职时间、加班费的金额，以及是否有相关证据，比如考勤记录、工资单、聊天记录等。这些证据对他后续维权很重要。\n\n接下来，用户可能不了解具体的法律程序，所以需要详细说明步骤。比如是否需要先与公司协商，如果失败再申请劳动仲裁。此外，他可能担心仲裁的费用和时间，需要提醒他劳动仲裁是免费的，而且有时间限制，比如一年内必须申请，所以要尽快处理。\n\n还要考虑用户可能的问题，比如公司不承认加班时间，或者不愿意支付，这时候需要建议他收集证据，比如同事的证词、电子邮件、打卡记录等。另外，如果公司是有限责任公司，可能可以考虑起诉法人代表或股东，但这可能比较复杂，需要专业律师的帮助。\n\n用户可能还担心自己的权益是否能被保障，特别是如果公司不配合的话。这时候需要强调法律的强制力，以及劳动仲裁的结果是终局性的，可以申请法院强制执行。另外，提醒他注意保留所有沟通记录，避免证据丢失。\n\n最后，要给出一个清晰的步骤指南，从收集证据到协商，再到仲裁或诉讼，确保用户知道每一步该怎么做，并且鼓励他不要放弃，及时行动。可能还需要提醒他不要采取过激行为，以免影响维权。\n&lt;think&gt;\n\n如果你因为公司拖欠加班费而离职，但公司仍未支付欠薪，你有合法权利维护自己的权益。以下是具体的解决步骤和建议：\n\n---\n\n### **一、确认你的权益**\n根据《劳动法》《劳动合同法》和《工资支付暂行规定》，劳动者因工作时间、加班工资、经济补偿等与用人单位发生争议的，可依法维权。公司拖欠加班费属于违法行为。\n\n---\n\n### **二、收集证据**\n这是关键，确保你的诉求有据可依：\n1. **劳动合同**：确认加班费的约定（是否有明确的加班工资标准）。\n2. **考勤记录**：如打卡记录、工时统计表、电子考勤系统截图等。\n3. **工资单/银行流水**：证明你的基本工资和实际发放情况（若已发部分工资，需区分正常工资与加班费）。\n4. **加班证据**：\n   - 工作群/邮件等电子记录（如提到加班、加班时长或工作内容）；\n   - 同事或主管的证人证言（可书面或录音）；\n   - 公司安排加班的通知、排班表等。\n5. **离职证明或通知书**：证明你已离职。\n6. **沟通记录**：与公司协商欠薪的聊天记录、通话记录等。\n\n---\n\n### **三、第一步：协商**\n先尝试与公司协商解决：\n1. **书面沟通**：通过微信、邮件或挂号信等方式要求公司支付欠薪，并保留证据。\n2. **报警处理**：若公司拒不支付，可向当地公安机关报案，但需注意：除非情节严重（如恶意拖欠导致生活困难），否则警方可能不予受理。\n\n---\n\n### **四、第二步：申请劳动仲裁**\n如协商无果，需在**离职后一年内**（注意时效）向劳动仲裁委员会申请仲裁：\n1. **准备材料**：\n   - 仲裁申请书（注明申请人、被申请人、仲裁请求、事实与理由）；\n   - 身份证复印件；\n   - 证据清单及复印件（如劳动合同、考勤记录等）；\n   - 如果公司未签劳动合同，需保留相关证据（如社保缴纳记录、工资支付凭证等）。\n2. **提交仲裁**：向公司所在地或劳动合同履行地的劳动仲裁委员会提交材料。  \n3. **仲裁流程**：一般30-60天内出结果，劳动仲裁是维权的前置程序，如不服可向法院起诉。\n\n---\n\n### **五、仲裁后的处理**\n1. **仲裁裁决**：若仲裁支持你的诉求，公司需在指定时间内支付欠薪；若公司拒不履行，可向法院申请强制执行。\n2. **起诉**：若仲裁结果不满意，可在15日内向法院提起诉讼，诉讼时效为2年。\n3. **法院强制执行**：若公司仍不支付，可申请法院强制执行工资，法院可冻结公司账户、查封财产等。\n\n---\n\n### **六、特殊注意事项**\

</details>


  </TabItem>

</Tabs>
多维度对比两类服务的回复，对比详情如下所示。

| 对比维度 | RAG服务回复 |原生模型回复 |
|---------|------------------|--------------------|
| **1. 计算基数主张** | ✅ 明确指出应以实际发放工资8000元为基数，并提出反驳策略：“工资条未列明加班费项目”、“银行流水为证” | ⚠️ 仅说明“可能存在违法风险”，未给出具体举证方式或反制话术 |
| **2. 证据链构建** | ✅ 强调“考勤截图+微信记录”需相互印证，并建议“当庭登录系统验证真实性”——这是关键实操技巧 | ❌ 仅说“需能证明具体时间”，缺乏操作指引 |
| **3. 加班事实认定** | ✅ 点出核心：“加班系公司安排而非自愿”——这是仲裁胜负关键点之一 | ❌ 未涉及此关键法律要件 |
| **4. 差额计算方法** | ✅ 明确列出倍数标准（工作日1.5倍、周末2倍、法定节假日3倍），要求精确核算差额金额 | ❌ 只提法律规定，无具体计算指导 |
| **5. 应对公司抗辩** | ✅ 预判对方可能说“3000是基本工资”，并提供标准反驳话术：“劳动合同未约定工资构成”、“公司未提供薪酬制度证明”，应视为未足额支付 | ❌ 虽提到地方差异，但未教用户如何应对企业常见推诿 |
| **6. 仲裁请求撰写** | ✅ 明确写出应提交的仲裁请求模板，包括诉求金额、法律依据（《劳动法》第44条、《工资支付暂行规定》第13条） | ❌ 仅有知识介绍，无行动指令 |
| **7. 用户角色定位** | ✅ 把用户当作即将出庭的申请人，给予战斗性、可执行的诉讼策略指导 | ❌ 把用户当作学习者，进行普法教育式的内容输出 |

原生模型回复偏重知识普及，无法提供具体的指导。RAG服务回复更具实战性、对抗性和可操作性，适合作为智能法律助手的输出标准，把用户当作即将出庭的申请人，给予战斗性、可执行的诉讼策略指导。




#### 4.2.4 总结
通过“轻量级RAG验证 → 功能迭代优化 → 企业级部署落地”的渐进式路径，我们基于LightLLM高效推理引擎，构建了一套可落地的智能知识服务系统。该方案有效缓解了大模型在专业领域存在的知识滞后与幻觉问题，可在法律场景中实现从法条检索、判例匹配到案情分析的全流程应用，显著提升响应准确率与业务可用性。

实践验证了该架构在高精度、强合规性场景下的可行性，具备向金融、医疗、政务等知识密集型行业快速复制的能力。后续将持续深化多模态内容理解、复杂任务编排与安全可控机制，推动RAG技术与企业业务流程深度融合，打造稳定、可解释、可扩展的行业AI解决方案，真正实现人工智能在核心业务场景中的价值闭环。

### 4.3 💰 金融：构建基于Llama-3.2-11B-Vision-Instruct金融分析


在金融领域中，图表承担了展示信息的重要责任，而读取图表中所包含的信息对每位金融从业者更是必不可少的机能。如今，金融分析愈发成熟，我们每天需要处理的图标也随之越来越复杂，此时AI的引入能够帮助我们更有效更快速地抓取关键信息在金融领域中，图表承担了展示信息的重要责任，而读取图表中所包含的信息对每位金融从业者更是必不可少的机能。如今，金融分析愈发成熟，我们每天需要处理的图表也随之越来越复杂，此时AI的引入能够帮助我们更有效更快速地抓取关键信息。

金融特化版Llama-3.2-11B-Vision-Instruct是针对金融分析与决策优化的多模态模型，能够同时处理文本、数据和视觉输入（如财报、市场指标、K线图等），输出包括财务分析、风险提示、趋势解读等结构化洞察。其应用涵盖自动化投研（快速提取关键财务信号）、合规审查（识别文档风险点）及可视化报告生成（结合图表与文本分析），通过金融领域语料和视觉数据的针对性微调，提升了处理复杂金融信息的准确性与实用性，适用于投行、资管和企业财务等场景。

### 4.3.1 前提条件
* 用户已经获取LLaMA-Factory Online平台账户和密码，如果需要帮助或尚未注册，可参考[注册账户](/docs/documents/quickstart/loginAccount)完成注册。
* 当前账号的余额充裕，可满足模型微调服务的需要。点击可了解[最新的活动](/docs/documents/quickstart/freetrialguide)及[费用信息](/docs/documents/recharge/charging)，或前往<Anchor label="充值" href="/cost/recharge" />，如需了解更多请[联系我们](/docs/documents/recharge/refill#发票)。



### 4.3.2 操作步骤
**配置概览**

| 配置参数 |  配置项  | 是否预置         |  说明|
| --- |:----------|:----------------------|----| 
| 模型 | Llama3.2-11B-Vision-Instruct     | 是| 经过指令微调，参数量约110亿 (11B)，专为处理图像和文本数据的任务而设计。 |
| 数据集| fin_mme |  是     |金融多模态推理评估基准数据集。|
|GPU| H800*4（推荐）  | -                       |H800**2（最少）。 |
|微调方法|lora|-|显著降低计算与存储成本，兼具高性能与部署灵活性。|



**操作详情**

<!-- LLaMA Factory Online支持通过实例模式和任务模式运行微调任务，不同模式下的微调/评估操作详情如下所示。

<Tabs>
  <TabItem value="任务模式微调" label="任务模式微调" >

1. 进入LLaMA-Factory Online平台，点击“控制台”，进入控制台后点击左侧导航栏的“模型微调”进入页面。

2. 选择基础模型和数据集，进行参数配置。

      * 本实践使用平台内置的Llama-3-8B-Instruct作为基础模型，数据集为平台内置的`haruhi_train`。
      * 资源配置。推荐卡数为4卡。
      * 选择价格模式。本实践选择“极速尊享”，不同模式的计费说明参考[计费说明](docs/documents/guide/Train/train1.md#操作步骤)。
      * 开始训练。点击“开始训练”按钮，开始模型训练。 

    ![开始微调](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/01.png)
  

     :::tip 
      配置模型与数据集后，系统将根据所需资源及其相关参数，动态预估任务运行时长及微调费用，您可在页面底部查看预估结果。
     :::

3. **通过任务中心查看任务状态。**  在左侧边栏选择”任务中心“，即可看到刚刚提交的任务。可以通过单击任务框，可查看任务的详细信息、超参数、训练追踪和日志。
    ![开始微调](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/02.png)
    ![开始微调](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/03.png)

4. 任务完成后，模型自动保存在"文件管理->模型->output"文件夹中。可在"任务中心->基本信息->模型成果"处查看保存路径。

    ![开始微调](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/04.png)

5. **进行模型评估。** 点击页面左侧导航栏“模型评估”，进行评估训练配置。
    微调模型选择上一步骤微调后的模型，评估数据集平台内置的测试集：`haruhi_val`。其他参数设置为默认。

    ![模型评估](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/05.png)

6. 可以在“任务中心->模型评估”下看到评估任务的运行状态。

     ![开始微调](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/06.png)
     :::tip 
      配置模型与数据集后，系统将根据所需资源及其相关参数，动态预估任务运行时长及微调费用，您可在页面底部查看预估结果。
     :::
7. 点击![swanapi](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/70.png)图标，进入任务基本信息查看页面。用户可查看评估任务的基本信息、日志以及评估结果。

  </TabItem>

 <TabItem value="实例模式微调" label="实例模式微调" > -->

1. 使用已注册的LLaMA Factory Online账号登录平台，选择[实例空间]菜单项，进入实例空间页面，例如下图所示。

     ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/startWT.png)

2. 单击上图“开始微调”按钮，进入[配置资源]页面，选择GPU资源，卡数填写`4`，其他参数保持为默认值，例如下图所示。

     ![SwanLab对比图](http://s1.llamafactory.online/lmlab/docs/v1.0/paper/2.png) 
     
3. 单击“启动”按钮，待实例启动后，点击[LLaMA-Factory快速微调模型]页签，进入LLaMA-Factory Online在线WebUI微调配置页面，语言选择`zh`，如下图高亮①所示；模型名称选择`Llama-3.2-11B-Vision-Instruct`，如下图高亮②所示；系统默认填充模型路径`/shared-only/models/meta-llama/Llama-3.2-11B-Vision-Instruct`。

4. 微调方法选择`lora`，如下图高亮④所示；选择“train”功能性，训练方式保持`Supervised Fine-Tuning`，如下图高亮⑤所示；数据路径保持`/workspace/llamafactory/data`，如下图高亮⑥所示；数据集选择平台已预置的`fin_mme`，如下图高亮⑦所示。

    ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/parameter.png)

点击“设置/常规”，即可获取API Key，示例如下图所示。
[SwanLab](https://www.swanlab.cn/)是一款开源且轻量级的AI模型训练可视化追踪工具。在本次微调任务中，我们将使用SwanLab记录整个微调过程。在开始之前，请确保您已经登录[SwanLab](https://www.swanlab.cn/)平台。登录后，点击“设置/常规”，即可获取API Key，示例如下图所示。



5. （可选）其余参数可根据实际需求调整，具体说明可参考[参数说明](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html)，本实践中的其他参数均保持默认值。


6. 参数配置完成后，点击“开始”按钮启动微调任务。页面底部将实时显示微调过程中的日志信息，例如下图高亮①所示；同时展示当前微调进度及Loss变化曲线。经过多轮微调后，例如下图高亮②所示，从图中可以看出Loss逐渐趋于收敛。微调完成后，系统提示“训练完毕”，例如下图高亮③所示。
      
       ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/loss.png)


<Tabs>
  <TabItem value="Windows" label="微调后模型对话" >


7. 切换至“chat”界面，如下图高亮①所示；选择上一步骤已经训练完成的检查点路径，如下图高亮②所示；单击“加载模型”按钮，微调的模型加载后，在系统提示词处填入提示词，如下图高亮③所示；上传图片后输入用户模拟词“Describe the overall change in trend of revenue presented in the chart”，观察模型回答，如下图高亮⑤所示。

      ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/fine.png)



</TabItem>


 <TabItem value="Liux" label="原生模型对话" >

7. 清空“检查点路径”中的LoRA配置，单击下图高亮②所示的“卸载模型”按钮，卸载微调后的模型，模型卸载完成后，单击“加载模型”按钮，加载原生的`Llama-3.2-11B-Vision-Instruct`模型进行对话，其余配置保持不变。用户模拟词依旧输入“Describe the overall change in trend of revenue presented in the chart”，观察模型回答，如下图高亮⑤所示。

      ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/rawchat.png)

 </TabItem>

</Tabs>


通过对比微调模型与原生模型的输出结果可以发现，微调后的模型在图表读取上表现出更强的契合度，其回答不仅更贴近系统预设的角色定位，也更符合用户的认知预期。

<Tabs>
  <TabItem value="Window" label="微调后模型评估" >

8. 切换至“Evaluate & Predict”页面，选择微调后模型的检查点路径，例如下图高亮①所示；然后选择平台预置的`fin_mme`数据集，并根据实际需求配置评估参数（本实践的参数设置如下图所示）。


     ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/evaluate.png)

9. 参数配置完成后，点击“开始”按钮即可启动评估，页面底部将实时显示评估过程中的日志信息，评估完成后，记录评估结果，结果如下所示。

          ```shell
         {
            "predict_bleu-4": 27.38479177927928,
            "predict_model_preparation_time": 0.0082,
            "predict_rouge-1": 46.59722225225225,
            "predict_rouge-2": 0.07882882882882883,
            "predict_rouge-l": 46.59722225225225,
            "predict_runtime": 475.4967,
            "predict_samples_per_second": 18.673,
            "predict_steps_per_second": 2.334
          }
          ```
      结果解读：该模型在语义覆盖（`ROUGE-1`）、句法连贯性（`ROUGE-L`）和短语匹配（`ROUGE-2`）方面均有良好表现，相较原生模型评估`BLEU-4=6.66`，有了明显改进，该版本已具备使用基础。
  
  </TabItem>
  <TabItem value="Lix" label="原生模型评估" >

8. 切换至“Evaluate & Predict”页面，清空检查点路径配置，数据集依旧选择平台预置的`fin_mme`数据集，并根据实际需求配置评估参数（本实践的参数设置如下图所示）。


      ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/raw.png)
9. 完成配置后，点击“开始”按钮即可启动评估，页面底部将实时显示评估过程中的日志信息，评估完成后，记录评估结果，结果如下所示。

          ```shell
         {
            "predict_bleu-4": 6.658312105855856,
            "predict_model_preparation_time": 0.0082,
            "predict_rouge-1": 16.22191519144144,
            "predict_rouge-2": 0.0002194144144144144,
            "predict_rouge-l": 15.523695371621622,
            "predict_runtime": 12083.6165,
            "predict_samples_per_second": 0.735,
            "predict_steps_per_second": 0.092
          }
          ```
     结果解读：评价指标（`BLEU-4`、`ROUGE-1/2/L`）均处于较低水平，尤其是`BLEU-4`和 `ROUGE-2`，表明模型生成内容与参考答案在词汇、短语和句子结构层面匹配度较差。当前模型的评估得分整体偏低，尤其在语言流畅性与结构准确性方面表现较弱，生成质量有进一步提升。

  </TabItem>

</Tabs>

对比微调后模型评估与原生模型评估结果可以看出，二者在生成质量方面存在显著差异。原生模型表现较差，不仅生成质量相关指标全面偏低（`BLEU-4: 6.66`，`ROUGE-1: 16.22`，`ROUGE-2:0.0002`,`ROUGE-L:15.52`），其在生成内容与参考答案在词汇、短语及句子结构层面匹配度低，语言连贯性和语义准确性不足。而微调后模型在相同评估条件下有明显提升，生成质量上，`BLEU-4:27.38`，`ROUGE-1:46.60`，`ROUGE-2`和`ROUGE-L`也分别达到`0.0079`和`46.60`，显示出更优的关键词覆盖能力、短语搭配合理性和句级语义连贯性。表明微调后的模型显著增强了语言生成质量。综上，微调后的模型生成性能上远优于原生模型，具备更好的应用潜力。

 <!-- </TabItem>

</Tabs> -->

## 总结
用户可通过LLaMA-Factory Online平台预置的模型及数据集完成快速微调与效果验证。从上述实践案例可以看出，基于`Llama3.2-11B-Vision-Instruct`模型，采用LoRA方法在`fin_mme`金融数据集上进行指令微调后，模型在处理文本、数据和视觉输入（如财报、市场指标、K线图等），输出包括财务分析、风险提示、趋势解读等结构化洞察等方面均有显著提升。

本实践构建自动化投研、合规审查、可视化报告生成系统提供了可复用的技术路径，适用于投行、资管和企业财务等场景。未来可进一步探索多模态输入输出、强化实时数据联动能力、优化长周期时序分析能力，构建行业细分场景的适配性，增强模型的可解释性和可追溯性。

### 4.4 🏭 工业：微调基于Qwen2.5-VL-7B-Instruct的自动驾驶辅助模型

智能驾驶领域正经历从“感知智能”向“认知智能”的深刻演进。在复杂多变的真实交通环境中，自动驾驶系统不仅需要精准识别车道线、车辆、行人等视觉信息，更需理解场景语义、响应自然语言指令，并做出符合安全逻辑与交通规则的决策。传统模块化系统（感知-规划-控制）在应对模糊或突发场景时往往缺乏灵活推理能力，而融合视觉与语言理解的多模态大模型为此提供了突破性解决方案。

`Qwen2.5-VL-7B-Instruct`作为支持VLA（Vision-Language-Action）架构的先进模型，在智能驾驶领域展现出强大潜力。它能够接收来自车载摄像头的实时图像或视频流作为视觉输入，同时理解驾驶员或车载系统发出的自然语言指令（如“前面堵车了，能换条路吗？”或“右转车道有自行车，帮我等一下”），实现对环境与意图的联合建模。基于对交通状况的全面理解，模型可生成结构化、可执行的操作建议。

#### 4.4.1 前提条件

* 用户已经获取LLaMA-Factory Online平台账户和密码，如果需要帮助或尚未注册，可参考[注册账户](/docs/documents/quickstart/loginAccount)完成注册。
* 当前账号的余额充裕，可满足模型微调服务的需要。点击可了解[最新的活动](/docs/documents/quickstart/freetrialguide)及[费用信息](/docs/documents/recharge/charging)，或前往<Anchor label="充值" href="/cost/recharge" />，如需了解更多请[联系我们](/docs/documents/recharge/refill#发票)。
* 在本次微调任务中，我们将使用SwanLab记录整个微调过程。请您访问[SwanLab](https://www.swanlab.cn/)并完成注册。


#### 4.4.2 操作步骤
**配置概览**

| 配置参数 |  配置项  | 是否预置         |  说明|
| --- |:----------|:----------------------|----| 
| 模型 |Qwen2.5-VL-7B-Instruct    | 是| 经过指令微调，7B参数多模态模型，擅图文理解与生成。 |
| 数据集| QA_from_CoVLA_zh |  是     |智能驾驶适合多模态场景的数据集。|
|GPU| H800*4（推荐）  | -                       |H800**1（最少）。 |
|微调方法|lora|-|显著降低计算与存储成本，兼具高性能与部署灵活性。|


#### 4.4.3 操作详情

LLaMA-Factory Online支持通过实例模式和任务模式运行微调任务，不同模式下的微调/评估操作详情如下所示。
1. 登录[SwanLab](https://www.swanlab.cn/)后，选择[设置/常规]菜单项，下滑主页面到找到[开发]模块，获取SwanLab的API Key，例如下图所示。

     ![swanapi](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/usercenter/swanapi.png)

<Tabs>
<TabItem value="Widws" label="任务模式" >



2. 登录已注册的<a href={`https://www.llamafactory.online${getSource()}`} target="_blank">LLaMA-Factory Online</a>平台，切换至[微调/模型微调]菜单项，进入模型微调配置页面。点击页面上方的“配置SwanLab API Key”链接，例如下图所示，进入用户中心页面。

      ![swanapi](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/64.png)

3. 在用户中心页面的“访问管理”模块中，将第一步获取的`SwanLab API Key`填入对应字段，点击“校验”按钮。若状态显示“校验成功”，连接SwanLab成功，例如下图所示。
  
     ![swanapi](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/65.png)

4. 返回“微调/模型微调”菜单项，进入模型微调参数配置页面，本实践参数配置详情如下所示。
     
     ![swanapi](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/67.png)


  * `基础模型`选择`Qwen2.5-VL-7B-Instruct`，如下图高亮①所示。
  * `训练数据-公共数据`选择`QA_from_CoVLA_zh`，如下图高亮②所示。
  * `截断长度`修改为`2048`，如下图高亮③所示。
  * `资源规格`配置GPU卡数为4，如下图高亮④所示。
  * 价格优惠本实践中选择“极速尊享”，如下图高亮⑤所示，您可按需进行选择。
  * 其余参数可根据实际需求调整，具体说明可参考[参数说明](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html)，本实践中的其他参数均保持默认值。

  :::tip 
配置模型与数据集后，系统将根据所需资源及其相关参数，动态预估任务运行时长及微调费用，您可在页面底部查看预估结果。
  :::
5. 参数配置完成后，单击“开始训练”按钮，进入[任务中心/模型微调]列表页面，以查看任务的运行状态，例如下图高亮①所示。

   ![swanapi](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/68.png)

6. 点击<img src="https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Guide/center/train-manage/详情3.png" alt="详情" width="20" height="20"/>图标，进入微调任务信息查看页面，切换至“训练追踪”Tab页，单击“立即进入”按钮，例如下图所示，进入SwanLab训练追踪页面。
   
    ![swanapi](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/71.png)

7. 您在SwanLab训练追踪页面可以查看：损失曲线图（train/loss）、学习率变化图（train/learning_rate）、评估指标图（eval/accuracy等）、GPU使用率图、显存占用图、系统资源监控图、超参数面板及自定义图表（如混淆矩阵、ROC曲线）等，全面可视化训练过程，例如下图所示。

    ![swanapi](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/72.png)
      :::note  
      任务状态流转至“运行中”之后，您可开始查看训练追踪图表。

      :::
8. 切换至LLaMA-Factory Online的“微调/任务中心/模型微调”列表页面，点击<img src="https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Guide/center/train-manage/详情3.png" alt="详情" width="20" height="20"/>图标，在[基本信息]页面单击“评估”按钮，进入模型评估页面，选择微调时的数据集，例如下图高亮①所示，其余评估参数保持默认即可。


    ![swanapi](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/vla/76.png)

9. 参数配置完成后，单击“开始评估”按钮，进入[任务中心/模型评估]列表页面，以查看任务运行状态，例如下图所示。

      ![swanapi](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/vla/77.png)

  :::tip 
   配置模型与数据集后，系统将根据所需资源及其相关参数，动态预估任务运行时长及微调费用，您可在页面底部查看预估结果。
  :::
10. 点击<img src="https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Guide/center/train-manage/详情3.png" alt="详情" width="20" height="20"/>图标，进入任务基本信息查看页面。用户可查看评估任务的基本信息、日志以及评估结果。
</TabItem>


 <TabItem value="Lx" label="实例模式" >

2. 登录已注册的<a href={`https://www.llamafactory.online${getSource()}`} target="_blank">LLaMA-Factory Online</a>平台，选择[实例空间]菜单项，进入实例空间页面，例如下图所示。

     ![SwanLab对比图](http://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/roleplay/startWT.png)

3. 单击上图“开始微调”按钮，进入[配置资源]页面，选择GPU资源，卡数填写`4`，其他参数保持为默认值，例如下图所示。

      ![SwanLab对比图](http://s1.llamafactory.online/lmlab/docs/v1.0/paper/2.png)  

4. 单击“启动”按钮，待实例启动后，点击[LLaMA-Factory快速微调模型]页签，进入LLaMA Factory Online在线WebUI微调配置页面，本实践参数配置详情如下所示。
* `语言`选择`zh`，如下图高亮①所示。
* `模型名称`选择`Qwen2.5-VL-7B-Instruct`，如下图高亮②所示。系统默认填充模型路径`/shared-only/models/Qwen/Qwen2.5-VL-7B-Instruct`。

* `微调方法`选择`lora`，如下图高亮④所示。
* 选择“train”功能，`训练方式`保持`Supervised Fine-Tuning`，如下图高亮⑤所示。
* `数据路径`保持`/workspace/llamafactory/data`，如下图高亮⑥所示。
* `数据集`选择平台已预置的`QA_from_CoVLA_zh`，如下图高亮⑦所示。
* `SwanLab参数设置`勾选`使用Swanlab`，例如下图高亮⑧所示。
* `SwanLab API秘钥`填写已获取API Key，例如下图高亮⑨所示。
* 其余参数可根据实际需求调整，具体说明可参考[参数说明](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html)，本实践中的其他参数均保持默认值。
      
      ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/vla/2.png)

5. 参数配置完成后，点击“开始”按钮启动微调任务。页面底部将实时显示微调过程中的日志信息，当页面弹出`SwanLab链接`访问链接时，例如下图所示。
     ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/vla/3.png)
   点击链接即可查看模型的训练追踪图表，例如下图所示。
      ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/vla/4.png)

6. 微调页面底部同时展示当前微调进度及Loss变化曲线。经过多轮微调后，例如下图高亮②所示，从图中可以看出Loss逐渐趋于收敛。微调完成后，系统提示“训练完毕”，例如下图高亮③所示。
      
       ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/finance/loss.png)


<Tabs>
<TabItem value="Windows" label="微调后模型对话" >


7. 切换至“chat”界面，如下图高亮①所示；选择检查点路径，如下图高亮②所示；单击“加载模型”按钮，微调的模型加载后，在系统提示词处填入提示词，如下图高亮④所示；上传[素材图片](https://llamafactory-online-assets.oss-cn-beijing.aliyuncs.com/llamafactory-online/docs/v2.0/documents/Practice/item.png)后输入用户模拟词“自动驾驶车辆应该关注哪些风险？”，观察模型回答，如下图高亮⑥所示。

    ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/vla/78.png)


</TabItem>


 <TabItem value="Liux" label="原生模型对话" >

7. 清空“检查点路径”中的LoRA配置，单击下图高亮②所示的“卸载模型”按钮，卸载微调后的模型，模型卸载完成后，单击“加载模型”按钮，加载原生的`Qwen2.5-VL-7B-Instruct`模型进行对话，其余配置保持不变。用户模拟词依旧输入“自动驾驶车辆应该关注哪些风险？”，观察模型回答，如下图高亮⑤所示。

       ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/vla/79.png)

 </TabItem>

</Tabs>


综合看来通过使用微调后的模型进行对话，可以获得更具参考价值的回答。相较于原生模型，后者往往提供的是宽泛且笼统的可能性描述，而经过特定任务或数据集微调的模型能够基于实际的图片场景生成更为精准、有针对性的回答。 

* 提高回答的准确性：根据具体的图像内容提供详细且准确的信息。
* 增强实用性：减少无关信息，聚焦于与用户查询最相关的细节。
* 提升用户体验：通过更贴近实际场景的回答，使交互过程更加流畅和高效。

<Tabs>
<TabItem value="Window" label="微调后模型评估" >

8. 切换至“Evaluate & Predict”页面，选择微调后模型的检查点路径，例如下图高亮①所示；然后选择平台预置的`QA_from_CoVLA_zh`数据集，并根据实际需求配置评估参数（本实践的参数设置如下图所示）。


     ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/vla/82.png)

9. 参数配置完成后，点击“开始”按钮即可启动评估，页面底部将实时显示评估过程中的日志信息，评估完成后，记录评估结果，结果如下所示。

          ```shell
         {
         "predict_bleu-4": 52.764348416666664,
         "predict_model_preparation_time": 0.0053,
         "predict_rouge-1": 71.00543958333333,
         "predict_rouge-2": 57.82092183333333,
         "predict_rouge-l": 70.30141458333334,
         "predict_runtime": 232.7091,
         "predict_samples_per_second": 5.157,
         "predict_steps_per_second": 0.645
         }
          ```
      结果解读：该模型在生成任务中表现出色，`BLEU-4`得分为`52.76`，`ROUGE-L`达到`70.30`，表明生成文本在词汇匹配、短语重合和句子级逻辑连贯性方面均具有较高质量。同时，模型推理效率良好，准备时间仅`0.0053`秒，平均每秒可处理`5.16`个样本，整体性能稳定且具备实际应用价值。
</TabItem>


 <TabItem value="Lix" label="原生模型评估" >

8. 切换至“Evaluate & Predict”页面，清空检查点路径配置，数据集依旧选择平台预置的`QA_from_CoVLA_zh`数据集，并根据实际需求配置评估参数（本实践的参数设置如下图所示）。


      ![SwanLab对比图](https://s1.llamafactory.online/llamafactory-online/docs/v2.0/documents/Practice/vla/83.png)

9. 完成配置后，点击“开始”按钮即可启动评估，页面底部将实时显示评估过程中的日志信息，评估完成后，记录评估结果，结果如下所示。

          ```shell
         {
         "predict_bleu-4": 5.996730166666667,
         "predict_model_preparation_time": 0.0047,
         "predict_rouge-1": 29.889636416666665,
         "predict_rouge-2": 7.416755416666667,
         "predict_rouge-l": 22.993275999999998,
         "predict_runtime": 734.3082,
         "predict_samples_per_second": 1.634,
         "predict_steps_per_second": 0.204
         }
          ```
     结果解读：该模型在生成任务中的表现较差，`BLEU-4`得分为`6.00`，`ROUGE-L`为`22.9`9，且`ROUGE-2`低至`7.42`，表明生成文本在`n-gram`匹配、短语重合度和句子级语义连贯性方面与参考文本存在较大差距，生成质量较低。尽管模型准备时间极短（仅`0.0047`秒），但推理速度较慢，总运行时间达`734`秒（约`12.2`分钟），每秒仅处理约1.63个样本，效率偏低。


  </TabItem>

</Tabs>
对比两组结果可见，第一组模型在生成质量与推理效率上均显著优于第二组：其`BLEU-4`（`52.76`vs`6.00`）、`ROUGE-L`（`70.30`vs`22.99`）等指标远高于第二组，且推理速度达`5.16`样本/秒，是第二组（`1.63`样本/秒）的三倍以上，运行时间更短。综合表明，微调后模型不仅输出更准确、连贯，且响应更快，具备更强的实用性与部署价值。



 </TabItem>
 

</Tabs>

#### 4.4.4 总结

用户可通过LLaMA-Factory Online平台预置的模型及数据集完成快速微调与效果验证。从上述实践案例可以看出，基于`Qwen2.5-VL-7B-Instruct`模型，采用LoRA方法在`QA_from_CoVLA_zh`多模态智能驾驶数据集上进行指令微调后，模型在问题理解、行为指导等方面均有显著提升。

本实践为构建高效能、轻量化的多模态自动驾驶感知系统提供了可复用的技术路径，未来可进一步探索多模态输入输出融合、模型在线增量学习与动态场景自适应能力，持续提升系统在复杂真实环境下的感知性与决策智能化水平。

