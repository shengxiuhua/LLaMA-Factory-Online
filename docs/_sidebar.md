- [LLM-Cookbook](./README.md)
  - [前言](./前言.md)
  - [第一章 NLP 基础概念](./chapter1/1.%20NLP是什么.md)
    - [1. NLP概念](/docs/chapter1/concept)
    - [2. NLP的演进之路](./chapter1/2.%20NLP的演进之路.md)
    - [3. NLP的核心任务](./chapter1/3.%20NLP的核心任务.md)
    - [4. 文本表示技术的演进](./chapter1/4.%20文本表示技术的演进.md)
  - [第二章 Transformer 架构](./chapter2/1.%20Transformer的历史意义与技术革命.md)
    - [1. Transformer的历史意义与技术革命](./chapter2/1.%20Transformer的历史意义与技术革命.md)
    - [2. 注意力机制](./chapter2/2.%20注意力机制.md)  
    - [3. Encoder-Decoder：序列转换的经典架构](./chapter2/3.%20Encoder-Decoder：序列转换的经典架构.md)
    - [4. 搭建一个Transformer](./chapter2/4.%20搭建一个Transformer.md)
  - [第三章 预训练语言模型](./chapter3/1.%20预训练语言模型的革命性意义.md)
    - [1. 预训练语言模型的革命性意义](./chapter3/1.%20预训练语言模型的革命性意义.md)
    - [2. Encoder-only PLM](./chapter3/2.%20Encoder-only%20PLM.md)
    - [3. Encoder-Decoder PLM](./chapter3/3.%20Encoder-Decoder%20PLM.md)
    - [4. Decoder-Only PLM](./chapter3/4.%20Decoder-Only%20PLM.md)
    - [5. 三种架构的对比与选择指南](./chapter3/5.%20三种架构的对比与选择指南.md)
  - [第四章 大语言模型](./chapter4/1.%20大语言模型的时代意义.md)
    - [1. 大语言模型的时代意义](./chapter4/1.%20大语言模型的时代意义.md)
    - [2. 什么是LLM](./chapter4/2.%20什么是LLM.md)
    - [3. 如何训练一个LLM](./chapter4/3.%20如何训练一个LLM.md)
  - [第五章 大模型微调](./chapter5/1.%20大模型微调的必要性与价值.md)
    - [1. 大模型微调的必要性与价值](./chapter5/1.%20大模型微调的必要性与价值.md)
    - [2. 监督微调（SFT）](./chapter5/2.%20监督微调（SFT）.md)
    - [3. LoRA：参数高效的微调革命](./chapter5/3.%20LoRA：参数高效的微调革命.md)  
    - [4. Adapter：模块化的微调方案](./chapter5/4.%20Adapter：模块化的微调方案.md)
    - [5. Prefix Tuning与Prompt Tuning](./chapter5/5.%20Prefix%20Tuning与Prompt%20Tuning.md)
    - [6. RLHF：对齐人类偏好的高级技术](./chapter5/6.%20RLHF：对齐人类偏好的高级技术.md)
    - [7. Prompt Engineering vs Fine-tuning](./chapter5/7.%20Prompt%20Engineering%20vs%20Fine-tuning.md)
    - [8. 实战指南：从零到一完成微调项目](./chapter5/8.%20实战指南：从零到一完成微调项目.md)
    - [9. 常见陷阱与解决方案](./chapter5/9.%20常见陷阱与解决方案.md)
  - [第六章 微调与其他模型优化方案的区别](./chapter6/1.%20理解模型优化的全景图.md)
    - [1. 理解模型优化的全景图](./chapter6/1.%20理解模型优化的全景图.md)
    - [2. 微调 vs 提示工程](./chapter6/2.%20微调%20vs%20提示工程.md)
    - [3. 微调 vs 模型蒸馏](./chapter6/3.%20微调%20vs%20模型蒸馏.md)
    - [4. 微调 vs 预训练](./chapter6/4.%20微调%20vs%20预训练.md)
    - [5. 综合决策框架](./chapter6/5.%20综合决策框架.md)
  - [第七章 主流大模型微调框架与工具栈](./chapter7/1.%20构建高效微调生态系统.md)
    - [1. 构建高效微调生态系统](./chapter7/1.%20构建高效微调生态系统.md)
    - [2. 深度学习框架](./chapter7/2.%20深度学习框架.md)
    - [3. Transformers库：预训练模型的统一接口](./chapter7/3.%20Transformers库：预训练模型的统一接口.md)
    - [4. PEFT框架：参数高效微调的专用工具](/chapter7/4.%20PEFT框架：参数高效微调的专用工具.md)
    - [5. 数据处理与实验管理工具](./chapter7/5.%20数据处理与实验管理工具.md)
    - [6. 硬件优化与分布式训练](./chapter7/6.%20硬件优化与分布式训练.md)
    - [7. 工具栈选择与最佳实践](./chapter7/7.%20工具栈选择与最佳实践.md)
  - [第八章 数据集构建与处理](./chapter8/1.%20数据质量决定模型上限.md)
    - [1. 数据质量决定模型上限](./chapter8/1.%20数据质量决定模型上限.md)
    - [2. 数据集构建原则](./chapter8/2.%20数据集构建原则.md)
    - [3. 数据预处理流程](./chapter8/3.%20数据预处理流程.md)
  - [第九章 微调参数详解](./chapter9/1.%20超参数的艺术与科学.md)
    - [1. 超参数的艺术与科学](/chapter9/1.%20超参数的艺术与科学.md)
    - [2. Learning Rate (学习率) 设置技巧](./chapter9/2.%20Learning%20Rate%20(学习率)%20设置技巧.md)
    - [3. Batch Size 与 Gradient Accumulation 的平衡](./chapter9/3.%20Batch%20Size%20与%20Gradient%20Accumulation%20的平衡.md)
    - [4. Epochs 与 Overfitting (过拟合).md](./chapter9/4.%20Epochs%20与%20Overfitting%20(过拟合).md)
    - [5. 常见微调超参数速查表](./chapter9/5.%20常见微调超参数速查表.md)
    - [6. 高级超参数调优技术](./chapter9/6.%20高级超参数调优技术.md)
  - [第十章 最佳实践](./chapter10/1.%20从理论到实践的桥梁.md)
    - [1. 从理论到实践的桥梁](./chapter10/1.%20从理论到实践的桥梁.md)
    - [2. 按模型分类的最佳实践](./chapter10/2.%20按模型分类的最佳实践.md)
    - [3. 全参数微调（Full Fine-Tuning）深度解析](./chapter10/3.%20全参数微调（Full%20Fine-Tuning）深度解析.md)
    - [4. 按行业分类的最佳实践](./chapter10/4.%20按行业分类的最佳实践.md) 
    - [5. 跨行业通用最佳实践](./chapter10/5.%20跨行业通用最佳实践.md)