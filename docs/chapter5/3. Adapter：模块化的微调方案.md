# 3. Adapter：模块化的微调方案

## 3.1 Adapter的设计理念

Adapter是另一种重要的参数高效微调方法，由Google在2019年提出。与LoRA不同，Adapter采用了**模块化设计**的思路：在Transformer的每一层中插入小型的"适配器"模块，只训练这些适配器，冻结原始模型。

Adapter模块的结构非常简单：一个下投影层（将维度从d_model降到bottleneck_dim）、一个非线性激活函数（如GELU）、一个上投影层（将维度从bottleneck_dim升回d_model）、以及一个残差连接。这种"降维-激活-升维"的瓶颈结构迫使模型学习输入的压缩表示，从而实现任务特定的特征变换。

Adapter的**关键优势**在于其独立性和稳定性。由于Adapter是独立的模块，不会直接修改原始层的权重，因此训练过程非常稳定，不容易出现训练崩溃或灾难性遗忘。此外，Adapter的模块化设计使得多任务管理非常直观，可以为不同任务训练不同的Adapter，轻松切换。

## 3.2 Adapter的配置要点

Adapter的主要配置参数是**瓶颈维度（bottleneck_dim）**。这个参数决定了Adapter的容量和参数量。瓶颈维度越大，Adapter的表达能力越强，但参数量也越多。通常，瓶颈维度设置为模型隐藏维度的1/8到1/16。例如，对于隐藏维度768的模型，瓶颈维度可以设置为64或128。

另一个重要参数是**Adapter的插入位置**。Adapter可以插入在注意力层之后、前馈网络层之后，或两者都插入。只在FFN后插入是最常见的配置，参数量较少；两处都插入可以提供更强的适配能力，但参数量会翻倍。

**激活函数的选择**也会影响效果。GELU是最常用的选择，它在大多数任务上表现良好。ReLU更简单，计算更快，但表达能力稍弱。可以根据具体任务选择。

## 3.3 Adapter的实现

使用AdapterHub库可以方便地实现Adapter：

```python
from transformers import AutoAdapterModel, AdapterConfig

# 加载支持Adapter的模型
model = AutoAdapterModel.from_pretrained(model_name)

# 配置Adapter
adapter_config = AdapterConfig.load(
    "pfeiffer",  # Adapter类型
    reduction_factor=16  # 降维因子，bottleneck_dim = d_model / reduction_factor
)

# 添加Adapter
model.add_adapter("task_adapter", config=adapter_config)

# 激活Adapter进行训练
model.train_adapter("task_adapter")

# 设置active adapter
model.set_active_adapters("task_adapter")
```

训练Adapter时，学习率通常设置为1e-4左右，比LoRA稍小。Adapter对批次大小相对不敏感，可以使用较大的批次以加快训练。

## 3.4 Adapter vs LoRA：如何选择

Adapter和LoRA各有优势，选择哪个取决于具体需求。**参数效率**方面，LoRA通常更优，相同效果下参数量更少。**推理效率**方面，LoRA也更优，因为可以合并权重，而Adapter会增加推理延迟。**训练稳定性**方面，Adapter更优，因为它不直接修改原始权重。**多任务管理**方面，两者都很方便，但Adapter的模块化设计更直观。

一般建议：如果追求极致的参数效率和推理速度，选择LoRA；如果需要高度稳定的训练过程和精细的中间层控制，选择Adapter；如果任务非常复杂，可以考虑结合两者。

