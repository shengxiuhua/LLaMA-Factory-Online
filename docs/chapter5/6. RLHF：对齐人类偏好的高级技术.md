
## 6. RLHF：对齐人类偏好的高级技术

### 6.1 RLHF的必要性

RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）是目前最先进的模型对齐技术，被ChatGPT、Claude等顶级模型广泛采用。它解决了一个关键问题：**如何让模型的输出更符合人类的偏好和价值观**。

传统的监督微调只能让模型学会"模仿"训练数据，但无法保证输出的质量、安全性和有用性。例如，模型可能生成事实错误的内容、有害的建议、或者冗长无用的回答。RLHF通过引入人类反馈，让模型学会什么样的输出是好的，什么样的是不好的。

### 6.2 RLHF的三个阶段

RLHF包含三个主要阶段。**第一阶段是监督微调（SFT）**，使用高质量的示例数据对模型进行初步微调，让模型学会基本的任务格式和风格。这一阶段通常需要几千到几万条高质量的示例。

**第二阶段是训练奖励模型（Reward Model）**。收集人类对模型输出的偏好数据，通常是成对比较的形式：给定同一个输入，人类标注哪个输出更好。然后训练一个奖励模型来预测人类的偏好。这个奖励模型本质上是一个分类器，输入是"问题+回答"，输出是一个分数，表示这个回答的质量。

**第三阶段是使用PPO算法优化模型**。使用奖励模型作为反馈信号，通过强化学习来优化模型，使其生成的输出获得更高的奖励。同时，为了防止模型过度优化奖励而偏离原始能力，会加入KL散度约束，限制模型与SFT模型的差异。

### 6.3 RLHF的实践挑战

RLHF虽然强大，但实践起来有不少挑战。首先是**数据收集成本高**。需要大量的人类标注数据，特别是偏好数据的标注需要专业人员，成本很高。其次是**训练复杂度高**。RLHF涉及多个模型（策略模型、奖励模型、参考模型）和复杂的训练流程，对工程能力要求很高。第三是**奖励Hacking问题**。模型可能学会欺骗奖励模型，生成看起来好但实际不好的输出。

因此，RLHF通常只在有充足资源和专业团队的情况下使用。对于大多数应用，高质量的SFT就已经足够。

### 6.4 RLHF的简化替代方案

如果想获得类似RLHF的效果但又没有足够资源，可以考虑一些简化方案。**DPO（Direct Preference Optimization）**是一种新方法，它直接从偏好数据中学习，不需要训练独立的奖励模型，大大简化了流程。**RLAIF（Reinforcement Learning from AI Feedback）**使用AI模型（如GPT-4）来提供反馈，代替人类标注，降低了成本。

这些方法虽然效果可能略逊于完整的RLHF，但在实践中已经能够带来显著的提升，而且实现成本要低得多。

