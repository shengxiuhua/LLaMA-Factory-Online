
## 2. 监督微调（SFT）

### 2.1 SFT的基本原理

监督微调（Supervised Fine-Tuning，简称SFT）是最直接、最常用的微调方法。它的核心思想非常简单：**在预训练模型的基础上，使用带标签的特定任务数据，通过监督学习的方式继续训练模型**。

SFT的工作流程可以分为以下几个步骤：首先，我们需要准备特定任务的训练数据，这些数据通常是输入-输出对的形式，例如问题-答案、指令-响应等。然后，我们加载预训练模型作为初始化参数，这相当于站在巨人的肩膀上开始训练。接下来，我们使用这些标注数据对模型进行训练，通常使用交叉熵损失函数来衡量模型输出与真实标签之间的差异。最后，通过反向传播和梯度下降来更新模型参数，使其逐渐适应新任务。

**SFT与预训练的关键区别**在于训练目标和数据规模。预训练阶段，模型在海量无标注数据上学习通用的语言表示，训练目标通常是预测下一个词（因果语言模型）或填充被遮盖的词（掩码语言模型）。而SFT阶段，模型在相对较少的标注数据上学习特定任务，训练目标是生成符合任务要求的输出。此外，SFT通常使用比预训练更小的学习率，以避免破坏模型已经学到的通用知识。

### 2.2 SFT的实践要点

在实际应用SFT时，有几个关键要点需要注意。首先是**数据质量**。高质量的训练数据是成功的关键，数据应该准确、多样、具有代表性。一条高质量的数据胜过十条低质量的数据。建议在数据准备阶段投入足够的时间和精力，进行人工审核和清洗。

其次是**学习率的选择**。这是SFT中最重要的超参数之一。学习率过大会导致模型遗忘预训练知识，甚至训练崩溃；学习率过小则会导致收敛缓慢，无法充分适配新任务。一般来说，SFT的学习率应该比预训练小1-2个数量级。对于7B规模的模型，推荐的学习率范围是1e-5到2e-5。可以从2e-5开始尝试，观察训练曲线，如果loss下降太慢就适当增大，如果出现震荡就适当减小。

第三是**训练轮数的控制**。过多的训练轮数容易导致过拟合，即模型在训练集上表现很好，但在新数据上表现差。一般来说，对于数据量在1000-10000条的任务，3-5个epoch就足够了。建议使用验证集来监控过拟合，当验证集loss开始上升时就应该停止训练。

第四是**批次大小的设置**。批次大小影响训练的稳定性和速度。较大的批次大小可以提供更稳定的梯度估计，但需要更多显存。如果显存不足，可以通过梯度累积来模拟大批次训练。例如，如果你的GPU只能支持batch_size=2，但你想要有效批次大小为16，可以设置gradient_accumulation_steps=8。

### 2.3 SFT的代码实现

下面是一个完整的SFT实现示例：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
import torch

# 1. 加载模型和分词器
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# 2. 准备数据
def prepare_data(examples):
    """将原始数据转换为模型输入格式"""
    texts = []
    for instruction, output in zip(examples['instruction'], examples['output']):
        text = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
        texts.append(text)
    
    # 分词
    model_inputs = tokenizer(
        texts,
        max_length=512,
        truncation=True,
        padding="max_length"
    )
    model_inputs["labels"] = model_inputs["input_ids"].copy()
    return model_inputs

# 3. 配置训练参数
training_args = TrainingArguments(
    output_dir="./sft_model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    fp16=True
)

# 4. 创建Trainer并训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer
)

trainer.train()
```

这个示例展示了SFT的基本流程。在实际应用中，你可能需要根据具体任务调整数据格式、超参数等。

### 2.4 SFT的常见问题与解决方案

在实践SFT时，经常会遇到一些问题。**过拟合**是最常见的问题之一。表现为训练loss持续下降，但验证loss开始上升。解决方法包括：减少训练轮数、增加数据量、使用数据增强、增大weight_decay（如0.01-0.1）、使用dropout等正则化技术。

**灾难性遗忘**是另一个重要问题，指模型在学习新任务时忘记了原有的通用能力。例如，在医疗数据上微调后，模型可能在医疗问答上表现很好，但在其他常识问答上表现下降。解决方法包括：使用较小的学习率、在训练数据中混入部分通用数据（如20%）、冻结模型的部分层（如前几层）、使用知识蒸馏等技术。

**训练不稳定**也时有发生，表现为loss剧烈震荡或突然上升。这通常是学习率过大或数据有问题导致的。解决方法包括：减小学习率、使用梯度裁剪（max_grad_norm=1.0）、检查数据质量、使用warmup策略等。
