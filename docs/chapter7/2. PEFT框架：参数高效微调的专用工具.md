# 2. PEFT框架：参数高效微调的专用工具

## 2.1 PEFT库的设计理念

Hugging Face的PEFT（Parameter-Efficient Fine-Tuning）库是目前最完整的参数高效微调解决方案。**PEFT库的设计理念**是提供统一的API来支持各种参数高效微调方法，包括LoRA、Adapter、Prefix Tuning、P-Tuning等。这种统一的设计让开发者可以轻松切换和比较不同的方法。

PEFT库的核心优势包括：**方法丰富**，支持目前主流的所有参数高效微调方法；**易于使用**，只需几行代码就能应用到任何Transformers模型；**高度优化**，针对大模型进行了内存和计算优化；**可组合性**，可以同时使用多种方法或为不同任务训练不同的适配器。

```python
from peft import (
    get_peft_model, 
    LoraConfig, 
    AdapterConfig,
    PrefixTuningConfig,
    TaskType,
    PeftModel,
    PeftConfig
)
from transformers import AutoModelForCausalLM, AutoTokenizer

# 加载基础模型
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 配置LoRA
lora_config = LoraConfig(
    r=16,  # rank
    lora_alpha=32,  # scaling factor
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # 目标模块
    lora_dropout=0.05,  # dropout
    bias="none",  # bias类型
    task_type=TaskType.CAUSAL_LM  # 任务类型
)

# 应用LoRA到模型
peft_model = get_peft_model(model, lora_config)

# 查看可训练参数
peft_model.print_trainable_parameters()
# 输出: trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%
```

## 2.2 多种微调方法的对比与选择

PEFT库支持多种参数高效微调方法，每种方法都有其特点和适用场景：

```python
# 1. LoRA配置 - 最通用的选择
lora_config = LoraConfig(
    r=8,  # 较小的rank适合简单任务
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # 最小配置
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# 2. AdaLoRA配置 - 自适应LoRA
from peft import AdaLoraConfig

adalora_config = AdaLoraConfig(
    r=12,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    # AdaLoRA特有参数
    init_r=12,  # 初始rank
    tinit=200,  # 开始剪枝的步数
    tfinal=1000,  # 结束剪枝的步数
    deltaT=10,  # 剪枝间隔
)

# 3. Prefix Tuning配置 - 适合生成任务
prefix_config = PrefixTuningConfig(
    task_type=TaskType.CAUSAL_LM,
    num_virtual_tokens=20,  # 虚拟token数量
    token_dim=768,  # token维度
    num_transformer_submodules=2,
    num_attention_heads=12,
    num_layers=12,
    encoder_hidden_size=768,
    prefix_projection=True  # 是否使用投影
)

# 4. Prompt Tuning配置 - 参数最少
from peft import PromptTuningConfig

prompt_config = PromptTuningConfig(
    task_type=TaskType.CAUSAL_LM,
    prompt_tuning_init="TEXT",  # 初始化方式
    num_virtual_tokens=8,  # 虚拟token数量
    prompt_tuning_init_text="Classify if this text is positive or negative:",
    tokenizer_name_or_path=model_name,
)

# 比较不同方法的参数量
def compare_peft_methods(base_model, configs):
    """比较不同PEFT方法的参数量"""
    results = {}
    
    for name, config in configs.items():
        peft_model = get_peft_model(base_model, config)
        
        total_params = sum(p.numel() for p in peft_model.parameters())
        trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
        
        results[name] = {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'trainable_ratio': trainable_params / total_params * 100
        }
        
        print(f"{name}:")
        print(f"  Total params: {total_params:,}")
        print(f"  Trainable params: {trainable_params:,}")
        print(f"  Trainable ratio: {trainable_params/total_params*100:.4f}%\n")
    
    return results

# 运行比较
configs = {
    "LoRA": lora_config,
    "AdaLoRA": adalora_config,
    "Prefix Tuning": prefix_config,
    "Prompt Tuning": prompt_config
}

comparison_results = compare_peft_methods(model, configs)
```

**不同方法的选择建议**：

- **LoRA**：最通用的选择，适合大多数任务，参数少，效果好
- **AdaLoRA**：LoRA的改进版，可以自适应调整rank，适合复杂任务
- **Prefix Tuning**：适合生成任务，特别是需要控制生成风格的场景
- **Prompt Tuning**：参数最少，适合简单任务或资源极度受限的场景

## 2.3 多任务和多适配器管理

PEFT库的一个强大功能是支持多任务和多适配器管理：

```python
class MultiTaskPEFTManager:
    """多任务PEFT管理器"""
    
    def __init__(self, base_model_name):
        self.base_model_name = base_model_name
        self.base_model = None
        self.adapters = {}
        self.current_adapter = None
        
    def load_base_model(self):
        """加载基础模型"""
        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
    def add_adapter(self, adapter_name, config, train_dataset=None):
        """添加新的适配器"""
        if self.base_model is None:
            self.load_base_model()
            
        # 创建PEFT模型
        peft_model = get_peft_model(self.base_model, config)
        
        # 如果提供了训练数据，进行训练
        if train_dataset is not None:
            self._train_adapter(peft_model, train_dataset, adapter_name)
        
        # 保存适配器
        adapter_path = f"./adapters/{adapter_name}"
        peft_model.save_pretrained(adapter_path)
        
        self.adapters[adapter_name] = adapter_path
        print(f"Adapter '{adapter_name}' added and saved to {adapter_path}")
        
    def _train_adapter(self, peft_model, train_dataset, adapter_name):
        """训练适配器"""
        training_args = TrainingArguments(
            output_dir=f"./training_{adapter_name}",
            num_train_epochs=3,
            per_device_train_batch_size=4,
            learning_rate=1e-4,
            logging_steps=10,
            save_strategy="no"  # 不保存中间checkpoint
        )
        
        trainer = Trainer(
            model=peft_model,
            args=training_args,
            train_dataset=train_dataset
        )
        
        trainer.train()
        
    def load_adapter(self, adapter_name):
        """加载特定的适配器"""
        if adapter_name not in self.adapters:
            raise ValueError(f"Adapter '{adapter_name}' not found")
            
        if self.base_model is None:
            self.load_base_model()
            
        adapter_path = self.adapters[adapter_name]
        model = PeftModel.from_pretrained(self.base_model, adapter_path)
        self.current_adapter = adapter_name
        
        return model

# 使用示例
manager = MultiTaskPEFTManager("meta-llama/Llama-2-7b-hf")

# 为不同任务添加适配器
sentiment_config = LoraConfig(r=8, lora_alpha=16, target_modules=["q_proj", "v_proj"])
qa_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "o_proj"])

manager.add_adapter("sentiment", sentiment_config, sentiment_dataset)
manager.add_adapter("qa", qa_config, qa_dataset)

# 使用不同的适配器
sentiment_model = manager.load_adapter("sentiment")
qa_model = manager.load_adapter("qa")
```

