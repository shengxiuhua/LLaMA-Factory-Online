
## 6. 硬件优化与分布式训练

### 6.1 GPU配置与显存优化

大模型微调对硬件有较高要求，合理的GPU配置和显存优化策略是成功的关键。**GPU选择的考虑因素**包括：显存大小、计算能力、价格性能比、软件支持等。

```python
import torch
import psutil

class HardwareOptimizer:
    """硬件优化器"""
    
    def __init__(self):
        self.device_info = self._get_device_info()
        
    def _get_device_info(self):
        """获取设备信息"""
        info = {
            'cpu_count': psutil.cpu_count(),
            'memory_total': psutil.virtual_memory().total / (1024**3),  # GB
            'cuda_available': torch.cuda.is_available(),
        }
        
        if torch.cuda.is_available():
            info['gpu_count'] = torch.cuda.device_count()
            info['gpus'] = []
            
            for i in range(torch.cuda.device_count()):
                gpu_info = {
                    'id': i,
                    'name': torch.cuda.get_device_name(i),
                    'memory_total': torch.cuda.get_device_properties(i).total_memory / (1024**3),
                    'compute_capability': torch.cuda.get_device_properties(i).major
                }
                info['gpus'].append(gpu_info)
        
        return info
    
    def print_device_info(self):
        """打印设备信息"""
        print("=== Hardware Information ===")
        print(f"CPU cores: {self.device_info['cpu_count']}")
        print(f"System memory: {self.device_info['memory_total']:.1f} GB")
        print(f"CUDA available: {self.device_info['cuda_available']}")
        
        if self.device_info['cuda_available']:
            print(f"GPU count: {self.device_info['gpu_count']}")
            for gpu in self.device_info['gpus']:
                print(f"  GPU {gpu['id']}: {gpu['name']}")
                print(f"    Memory: {gpu['memory_total']:.1f} GB")
                print(f"    Compute capability: {gpu['compute_capability']}.x")
    
    def recommend_config(self, model_size="7b"):
        """推荐配置"""
        recommendations = {}
        
        if not self.device_info['cuda_available']:
            recommendations['method'] = "CPU inference only"
            recommendations['note'] = "No GPU available, consider using cloud services"
            return recommendations
        
        total_gpu_memory = sum(gpu['memory_total'] for gpu in self.device_info['gpus'])
        
        if model_size == "7b":
            if total_gpu_memory >= 24:
                recommendations = {
                    'method': 'Full fine-tuning or LoRA',
                    'batch_size': 8,
                    'gradient_accumulation': 2,
                    'precision': 'fp16',
                    'note': 'Sufficient memory for full fine-tuning'
                }
            elif total_gpu_memory >= 12:
                recommendations = {
                    'method': 'LoRA with 8-bit',
                    'batch_size': 4,
                    'gradient_accumulation': 4,
                    'precision': 'fp16',
                    'note': 'Use 8-bit quantization to save memory'
                }
            elif total_gpu_memory >= 6:
                recommendations = {
                    'method': 'QLoRA (4-bit)',
                    'batch_size': 2,
                    'gradient_accumulation': 8,
                    'precision': 'bf16',
                    'note': 'Use 4-bit quantization for memory efficiency'
                }
            else:
                recommendations = {
                    'method': 'Not recommended',
                    'note': 'Insufficient GPU memory, consider smaller models or cloud services'
                }
        
        return recommendations

# 显存优化技术
class MemoryOptimizer:
    """显存优化器"""
    
    @staticmethod
    def setup_quantization(model_name, quantization_type="8bit"):
        """设置量化"""
        from transformers import BitsAndBytesConfig
        
        if quantization_type == "8bit":
            bnb_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False,
            )
        elif quantization_type == "4bit":
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16
            )
        else:
            raise ValueError("Unsupported quantization type")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        return model
    
    @staticmethod
    def enable_gradient_checkpointing(model):
        """启用梯度检查点"""
        if hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
            print("Gradient checkpointing enabled")
        else:
            print("Model does not support gradient checkpointing")
    
    @staticmethod
    def monitor_memory_usage():
        """监控显存使用"""
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                
                print(f"GPU {i}:")
                print(f"  Allocated: {allocated:.2f} GB")
                print(f"  Reserved: {reserved:.2f} GB")
                print(f"  Total: {total:.2f} GB")
                print(f"  Usage: {allocated/total*100:.1f}%")
```

### 6.2 DeepSpeed：微软的分布式训练框架

DeepSpeed是微软开发的深度学习优化库，特别适合大模型训练。**DeepSpeed的核心功能**包括：ZeRO优化器状态分片、梯度和参数分片、CPU offload、混合精度训练等。

```python
# DeepSpeed配置
def create_deepspeed_config(stage=2):
    """创建DeepSpeed配置"""
    
    config = {
        "train_batch_size": 16,
        "train_micro_batch_size_per_gpu": 2,
        "gradient_accumulation_steps": 8,
        "optimizer": {
            "type": "AdamW",
            "params": {
                "lr": 1e-4,
                "betas": [0.9, 0.999],
                "eps": 1e-8,
                "weight_decay": 0.01
            }
        },
        "scheduler": {
            "type": "WarmupLR",
            "params": {
                "warmup_min_lr": 0,
                "warmup_max_lr": 1e-4,
                "warmup_num_steps": 100
            }
        },
        "fp16": {
            "enabled": True,
            "auto_cast": False,
            "loss_scale": 0,
            "initial_scale_power": 16,
            "loss_scale_window": 1000,
            "hysteresis": 2,
            "min_loss_scale": 1
        },
        "activation_checkpointing": {
            "partition_activations": True,
            "cpu_checkpointing": True,
            "contiguous_memory_optimization": False,
            "number_checkpoints": 4,
            "synchronize_checkpoint_boundary": False,
            "profile": False
        }
    }
    
    # ZeRO配置
    if stage >= 1:
        config["zero_optimization"] = {
            "stage": stage,
            "allgather_partitions": True,
            "allgather_bucket_size": 2e8,
            "overlap_comm": True,
            "reduce_scatter": True,
            "reduce_bucket_size": 2e8,
            "contiguous_gradients": True
        }
    
    if stage >= 2:
        config["zero_optimization"]["cpu_offload"] = False
    
    if stage == 3:
        config["zero_optimization"]["cpu_offload"] = True
        config["zero_optimization"]["pin_memory"] = True
    
    return config

# 使用DeepSpeed训练
training_args = TrainingArguments(
    output_dir="./results",
    deepspeed=create_deepspeed_config(stage=2),
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=1e-4,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()
```

### 6.3 Megatron-LM：NVIDIA的大规模训练框架

Megatron-LM是NVIDIA开发的用于训练大规模语言模型的框架，支持模型并行、数据并行和流水线并行。

```python
# Megatron-LM配置示例
megatron_config = {
    "tensor_model_parallel_size": 2,  # 张量并行度
    "pipeline_model_parallel_size": 2,  # 流水线并行度
    "micro_batch_size": 1,
    "global_batch_size": 16,
    "sequence_length": 2048,
    "max_position_embeddings": 2048,
    "num_layers": 24,
    "hidden_size": 1024,
    "num_attention_heads": 16,
    "lr": 1.5e-4,
    "min_lr": 1.0e-5,
    "lr_decay_style": "cosine",
    "weight_decay": 0.1,
    "clip_grad": 1.0,
    "fp16": True,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
}
```


### 7.1 不同场景的工具栈推荐

**研究和实验场景**：
- 框架：PyTorch + Transformers + PEFT
- 数据处理：Datasets
- 实验管理：Weights & Biases
- 硬件优化：根据资源选择量化和LoRA

**生产部署场景**：
- 开发：PyTorch生态
- 部署：TensorFlow Serving或ONNX Runtime
- 监控：MLflow或Kubeflow
- 优化：模型量化和蒸馏

**资源受限场景**：
- 重点使用PEFT库的参数高效方法
- 配合4bit量化（QLoRA）
- 使用梯度检查点和混合精度
- 考虑模型蒸馏

### 7.2 最佳实践建议

**工具选择原则**：
1. **从简单开始**：先使用基础工具验证想法，再引入复杂技术
2. **重视兼容性**：选择生态系统完整、相互兼容的工具
3. **考虑团队技能**：选择团队熟悉或容易学习的工具
4. **关注社区活跃度**：活跃的社区意味着更好的支持和更快的问题解决

**实施建议**：
1. **建立标准化流程**：使用统一的数据格式、训练脚本、评估指标
2. **重视实验管理**：记录所有实验参数和结果，便于复现和对比
3. **持续优化**：根据实际使用情况不断调整工具栈
4. **关注新技术**：大模型技术发展很快，要及时了解新工具和方法

通过合理选择和使用这些框架与工具，我们可以构建高效、稳定、可扩展的大模型微调工作流，在有限的资源下实现优秀的模型性能。关键是要根据具体需求选择合适的工具组合，而不是盲目追求最新或最复杂的方案。