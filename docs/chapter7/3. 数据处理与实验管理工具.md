# 3. 数据处理与实验管理工具

## 3.1 Hugging Face Datasets
提供了统一的数据处理接口，支持从本地文件、远程数据集到流式数据的各种数据源。**Datasets库的核心优势**包括：内存高效的数据处理、丰富的预处理功能、与Transformers库的无缝集成、支持大规模数据集的流式处理。

```python
from datasets import Dataset, DatasetDict, load_dataset
import pandas as pd

# 1. 从各种数据源加载数据
def load_data_from_various_sources():
    """演示从不同数据源加载数据"""
    
    # 从CSV文件加载
    csv_dataset = load_dataset('csv', data_files='train.csv')
    
    # 从JSON文件加载
    json_dataset = load_dataset('json', data_files='train.json')
    
    # 从Hugging Face Hub加载
    hub_dataset = load_dataset('imdb')
    
    # 从pandas DataFrame创建
    df = pd.DataFrame({
        'text': ['This is great!', 'This is terrible.'],
        'label': [1, 0]
    })
    pandas_dataset = Dataset.from_pandas(df)
    
    # 从Python字典创建
    dict_dataset = Dataset.from_dict({
        'text': ['Amazing!', 'Awful!'],
        'label': [1, 0]
    })
    
    return {
        'csv': csv_dataset,
        'json': json_dataset,
        'hub': hub_dataset,
        'pandas': pandas_dataset,
        'dict': dict_dataset
    }

# 2. 数据预处理管道
class DataPreprocessor:
    """数据预处理器"""
    
    def __init__(self, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def preprocess_classification(self, examples):
        """分类任务预处理"""
        # 批量tokenize
        tokenized = self.tokenizer(
            examples['text'],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors=None
        )
        
        # 添加标签
        tokenized['labels'] = examples['label']
        
        return tokenized
    
    def preprocess_generation(self, examples):
        """生成任务预处理"""
        # 构建输入文本
        inputs = [f"Question: {q}\nAnswer: {a}" for q, a in zip(examples['question'], examples['answer'])]
        
        # Tokenize
        tokenized = self.tokenizer(
            inputs,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors=None
        )
        
        # 对于生成任务，labels就是input_ids
        tokenized['labels'] = tokenized['input_ids'].copy()
        
        return tokenized
    
    def preprocess_instruction_tuning(self, examples):
        """指令微调预处理"""
        formatted_texts = []
        
        for instruction, input_text, output in zip(
            examples['instruction'], 
            examples.get('input', [''] * len(examples['instruction'])),
            examples['output']
        ):
            if input_text:
                text = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
            else:
                text = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
            
            formatted_texts.append(text)
        
        # Tokenize
        tokenized = self.tokenizer(
            formatted_texts,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors=None
        )
        
        tokenized['labels'] = tokenized['input_ids'].copy()
        
        return tokenized

# 3. 高级数据处理功能
def advanced_data_processing():
    """演示高级数据处理功能"""
    
    # 加载数据集
    dataset = load_dataset('imdb')
    
    # 数据过滤
    filtered_dataset = dataset.filter(lambda x: len(x['text']) > 100)
    
    # 数据映射（并行处理）
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    preprocessor = DataPreprocessor(tokenizer)
    
    tokenized_dataset = filtered_dataset.map(
        preprocessor.preprocess_classification,
        batched=True,
        num_proc=4,  # 使用4个进程并行处理
        remove_columns=['text']  # 移除原始文本列
    )
    
    # 数据分片（用于分布式训练）
    train_dataset = tokenized_dataset['train']
    sharded_dataset = train_dataset.shard(num_shards=4, index=0)  # 取第0个分片
    
    # 数据流式处理（用于大数据集）
    streaming_dataset = load_dataset('imdb', streaming=True)
    
    # 数据缓存
    cached_dataset = tokenized_dataset.with_format('torch')  # 转换为PyTorch格式
    
    return {
        'filtered': filtered_dataset,
        'tokenized': tokenized_dataset,
        'sharded': sharded_dataset,
        'streaming': streaming_dataset,
        'cached': cached_dataset
    }

# 4. 自定义数据集类
class CustomInstructionDataset:
    """自定义指令数据集"""
    
    def __init__(self, data_path, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # 加载数据
        if data_path.endswith('.json'):
            self.dataset = load_dataset('json', data_files=data_path)['train']
        elif data_path.endswith('.csv'):
            self.dataset = load_dataset('csv', data_files=data_path)['train']
        else:
            raise ValueError("Unsupported file format")
        
        # 预处理
        self.dataset = self.dataset.map(
            self._preprocess,
            batched=True,
            remove_columns=self.dataset.column_names
        )
        
    def _preprocess(self, examples):
        """预处理函数"""
        formatted_texts = []
        
        for i in range(len(examples['instruction'])):
            instruction = examples['instruction'][i]
            input_text = examples.get('input', [''] * len(examples['instruction']))[i]
            output = examples['output'][i]
            
            if input_text:
                text = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
            else:
                text = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
            
            formatted_texts.append(text)
        
        # Tokenize
        tokenized = self.tokenizer(
            formatted_texts,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors=None
        )
        
        tokenized['labels'] = tokenized['input_ids'].copy()
        
        return tokenized
    
    def get_dataset(self):
        """获取处理后的数据集"""
        return self.dataset
```

## 3.2 Weights & Biases：实验追踪与管理

Weights & Biases（wandb）是目前最流行的机器学习实验追踪工具之一。**wandb的核心功能**包括：实验追踪、超参数优化、模型版本管理、团队协作、可视化分析。

```python
import wandb
from transformers import TrainingArguments, Trainer

# 1. 基本的wandb集成
def setup_wandb_tracking():
    """设置wandb实验追踪"""
    
    # 初始化wandb
    wandb.init(
        project="llm-finetuning",  # 项目名称
        name="lora-experiment-1",  # 实验名称
        config={  # 超参数配置
            "model_name": "meta-llama/Llama-2-7b-hf",
            "learning_rate": 1e-4,
            "batch_size": 4,
            "num_epochs": 3,
            "lora_r": 16,
            "lora_alpha": 32,
        },
        tags=["lora", "llama2", "instruction-tuning"]  # 标签
    )
    
    # 配置训练参数（自动集成wandb）
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=wandb.config.num_epochs,
        per_device_train_batch_size=wandb.config.batch_size,
        learning_rate=wandb.config.learning_rate,
        logging_steps=10,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        report_to="wandb",  # 自动报告到wandb
        run_name=wandb.run.name,  # 使用wandb的运行名称
    )
    
    return training_args

# 2. 自定义指标追踪
class WandbTrainer(Trainer):
    """集成wandb的自定义Trainer"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_count = 0
        
    def log(self, logs):
        """重写log方法，添加自定义指标"""
        super().log(logs)
        
        # 添加自定义指标
        if 'train_loss' in logs:
            # 计算困惑度
            perplexity = torch.exp(torch.tensor(logs['train_loss']))
            logs['train_perplexity'] = perplexity.item()
            
        # 记录学习率
        if hasattr(self.lr_scheduler, 'get_last_lr'):
            logs['learning_rate'] = self.lr_scheduler.get_last_lr()[0]
            
        # 记录GPU使用情况
        if torch.cuda.is_available():
            logs['gpu_memory_allocated'] = torch.cuda.memory_allocated() / 1024**3  # GB
            logs['gpu_memory_reserved'] = torch.cuda.memory_reserved() / 1024**3  # GB
        
        # 记录到wandb
        wandb.log(logs, step=self.step_count)
        self.step_count += 1
    
    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        """重写evaluate方法，添加自定义评估"""
        eval_results = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)
        
        # 添加自定义评估指标
        if f"{metric_key_prefix}_loss" in eval_results:
            perplexity = torch.exp(torch.tensor(eval_results[f"{metric_key_prefix}_loss"]))
            eval_results[f"{metric_key_prefix}_perplexity"] = perplexity.item()
        
        # 生成样本并记录
        if eval_dataset is not None:
            sample_outputs = self._generate_samples()
            
            # 创建wandb表格
            table = wandb.Table(columns=["Input", "Output", "Target"])
            for inp, out, target in sample_outputs[:5]:  # 只记录前5个样本
                table.add_data(inp, out, target)
            
            wandb.log({f"{metric_key_prefix}_samples": table})
        
        return eval_results
    
    def _generate_samples(self):
        """生成样本用于可视化"""
        # 这里应该实现样本生成逻辑
        # 返回 [(input, output, target), ...] 格式的列表
        return []

# 3. 超参数优化
def hyperparameter_sweep():
    """使用wandb进行超参数优化"""
    
    # 定义搜索空间
    sweep_config = {
        'method': 'bayes',  # 贝叶斯优化
        'metric': {
            'name': 'eval_loss',
            'goal': 'minimize'
        },
        'parameters': {
            'learning_rate': {
                'distribution': 'log_uniform_values',
                'min': 1e-5,
                'max': 1e-3
            },
            'lora_r': {
                'values': [4, 8, 16, 32]
            },
            'lora_alpha': {
                'values': [8, 16, 32, 64]
            },
            'batch_size': {
                'values': [2, 4, 8]
            },
            'warmup_ratio': {
                'distribution': 'uniform',
                'min': 0.0,
                'max': 0.2
            }
        }
    }
    
    # 创建sweep
    sweep_id = wandb.sweep(sweep_config, project="llm-finetuning-sweep")
    
    def train_with_sweep():
        """sweep训练函数"""
        # 初始化wandb
        wandb.init()
        
        # 获取超参数
        config = wandb.config
        
        # 配置LoRA
        lora_config = LoraConfig(
            r=config.lora_r,
            lora_alpha=config.lora_alpha,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
        
        # 创建模型
        model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
        peft_model = get_peft_model(model, lora_config)
        
        # 配置训练参数
        training_args = TrainingArguments(
            output_dir="./sweep_results",
            num_train_epochs=3,
            per_device_train_batch_size=config.batch_size,
            learning_rate=config.learning_rate,
            warmup_ratio=config.warmup_ratio,
            logging_steps=10,
            evaluation_strategy="epoch",
            save_strategy="no",  # sweep时不保存模型
            report_to="wandb"
        )
        
        # 训练
        trainer = WandbTrainer(
            model=peft_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
        )
        
        trainer.train()
        
        # 记录最终结果
        final_eval = trainer.evaluate()
        wandb.log({"final_eval_loss": final_eval["eval_loss"]})
    
    # 运行sweep
    wandb.agent(sweep_id, train_with_sweep, count=20)  # 运行20次实验
```

