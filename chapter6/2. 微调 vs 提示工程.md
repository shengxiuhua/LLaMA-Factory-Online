
## 2. 微调 vs 提示工程

### 2.1 本质差异：改变模型 vs 引导模型

微调和提示工程代表了两种根本不同的优化思路。**微调是一种"教学"策略**，通过修改模型的参数来让模型学习新的知识或能力。就像是给学生上课，通过系统的训练让他们掌握新的技能。这个过程会永久性地改变模型的内部表示，使其在特定任务上表现更好。

相比之下，**提示工程是一种"引导"策略**，不改变模型本身，而是通过精心设计的输入来激发模型已有的能力。这就像是给一个有知识的人提供清晰的指令和背景信息，帮助他理解你的需求并给出合适的回答。模型的参数保持不变，所有的优化都体现在输入的设计上。

这种本质差异带来了一系列连锁反应。微调需要训练数据、计算资源和时间，但能够让模型真正"学会"新任务；提示工程不需要这些，但效果受限于模型的原有能力。微调的结果是稳定的，不依赖于输入的精确措辞；提示工程的效果可能因为提示词的微小变化而大幅波动。

### 2.2 技术实现的对比

让我们通过具体的代码示例来理解这两种方法的实现差异。

**提示工程的实现**非常直接，不需要任何训练过程：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# 加载预训练模型
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# 设计提示词模板
def create_prompt(task, input_text, examples=None):
    """创建提示词"""
    prompt = f"Task: {task}\n\n"
    
    # 添加示例（Few-shot Learning）
    if examples:
        prompt += "Examples:\n"
        for i, (ex_input, ex_output) in enumerate(examples, 1):
            prompt += f"{i}. Input: {ex_input}\n   Output: {ex_output}\n\n"
    
    # 添加当前输入
    prompt += f"Now, please process this input:\nInput: {input_text}\nOutput:"
    
    return prompt

# 使用提示工程进行推理
task = "Classify the sentiment of the following text as positive, negative, or neutral."
input_text = "This product exceeded my expectations!"

# Zero-shot（零样本）
prompt_zero = create_prompt(task, input_text)

# Few-shot（少样本）
examples = [
    ("I love this!", "positive"),
    ("Terrible experience.", "negative"),
    ("It's okay.", "neutral")
]
prompt_few = create_prompt(task, input_text, examples)

# 生成输出
inputs = tokenizer(prompt_few, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)
```

这个例子展示了提示工程的核心：通过精心设计输入来引导模型。注意，我们没有修改模型的任何参数，所有的"优化"都在提示词的设计上。

**微调的实现**则需要完整的训练流程：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import Dataset

# 1. 准备训练数据
train_data = [
    {"text": "This product exceeded my expectations!", "label": "positive"},
    {"text": "Terrible experience, would not recommend.", "label": "negative"},
    {"text": "It's okay, nothing special.", "label": "neutral"},
    # ... 更多数据
]

# 2. 数据预处理
def preprocess_function(examples):
    """将数据转换为模型输入格式"""
    texts = []
    for text, label in zip(examples['text'], examples['label']):
        formatted = f"Classify sentiment: {text}\nSentiment: {label}"
        texts.append(formatted)
    
    model_inputs = tokenizer(texts, max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = model_inputs["input_ids"].copy()
    return model_inputs

dataset = Dataset.from_list(train_data)
tokenized_dataset = dataset.map(preprocess_function, batched=True)

# 3. 配置训练参数
training_args = TrainingArguments(
    output_dir="./sentiment_model",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    learning_rate=2e-5,
    warmup_ratio=0.1,
    logging_steps=10,
    save_strategy="epoch"
)

# 4. 训练模型
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

trainer.train()

# 5. 保存微调后的模型
model.save_pretrained("./sentiment_model_finetuned")
```

可以看到，微调需要准备训练数据、配置训练参数、执行训练过程，这比提示工程复杂得多，但换来的是模型真正学会了新任务。

### 2.3 效果与成本的权衡

**提示工程的优势**在于其极低的成本和极高的灵活性。你不需要任何GPU资源，不需要等待训练完成，只需要几分钟设计提示词就能看到效果。这使得提示工程非常适合快速原型开发和实验。当你想要验证一个想法是否可行时，提示工程是最快的方式。此外，提示工程可以即时调整，如果发现效果不好，修改提示词立即就能看到新的结果。

然而，**提示工程的局限性**也很明显。首先是效果上限受限。如果模型本身不具备某种能力，再好的提示词也无法让它"无中生有"。例如，如果模型没有见过医学文本，你很难通过提示词让它准确回答专业医学问题。其次是不稳定性。提示词的微小变化可能导致输出的巨大差异，这种敏感性使得提示工程难以在生产环境中保证稳定性。第三是上下文长度限制。复杂任务可能需要很长的提示词（包括任务描述、示例、背景信息等），容易超出模型的上下文窗口。

**微调的优势**在于其能够真正提升模型在特定任务上的能力。通过训练，模型可以学习到训练数据中的模式和知识，在该任务上的表现可以达到甚至超越人类水平。微调后的模型不依赖于复杂的提示词，推理时只需要简单的输入即可，这提高了推理效率。此外，微调的效果是稳定的，不会因为输入措辞的变化而大幅波动。

**微调的劣势**主要是成本。首先是数据成本，需要收集和标注足够的训练数据，这可能需要大量的人力和时间。其次是计算成本，训练需要GPU资源，对于大模型可能需要多张高端GPU。第三是时间成本，训练可能需要几小时到几天。第四是维护成本，当任务需求变化时，需要重新收集数据、重新训练。

### 2.4 适用场景分析

**何时应该使用提示工程？**

第一，当你的任务在模型的能力范围内时。例如，对于GPT-4这样的强大模型，很多常见任务（如摘要、翻译、简单问答）通过提示工程就能获得不错的效果。

第二，当你需要快速验证想法时。在项目初期，使用提示工程可以快速评估任务的可行性和难度，避免在不可行的方向上浪费资源。

第三，当你的数据量很少时（少于100条）。这种情况下，微调可能会严重过拟合，提示工程反而更合适。

第四，当任务需求频繁变化时。提示工程可以即时调整，而微调每次调整都需要重新训练。

第五，当你完全没有GPU资源时。提示工程只需要推理，可以使用API服务或CPU进行。

**何时应该使用微调？**

第一，当任务超出模型原有能力时。例如，让模型学习特定领域的专业知识（医疗、法律）、特定的输出格式、或者特定的风格。

第二，当你有充足的训练数据时（通常500条以上）。数据越多，微调的优势越明显。

第三，当你需要稳定的生产级性能时。微调后的模型不依赖于提示词设计，更适合部署到生产环境。

第四，当推理效率很重要时。微调后的模型不需要长提示词，推理速度更快，成本更低。

第五，当你有足够的资源（GPU、时间、人力）时。微调需要投入，但回报也更高。

### 2.5 混合策略：最佳实践

在实际应用中，最佳实践往往是**结合两种方法**。一个典型的工作流程是：

**阶段一：提示工程探索**。使用提示工程快速测试模型在任务上的基本能力。设计几个不同的提示词模板，尝试zero-shot和few-shot方法，评估效果。这个阶段的目标是了解任务难度和模型能力边界。

**阶段二：决策点**。如果提示工程的效果已经满足需求（如准确率达到80%以上），可以直接使用，无需微调。如果效果不理想但有改进空间，进入下一阶段。

**阶段三：数据准备与微调**。收集和标注训练数据，使用参数高效的方法（如LoRA）进行微调。从小规模实验开始（如500条数据），评估效果提升。

**阶段四：提示工程增强**。即使微调后，仍然可以使用提示工程来进一步优化。例如，在输入中提供上下文信息、明确输出格式、添加思维链提示等。

**阶段五：持续优化**。根据实际使用反馈，收集badcase，补充到训练数据中，定期重新微调。同时，也可以持续优化提示词设计。

这种混合策略充分发挥了两种方法的优势：用提示工程快速验证和迭代，用微调获得稳定的高性能，用提示工程增强微调后的模型。

### 2.6 实战案例：客户服务机器人

让我们通过一个具体案例来说明如何选择和结合这两种方法。假设我们要构建一个客户服务机器人，需要回答产品相关问题。

**第一步：提示工程原型**

```python
# 设计提示词模板
system_prompt = """You are a helpful customer service assistant for TechCorp.
You should answer questions about our products professionally and accurately.

Product Information:
- Product A: A smartphone with 128GB storage, priced at $699
- Product B: A laptop with 16GB RAM, priced at $1299
- Product C: Wireless earbuds, priced at $199

Guidelines:
1. Be polite and professional
2. Provide accurate information
3. If you don't know, say so
4. Suggest related products when appropriate
"""

def answer_question(question):
    prompt = f"{system_prompt}\n\nCustomer Question: {question}\n\nYour Response:"
    # 调用模型生成回答
    return generate_response(prompt)

# 测试
print(answer_question("What's the price of Product A?"))
print(answer_question("Which product is best for gaming?"))
```

通过测试，我们发现提示工程在简单问题上表现不错，但在复杂问题（如产品比较、故障排查）上表现不佳。

**第二步：收集数据并微调**

```python
# 准备训练数据
training_data = [
    {
        "question": "My Product A won't turn on, what should I do?",
        "answer": "I'm sorry to hear that. Please try these steps: 1) Hold the power button for 10 seconds..."
    },
    {
        "question": "Can I upgrade the RAM on Product B?",
        "answer": "Yes, Product B supports RAM upgrades up to 32GB. You can purchase compatible RAM..."
    },
    # ... 收集500+条真实客服对话
]

# 使用LoRA微调
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)

model = get_peft_model(base_model, lora_config)

# 训练（代码省略，参考前面的示例）
```

**第三步：结合使用**

```python
# 微调后，仍然使用提示词来提供上下文
def answer_with_finetuned_model(question, context=None):
    prompt = f"Customer Question: {question}\n"
    if context:
        prompt = f"Context: {context}\n" + prompt
    prompt += "Response:"
    
    # 使用微调后的模型
    return finetuned_model.generate(prompt)

# 现在模型在复杂问题上也表现很好
print(answer_with_finetuned_model(
    "My Product A screen is flickering",
    context="Customer purchased 2 months ago, no physical damage"
))
```

这个案例展示了如何从提示工程开始，逐步过渡到微调，并在微调后继续使用提示工程来提供额外信息。
